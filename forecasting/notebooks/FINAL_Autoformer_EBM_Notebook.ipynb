{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7EbFfOlPFYP"
      },
      "source": [
        "MOUNTING STORAGE DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW00_Ckq5vQO",
        "outputId": "a0cbcf5b-bcae-436b-87df-03f5d4bb5afe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF4t8ZOpRn-b"
      },
      "source": [
        "AUTOFORMER GIT REPO CLONING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2548e9c2",
        "outputId": "cccd0d67-3aeb-4c49-c88b-0eb00cbe3fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Cloning into 'Autoformer'...\n",
            "remote: Enumerating objects: 376, done.\u001b[K\n",
            "remote: Counting objects: 100% (212/212), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 376 (delta 157), reused 143 (delta 143), pack-reused 164 (from 2)\u001b[K\n",
            "Receiving objects: 100% (376/376), 2.20 MiB | 31.78 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n",
            "/content/Autoformer/Autoformer/Autoformer/Autoformer/Autoformer\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: reformer_pytorch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 3)) (2.8.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: axial-positional-embedding>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (0.3.12)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (0.8.1)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.12/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory in /usr/local/lib/python3.12/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (0.2.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from local-attention->reformer_pytorch->-r requirements.txt (line 6)) (0.2.1)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.12/dist-packages (from product-key-memory->reformer_pytorch->-r requirements.txt (line 6)) (0.11.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install dependencies for Autoformer (assuming requirements.txt in repo or manual install)\n",
        "!pip install pandas numpy matplotlib scikit-learn\n",
        "!git clone https://github.com/thuml/Autoformer.git\n",
        "%cd Autoformer\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Kfze02RyLZ"
      },
      "source": [
        "FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "807764bc",
        "outputId": "c67fd468-7356-43c8-b2b1-7e590ef1958e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded /content/drive/MyDrive/final_concatenated_data_mice_imputed.csv: 618 rows\n",
            "Tag column dropped\n",
            "Date range of combined data: 2024-06-01 00:00:00 to 2025-07-31 00:00:00\n",
            "\n",
            "Filtered data (using full MICE dataset): 618 rows\n",
            "Filtered date range: 2024-06-01 00:00:00 to 2025-07-31 00:00:00\n",
            "\n",
            "üéØ Target columns identified: ['Total_Stabilized_Naphtha_Product_Flowrate', 'Total_Kerosene_Product_Flowrate', 'Jet_Fuel_Product_Train1_Flowrate', 'Total_Light_Diesel_Product_Flowrate', 'Total_Heavy_Diesel_Product_Flowrate', 'Total_Atmospheric_Residue_Flowrate', 'Blend_Yield_Gas & LPG', 'Blend_Yield_Kerosene', 'Blend_Yield_Light Diesel', 'Blend_Yield_Heavy Diesel', 'Blend_Yield_RCO']\n",
            "üìà Feature columns identified: ['Light_Diesel_to_DHT_Unit_Flowrate', 'Kerosene_to_Light_Diesel_DHT_Flowrate', 'Atm_Residue_to_RFCC_Unit_Flowrate', 'Atm_Residue_to_Storage_Flowrate', 'Crude_Column_Naphtha_to_SGCU_Flowrate', 'Heavy_Diesel_to_MHC_Flowrate', 'Atm_Residue_to_Storage_EE1027_Flowrate', 'blend_id', 'API', 'Sulphur'] ... (656 total)\n",
            "\n",
            "Verifying data types:\n",
            "Date column dtype: datetime64[ns]\n",
            "‚ö†Ô∏è  Non-numeric column found: Total_Light_Diesel_Product_Flowrate (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: Total_Heavy_Diesel_Product_Flowrate (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TI2805 (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4505A (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4505B (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4505D (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4505E (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4505F (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4506A (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4506C (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4506D (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4506E (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4506F (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4507B (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4507C (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4507D (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4507E (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4508A (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4508B (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4508C (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4508D (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4508F (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4509A (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4509B (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101TXI4509C (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101FQ1401 (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101FQ3001 (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101FQ3101 (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101FQ4201 (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 101FQ5303 (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 102FQ2801 (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 102FQ4201 (dtype: object)\n",
            "‚ö†Ô∏è  Non-numeric column found: 102FQ4401 (dtype: object)\n",
            "\n",
            "‚ùå Converting non-numeric columns to numeric...\n",
            "‚úÖ Successfully converted Total_Light_Diesel_Product_Flowrate to numeric\n",
            "‚úÖ Successfully converted Total_Heavy_Diesel_Product_Flowrate to numeric\n",
            "‚úÖ Successfully converted 101TI2805 to numeric\n",
            "‚úÖ Successfully converted 101TXI4505A to numeric\n",
            "‚úÖ Successfully converted 101TXI4505B to numeric\n",
            "‚úÖ Successfully converted 101TXI4505D to numeric\n",
            "‚úÖ Successfully converted 101TXI4505E to numeric\n",
            "‚úÖ Successfully converted 101TXI4505F to numeric\n",
            "‚úÖ Successfully converted 101TXI4506A to numeric\n",
            "‚úÖ Successfully converted 101TXI4506C to numeric\n",
            "‚úÖ Successfully converted 101TXI4506D to numeric\n",
            "‚úÖ Successfully converted 101TXI4506E to numeric\n",
            "‚úÖ Successfully converted 101TXI4506F to numeric\n",
            "‚úÖ Successfully converted 101TXI4507B to numeric\n",
            "‚úÖ Successfully converted 101TXI4507C to numeric\n",
            "‚úÖ Successfully converted 101TXI4507D to numeric\n",
            "‚úÖ Successfully converted 101TXI4507E to numeric\n",
            "‚úÖ Successfully converted 101TXI4508A to numeric\n",
            "‚úÖ Successfully converted 101TXI4508B to numeric\n",
            "‚úÖ Successfully converted 101TXI4508C to numeric\n",
            "‚úÖ Successfully converted 101TXI4508D to numeric\n",
            "‚úÖ Successfully converted 101TXI4508F to numeric\n",
            "‚úÖ Successfully converted 101TXI4509A to numeric\n",
            "‚úÖ Successfully converted 101TXI4509B to numeric\n",
            "‚úÖ Successfully converted 101TXI4509C to numeric\n",
            "‚úÖ Successfully converted 101FQ1401 to numeric\n",
            "‚úÖ Successfully converted 101FQ3001 to numeric\n",
            "‚úÖ Successfully converted 101FQ3101 to numeric\n",
            "‚úÖ Successfully converted 101FQ4201 to numeric\n",
            "‚úÖ Successfully converted 101FQ5303 to numeric\n",
            "‚úÖ Successfully converted 102FQ2801 to numeric\n",
            "‚úÖ Successfully converted 102FQ4201 to numeric\n",
            "‚úÖ Successfully converted 102FQ4401 to numeric\n",
            "\n",
            "‚úÖ All columns in training dataset are now numeric\n",
            "Training dataset shape (no date): (0, 667)\n",
            "Full dataset shape (with date): (0, 668)\n",
            "\n",
            "Summary statistics for target columns:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(f\\\"   - Use 'custom_with_date\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Total_Stabilized_Naphtha_Product_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Kerosene_Product_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Jet_Fuel_Product_Train1_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Light_Diesel_Product_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Heavy_Diesel_Product_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Atmospheric_Residue_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_Gas & LPG\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_Kerosene\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_Light Diesel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_Heavy Diesel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_RCO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a05b9038-631a-4e6d-bf55-c5c37eed354d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total_Stabilized_Naphtha_Product_Flowrate</th>\n",
              "      <th>Total_Kerosene_Product_Flowrate</th>\n",
              "      <th>Jet_Fuel_Product_Train1_Flowrate</th>\n",
              "      <th>Total_Light_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Heavy_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Atmospheric_Residue_Flowrate</th>\n",
              "      <th>Blend_Yield_Gas &amp; LPG</th>\n",
              "      <th>Blend_Yield_Kerosene</th>\n",
              "      <th>Blend_Yield_Light Diesel</th>\n",
              "      <th>Blend_Yield_Heavy Diesel</th>\n",
              "      <th>Blend_Yield_RCO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a05b9038-631a-4e6d-bf55-c5c37eed354d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a05b9038-631a-4e6d-bf55-c5c37eed354d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a05b9038-631a-4e6d-bf55-c5c37eed354d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f705f57b-8f56-4815-9fa1-258c08959e6b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f705f57b-8f56-4815-9fa1-258c08959e6b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f705f57b-8f56-4815-9fa1-258c08959e6b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       Total_Stabilized_Naphtha_Product_Flowrate  \\\n",
              "count                                        0.0   \n",
              "mean                                         NaN   \n",
              "std                                          NaN   \n",
              "min                                          NaN   \n",
              "25%                                          NaN   \n",
              "50%                                          NaN   \n",
              "75%                                          NaN   \n",
              "max                                          NaN   \n",
              "\n",
              "       Total_Kerosene_Product_Flowrate  Jet_Fuel_Product_Train1_Flowrate  \\\n",
              "count                              0.0                               0.0   \n",
              "mean                               NaN                               NaN   \n",
              "std                                NaN                               NaN   \n",
              "min                                NaN                               NaN   \n",
              "25%                                NaN                               NaN   \n",
              "50%                                NaN                               NaN   \n",
              "75%                                NaN                               NaN   \n",
              "max                                NaN                               NaN   \n",
              "\n",
              "       Total_Light_Diesel_Product_Flowrate  \\\n",
              "count                                  0.0   \n",
              "mean                                   NaN   \n",
              "std                                    NaN   \n",
              "min                                    NaN   \n",
              "25%                                    NaN   \n",
              "50%                                    NaN   \n",
              "75%                                    NaN   \n",
              "max                                    NaN   \n",
              "\n",
              "       Total_Heavy_Diesel_Product_Flowrate  \\\n",
              "count                                  0.0   \n",
              "mean                                   NaN   \n",
              "std                                    NaN   \n",
              "min                                    NaN   \n",
              "25%                                    NaN   \n",
              "50%                                    NaN   \n",
              "75%                                    NaN   \n",
              "max                                    NaN   \n",
              "\n",
              "       Total_Atmospheric_Residue_Flowrate  Blend_Yield_Gas & LPG  \\\n",
              "count                                 0.0                    0.0   \n",
              "mean                                  NaN                    NaN   \n",
              "std                                   NaN                    NaN   \n",
              "min                                   NaN                    NaN   \n",
              "25%                                   NaN                    NaN   \n",
              "50%                                   NaN                    NaN   \n",
              "75%                                   NaN                    NaN   \n",
              "max                                   NaN                    NaN   \n",
              "\n",
              "       Blend_Yield_Kerosene  Blend_Yield_Light Diesel  \\\n",
              "count                   0.0                       0.0   \n",
              "mean                    NaN                       NaN   \n",
              "std                     NaN                       NaN   \n",
              "min                     NaN                       NaN   \n",
              "25%                     NaN                       NaN   \n",
              "50%                     NaN                       NaN   \n",
              "75%                     NaN                       NaN   \n",
              "max                     NaN                       NaN   \n",
              "\n",
              "       Blend_Yield_Heavy Diesel  Blend_Yield_RCO  \n",
              "count                       0.0              0.0  \n",
              "mean                        NaN              NaN  \n",
              "std                         NaN              NaN  \n",
              "min                         NaN              NaN  \n",
              "25%                         NaN              NaN  \n",
              "50%                         NaN              NaN  \n",
              "75%                         NaN              NaN  \n",
              "max                         NaN              NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Saved training dataset (NO DATE) to ./dataset/custom/custom.csv\n",
            "‚úÖ Saved reference dataset (WITH DATE) to ./dataset/custom/custom_with_date.csv\n",
            "Final training dataset shape: (0, 667)\n",
            "Training dataset columns: ['Total_Stabilized_Naphtha_Product_Flowrate', 'Total_Kerosene_Product_Flowrate', 'Jet_Fuel_Product_Train1_Flowrate', 'Total_Light_Diesel_Product_Flowrate', 'Total_Heavy_Diesel_Product_Flowrate', 'Total_Atmospheric_Residue_Flowrate', 'Blend_Yield_Gas & LPG', 'Blend_Yield_Kerosene', 'Blend_Yield_Light Diesel', 'Blend_Yield_Heavy Diesel', 'Blend_Yield_RCO', 'Light_Diesel_to_DHT_Unit_Flowrate', 'Kerosene_to_Light_Diesel_DHT_Flowrate', 'Atm_Residue_to_RFCC_Unit_Flowrate', 'Atm_Residue_to_Storage_Flowrate'] ... (total: 667)\n",
            "\n",
            "Final verification - Data types in training dataset:\n",
            "float64    667\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First 5 rows of the TRAINING dataset (no date column):\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5945b247-a3a0-488c-9efb-3b89a8f6cd03\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total_Stabilized_Naphtha_Product_Flowrate</th>\n",
              "      <th>Total_Kerosene_Product_Flowrate</th>\n",
              "      <th>Jet_Fuel_Product_Train1_Flowrate</th>\n",
              "      <th>Total_Light_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Heavy_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Atmospheric_Residue_Flowrate</th>\n",
              "      <th>Blend_Yield_Gas &amp; LPG</th>\n",
              "      <th>Blend_Yield_Kerosene</th>\n",
              "      <th>Blend_Yield_Light Diesel</th>\n",
              "      <th>Blend_Yield_Heavy Diesel</th>\n",
              "      <th>...</th>\n",
              "      <th>101FI8301</th>\n",
              "      <th>101FI8401</th>\n",
              "      <th>101FI8801</th>\n",
              "      <th>101FI8901</th>\n",
              "      <th>crude_WTI Midland</th>\n",
              "      <th>crude_MERO</th>\n",
              "      <th>101FIC3802</th>\n",
              "      <th>101FIC5801</th>\n",
              "      <th>101FIC6702</th>\n",
              "      <th>MW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows √ó 667 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5945b247-a3a0-488c-9efb-3b89a8f6cd03')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5945b247-a3a0-488c-9efb-3b89a8f6cd03 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5945b247-a3a0-488c-9efb-3b89a8f6cd03');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Total_Stabilized_Naphtha_Product_Flowrate, Total_Kerosene_Product_Flowrate, Jet_Fuel_Product_Train1_Flowrate, Total_Light_Diesel_Product_Flowrate, Total_Heavy_Diesel_Product_Flowrate, Total_Atmospheric_Residue_Flowrate, Blend_Yield_Gas & LPG, Blend_Yield_Kerosene, Blend_Yield_Light Diesel, Blend_Yield_Heavy Diesel, Blend_Yield_RCO, Light_Diesel_to_DHT_Unit_Flowrate, Kerosene_to_Light_Diesel_DHT_Flowrate, Atm_Residue_to_RFCC_Unit_Flowrate, Atm_Residue_to_Storage_Flowrate, Crude_Column_Naphtha_to_SGCU_Flowrate, Heavy_Diesel_to_MHC_Flowrate, Atm_Residue_to_Storage_EE1027_Flowrate, blend_id, API, Sulphur, crude_AGB, crude_BOL, crude_CJB, crude_WCD, crude_QUI, crude_AME, crude_FOR, crude_ABO, crude_OK2, crude_ESC, crude_BOC, crude_BRL, crude_AKP, crude_ASC, crude_AGO, crude_YOH, crude_OKW, crude_ERH, crude_PAZ, crude_Nembe, crude_Total, crude_Gas & LPG, crude_Naptha , crude_Kerosene, crude_Light Diesel, crude_Heavy Diesel, crude_RCO, Train_A_Raw_Crude_Flow, Train_B_Raw_Crude_Flow, Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU, Rich_Gas_Knockout_Drum_Liquid_to_CDU, 100FI2901, 100FI2902, 100FI4001, 100FI4002, 100FI4003, 101AI8501A, 101AI8501B, 101AI8501C, 101AI8501D, 101AI8601A, 101AI8601B, 101AI8601C, 101AI8601D, 101AI8701A, 101AI8701B, 101AI8701C, 101AI8701D, 101FI1401, Train_A_Raw_Crude_Flow.1, Train_B_Raw_Crude_Flow.1, 101FI3201, 101FI3202, 101FI3702, 101FI6502, 101FI8102, 101FI8103, 101FI8110, 101FI8202, 101FI8203, 101FI8210, 101FI8302, 101FI8303, 101FI8310, 101FI8402, 101FI8403, 101FI8410, 101FI8802, 101FI8803, 101FI8810, 101FI8902, 101FI8903, 101FI8910, 101FIC1404, 101FIC1405, 101FIC1701, 101FIC2002, 101FIC2004, 101FIC2101, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 667 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Last 5 rows of the TRAINING dataset (no date column):\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f3b35f6b-e766-43cf-8c1d-abc3f21c9534\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total_Stabilized_Naphtha_Product_Flowrate</th>\n",
              "      <th>Total_Kerosene_Product_Flowrate</th>\n",
              "      <th>Jet_Fuel_Product_Train1_Flowrate</th>\n",
              "      <th>Total_Light_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Heavy_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Atmospheric_Residue_Flowrate</th>\n",
              "      <th>Blend_Yield_Gas &amp; LPG</th>\n",
              "      <th>Blend_Yield_Kerosene</th>\n",
              "      <th>Blend_Yield_Light Diesel</th>\n",
              "      <th>Blend_Yield_Heavy Diesel</th>\n",
              "      <th>...</th>\n",
              "      <th>101FI8301</th>\n",
              "      <th>101FI8401</th>\n",
              "      <th>101FI8801</th>\n",
              "      <th>101FI8901</th>\n",
              "      <th>crude_WTI Midland</th>\n",
              "      <th>crude_MERO</th>\n",
              "      <th>101FIC3802</th>\n",
              "      <th>101FIC5801</th>\n",
              "      <th>101FIC6702</th>\n",
              "      <th>MW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows √ó 667 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3b35f6b-e766-43cf-8c1d-abc3f21c9534')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f3b35f6b-e766-43cf-8c1d-abc3f21c9534 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f3b35f6b-e766-43cf-8c1d-abc3f21c9534');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Total_Stabilized_Naphtha_Product_Flowrate, Total_Kerosene_Product_Flowrate, Jet_Fuel_Product_Train1_Flowrate, Total_Light_Diesel_Product_Flowrate, Total_Heavy_Diesel_Product_Flowrate, Total_Atmospheric_Residue_Flowrate, Blend_Yield_Gas & LPG, Blend_Yield_Kerosene, Blend_Yield_Light Diesel, Blend_Yield_Heavy Diesel, Blend_Yield_RCO, Light_Diesel_to_DHT_Unit_Flowrate, Kerosene_to_Light_Diesel_DHT_Flowrate, Atm_Residue_to_RFCC_Unit_Flowrate, Atm_Residue_to_Storage_Flowrate, Crude_Column_Naphtha_to_SGCU_Flowrate, Heavy_Diesel_to_MHC_Flowrate, Atm_Residue_to_Storage_EE1027_Flowrate, blend_id, API, Sulphur, crude_AGB, crude_BOL, crude_CJB, crude_WCD, crude_QUI, crude_AME, crude_FOR, crude_ABO, crude_OK2, crude_ESC, crude_BOC, crude_BRL, crude_AKP, crude_ASC, crude_AGO, crude_YOH, crude_OKW, crude_ERH, crude_PAZ, crude_Nembe, crude_Total, crude_Gas & LPG, crude_Naptha , crude_Kerosene, crude_Light Diesel, crude_Heavy Diesel, crude_RCO, Train_A_Raw_Crude_Flow, Train_B_Raw_Crude_Flow, Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU, Rich_Gas_Knockout_Drum_Liquid_to_CDU, 100FI2901, 100FI2902, 100FI4001, 100FI4002, 100FI4003, 101AI8501A, 101AI8501B, 101AI8501C, 101AI8501D, 101AI8601A, 101AI8601B, 101AI8601C, 101AI8601D, 101AI8701A, 101AI8701B, 101AI8701C, 101AI8701D, 101FI1401, Train_A_Raw_Crude_Flow.1, Train_B_Raw_Crude_Flow.1, 101FI3201, 101FI3202, 101FI3702, 101FI6502, 101FI8102, 101FI8103, 101FI8110, 101FI8202, 101FI8203, 101FI8210, 101FI8302, 101FI8303, 101FI8310, 101FI8402, 101FI8403, 101FI8410, 101FI8802, 101FI8803, 101FI8810, 101FI8902, 101FI8903, 101FI8910, 101FIC1404, 101FIC1405, 101FIC1701, 101FIC2002, 101FIC2004, 101FIC2101, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 667 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Dataset summary saved to: ./dataset/custom/dataset_summary.txt\n",
            "\n",
            "üéâ Data preparation completed successfully!\n",
            "   - Use 'custom.csv' for Autoformer training (contains 667 numeric columns)\n",
            "   - Use 'custom_with_date.csv' for reference (contains date + 667 numeric columns)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the path to the MICE interpolated and concatenated data\n",
        "file_path = '/content/drive/MyDrive/final_concatenated_data_mice_imputed.csv' # Ensure this is the correct file path\n",
        "\n",
        "# List of specified target columns\n",
        "target_cols_list = [\n",
        "    'Total_Stabilized_Naphtha_Product_Flowrate',\n",
        "    'Total_Kerosene_Product_Flowrate',\n",
        "    'Jet_Fuel_Product_Train1_Flowrate',\n",
        "    'Total_Light_Diesel_Product_Flowrate',\n",
        "    'Total_Heavy_Diesel_Product_Flowrate',\n",
        "    'Total_Atmospheric_Residue_Flowrate',\n",
        "    'Blend_Yield_Gas & LPG',\n",
        "    'Blend_Yield_Kerosene',\n",
        "    'Blend_Yield_Light Diesel',\n",
        "    'Blend_Yield_Heavy Diesel',\n",
        "    'Blend_Yield_RCO'\n",
        "]\n",
        "\n",
        "# Load the combined CSV file with robust error handling\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if 'date' column exists and parse dates\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "            df = df.dropna(subset=['date'])\n",
        "            if not df.empty:\n",
        "                print(f\"Successfully loaded {file_path}: {len(df)} rows\")\n",
        "            else:\n",
        "                raise ValueError(f\"Error: {file_path} loaded but had no valid date entries after parsing.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Error: File {file_path} does not contain a 'date' column.\")\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        raise ValueError(f\"Error: File {file_path} is empty.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading file {file_path}: {e}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Error: File not found - {file_path}.\")\n",
        "\n",
        "# Sort by date and reset index\n",
        "df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "# dirty column drop\n",
        "if \"Tag\" in df.columns:\n",
        "    df = df.drop(columns=[\"Tag\"])\n",
        "    print(\"Tag column dropped\")\n",
        "\n",
        "# Display date range\n",
        "if not df.empty:\n",
        "    print(f\"Date range of combined data: {df['date'].min()} to {df['date'].max()}\")\n",
        "else:\n",
        "    print(\"Combined dataframe is empty.\")\n",
        "    exit()\n",
        "\n",
        "# Use the entire MICE dataset\n",
        "df_filtered = df.copy()\n",
        "\n",
        "print(f\"\\nFiltered data (using full MICE dataset): {len(df_filtered)} rows\")\n",
        "if not df_filtered.empty:\n",
        "    print(f\"Filtered date range: {df_filtered['date'].min()} to {df_filtered['date'].max()}\")\n",
        "else:\n",
        "    print(\"Filtered dataframe is empty.\")\n",
        "    exit()\n",
        "\n",
        "# Identify feature columns (all columns except 'date' and the specified target columns)\n",
        "feature_cols = [col for col in df_filtered.columns if col != 'date' and col not in target_cols_list]\n",
        "\n",
        "# Check if all specified target columns exist in the DataFrame\n",
        "missing_targets = [col for col in target_cols_list if col not in df_filtered.columns]\n",
        "if missing_targets:\n",
        "    raise ValueError(f\"‚ùå The following specified target columns were not found in the dataset: {missing_targets}\")\n",
        "\n",
        "print(f\"\\nüéØ Target columns identified: {target_cols_list}\")\n",
        "print(f\"üìà Feature columns identified: {feature_cols[:10]} ... ({len(feature_cols)} total)\")\n",
        "\n",
        "# CRITICAL FIX: Create TWO different datasets for Autoformer\n",
        "\n",
        "# 1. Dataset with date column (for time-based operations and splitting)\n",
        "df_with_date = df_filtered[['date'] + target_cols_list + feature_cols].copy()\n",
        "\n",
        "# 2. Dataset WITHOUT date column (for model training - this is what gets fed to the neural network)\n",
        "# The Autoformer library expects the data file to contain ONLY the numeric columns\n",
        "# The date handling is done separately in the data loader\n",
        "df_auto = df_filtered[target_cols_list + feature_cols].copy()\n",
        "\n",
        "# CRITICAL FIX: Convert all columns to numeric types BEFORE any other processing\n",
        "print(f\"\\nüîß Converting all columns to numeric types...\")\n",
        "non_numeric_cols = []\n",
        "\n",
        "# Convert all columns except 'date' to numeric\n",
        "for col in df_auto.columns:\n",
        "    if not np.issubdtype(df_auto[col].dtype, np.number):\n",
        "        try:\n",
        "            # Convert to numeric, coercing errors to NaN\n",
        "            df_auto[col] = pd.to_numeric(df_auto[col], errors='coerce')\n",
        "            df_with_date[col] = pd.to_numeric(df_with_date[col], errors='coerce')\n",
        "            print(f\"‚úÖ Converted {col} to numeric\")\n",
        "        except Exception as e:\n",
        "            non_numeric_cols.append(col)\n",
        "            print(f\"‚ùå Failed to convert {col} to numeric: {e}\")\n",
        "\n",
        "# Drop columns that couldn't be converted\n",
        "if non_numeric_cols:\n",
        "    print(f\"\\n‚ö†Ô∏è  Dropping {len(non_numeric_cols)} columns that couldn't be converted to numeric:\")\n",
        "    for col in non_numeric_cols:\n",
        "        print(f\"   - {col}\")\n",
        "    df_auto = df_auto.drop(columns=non_numeric_cols)\n",
        "    df_with_date = df_with_date.drop(columns=non_numeric_cols)\n",
        "\n",
        "# Handle missing values (forward/backward fill) - This is a safeguard, MICE should have handled most\n",
        "df_with_date = df_with_date.fillna(method='ffill').fillna(method='bfill')\n",
        "df_auto = df_auto.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "# Verify all columns are numeric (except date in the first dataset)\n",
        "print(f\"\\nVerifying data types:\")\n",
        "print(f\"Date column dtype: {df_with_date['date'].dtype}\")\n",
        "\n",
        "# Final verification - check for any remaining non-numeric columns\n",
        "remaining_non_numeric = []\n",
        "for col in df_auto.columns:\n",
        "    if not np.issubdtype(df_auto[col].dtype, np.number):\n",
        "        remaining_non_numeric.append(col)\n",
        "\n",
        "if remaining_non_numeric:\n",
        "    print(f\"‚ùå Still have non-numeric columns: {remaining_non_numeric}\")\n",
        "    # Force conversion for any remaining non-numeric columns\n",
        "    for col in remaining_non_numeric:\n",
        "        df_auto[col] = pd.to_numeric(df_auto[col], errors='coerce')\n",
        "        df_with_date[col] = pd.to_numeric(df_with_date[col], errors='coerce')\n",
        "\n",
        "# Final cleanup - remove any rows with NaN values that might have been introduced\n",
        "initial_rows = len(df_auto)\n",
        "df_auto = df_auto.dropna()\n",
        "df_with_date = df_with_date.dropna()\n",
        "final_rows = len(df_auto)\n",
        "\n",
        "if initial_rows != final_rows:\n",
        "    print(f\"‚ö†Ô∏è  Removed {initial_rows - final_rows} rows with NaN values after conversion\")\n",
        "\n",
        "print(f\"\\n‚úÖ All columns in training dataset are now numeric\")\n",
        "print(f\"Training dataset shape (no date): {df_auto.shape}\")\n",
        "print(f\"Full dataset shape (with date): {df_with_date.shape}\")\n",
        "\n",
        "# Final verification\n",
        "print(f\"\\nüîç Final data type verification:\")\n",
        "print(f\"   - All columns are numeric: {all(df_auto.dtypes.apply(lambda x: np.issubdtype(x, np.number)))}\")\n",
        "print(f\"   - No NaN values: {not df_auto.isnull().any().any()}\")\n",
        "print(f\"   - Data types: {df_auto.dtypes.value_counts()}\")\n",
        "\n",
        "# Display summary statistics for target columns\n",
        "print(\"\\nSummary statistics for target columns:\")\n",
        "display(df_auto[target_cols_list].describe())\n",
        "\n",
        "# Create output directory and save BOTH datasets\n",
        "os.makedirs('./dataset/custom/', exist_ok=True)\n",
        "\n",
        "# CRITICAL FIX: Create a dataset that Autoformer expects\n",
        "# Autoformer expects: ['date', ...other_columns..., target_column]\n",
        "# We need to create a proper date column and reorder columns\n",
        "\n",
        "# Create a date range for the dataset\n",
        "print(f\"\\nüîß Creating proper date column for Autoformer...\")\n",
        "date_range = pd.date_range(start='2024-06-01', periods=len(df_auto), freq='D')\n",
        "df_auto_with_date = df_auto.copy()\n",
        "df_auto_with_date.insert(0, 'date', date_range)\n",
        "\n",
        "# Reorder columns to match Autoformer's expected format: ['date', ...features..., targets]\n",
        "# Put ALL target columns at the end for multivariate prediction\n",
        "target_cols = [col for col in target_cols_list if col in df_auto_with_date.columns]\n",
        "if target_cols:\n",
        "    # Get all columns except date and targets\n",
        "    feature_cols = [col for col in df_auto_with_date.columns if col not in ['date'] + target_cols]\n",
        "    # Reorder: date + features + all targets\n",
        "    df_auto_with_date = df_auto_with_date[['date'] + feature_cols + target_cols]\n",
        "    print(f\"‚úÖ Reordered columns: date + {len(feature_cols)} features + {len(target_cols)} targets\")\n",
        "    print(f\"   - Target columns: {target_cols}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  No target columns found, using original order\")\n",
        "    target_cols = [df_auto_with_date.columns[-1]]  # Use last column as target\n",
        "\n",
        "# Save the dataset WITH proper date column and column order (this is what Autoformer will use)\n",
        "output_path = './dataset/custom/custom.csv'\n",
        "df_auto_with_date.to_csv(output_path, index=False)\n",
        "\n",
        "# Save the dataset WITHOUT date column (for reference)\n",
        "output_path_no_date = './dataset/custom/custom_no_date.csv'\n",
        "df_auto.to_csv(output_path_no_date, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved training dataset (WITH DATE, PROPER ORDER) to {output_path}\")\n",
        "print(f\"‚úÖ Saved reference dataset (NO DATE) to {output_path_no_date}\")\n",
        "print(f\"Final training dataset shape: {df_auto_with_date.shape}\")\n",
        "print(f\"Training dataset columns: {list(df_auto_with_date.columns)[:15]} ... (total: {len(df_auto_with_date.columns)})\")\n",
        "print(f\"Date range: {df_auto_with_date['date'].min()} to {df_auto_with_date['date'].max()}\")\n",
        "\n",
        "# Verify the training dataset has no string columns\n",
        "print(f\"\\nFinal verification - Data types in training dataset:\")\n",
        "print(df_auto.dtypes.value_counts())\n",
        "\n",
        "# Display first few rows of TRAINING dataset (without date)\n",
        "print(f\"\\nFirst 5 rows of the TRAINING dataset (no date column):\")\n",
        "display(df_auto.head())\n",
        "\n",
        "# Display last few rows of TRAINING dataset (without date)\n",
        "print(f\"\\nLast 5 rows of the TRAINING dataset (no date column):\")\n",
        "display(df_auto.tail())\n",
        "\n",
        "# Create a summary file with dataset information\n",
        "summary_info = {\n",
        "    'total_rows': len(df_auto),\n",
        "    'total_columns': len(df_auto.columns),\n",
        "    'target_columns': target_cols_list,\n",
        "    'num_target_columns': len(target_cols_list),\n",
        "    'feature_columns': feature_cols[:20],  # First 20 feature columns\n",
        "    'num_feature_columns': len(feature_cols),\n",
        "    'date_range_start': str(df_with_date['date'].min()),\n",
        "    'date_range_end': str(df_with_date['date'].max()),\n",
        "    'data_types': str(df_auto.dtypes.value_counts().to_dict())\n",
        "}\n",
        "\n",
        "summary_path = './dataset/custom/dataset_summary.txt'\n",
        "with open(summary_path, 'w') as f:\n",
        "    f.write(\"Autoformer Dataset Summary\\n\")\n",
        "    f.write(\"=\" * 30 + \"\\n\")\n",
        "    for key, value in summary_info.items():\n",
        "        f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "print(f\"\\nüìä Dataset summary saved to: {summary_path}\")\n",
        "print(f\"\\nüéâ Data preparation completed successfully!\")\n",
        "print(f\"   - Use 'custom.csv' for Autoformer training (contains {df_auto.shape[1]} numeric columns)\")\n",
        "print(f\"   - Use 'custom_with_date.csv' for reference (contains date + {df_auto.shape[1]} numeric columns)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW7jeCe6R3WG"
      },
      "source": [
        "MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c5620810",
        "outputId": "03063e08-2144-4dee-eaba-4510b4847b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All columns in the dataset:\n",
            "0: 'Total_Stabilized_Naphtha_Product_Flowrate' (type: object)\n",
            "1: 'Total_Kerosene_Product_Flowrate' (type: object)\n",
            "2: 'Jet_Fuel_Product_Train1_Flowrate' (type: object)\n",
            "3: 'Total_Light_Diesel_Product_Flowrate' (type: object)\n",
            "4: 'Total_Heavy_Diesel_Product_Flowrate' (type: object)\n",
            "5: 'Total_Atmospheric_Residue_Flowrate' (type: object)\n",
            "6: 'Blend_Yield_Gas & LPG' (type: object)\n",
            "7: 'Blend_Yield_Kerosene' (type: object)\n",
            "8: 'Blend_Yield_Light Diesel' (type: object)\n",
            "9: 'Blend_Yield_Heavy Diesel' (type: object)\n",
            "10: 'Blend_Yield_RCO' (type: object)\n",
            "11: 'Light_Diesel_to_DHT_Unit_Flowrate' (type: object)\n",
            "12: 'Kerosene_to_Light_Diesel_DHT_Flowrate' (type: object)\n",
            "13: 'Atm_Residue_to_RFCC_Unit_Flowrate' (type: object)\n",
            "14: 'Atm_Residue_to_Storage_Flowrate' (type: object)\n",
            "15: 'Crude_Column_Naphtha_to_SGCU_Flowrate' (type: object)\n",
            "16: 'Heavy_Diesel_to_MHC_Flowrate' (type: object)\n",
            "17: 'Atm_Residue_to_Storage_EE1027_Flowrate' (type: object)\n",
            "18: 'blend_id' (type: object)\n",
            "19: 'API' (type: object)\n",
            "20: 'Sulphur' (type: object)\n",
            "21: 'crude_AGB' (type: object)\n",
            "22: 'crude_BOL' (type: object)\n",
            "23: 'crude_CJB' (type: object)\n",
            "24: 'crude_WCD' (type: object)\n",
            "25: 'crude_QUI' (type: object)\n",
            "26: 'crude_AME' (type: object)\n",
            "27: 'crude_FOR' (type: object)\n",
            "28: 'crude_ABO' (type: object)\n",
            "29: 'crude_OK2' (type: object)\n",
            "30: 'crude_ESC' (type: object)\n",
            "31: 'crude_BOC' (type: object)\n",
            "32: 'crude_BRL' (type: object)\n",
            "33: 'crude_AKP' (type: object)\n",
            "34: 'crude_ASC' (type: object)\n",
            "35: 'crude_AGO' (type: object)\n",
            "36: 'crude_YOH' (type: object)\n",
            "37: 'crude_OKW' (type: object)\n",
            "38: 'crude_ERH' (type: object)\n",
            "39: 'crude_PAZ' (type: object)\n",
            "40: 'crude_Nembe' (type: object)\n",
            "41: 'crude_Total' (type: object)\n",
            "42: 'crude_Gas & LPG' (type: object)\n",
            "43: 'crude_Naptha ' (type: object)\n",
            "44: 'crude_Kerosene' (type: object)\n",
            "45: 'crude_Light Diesel' (type: object)\n",
            "46: 'crude_Heavy Diesel' (type: object)\n",
            "47: 'crude_RCO' (type: object)\n",
            "48: 'Train_A_Raw_Crude_Flow' (type: object)\n",
            "49: 'Train_B_Raw_Crude_Flow' (type: object)\n",
            "50: 'Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU' (type: object)\n",
            "51: 'Rich_Gas_Knockout_Drum_Liquid_to_CDU' (type: object)\n",
            "52: '100FI2901' (type: object)\n",
            "53: '100FI2902' (type: object)\n",
            "54: '100FI4001' (type: object)\n",
            "55: '100FI4002' (type: object)\n",
            "56: '100FI4003' (type: object)\n",
            "57: '101AI8501A' (type: object)\n",
            "58: '101AI8501B' (type: object)\n",
            "59: '101AI8501C' (type: object)\n",
            "60: '101AI8501D' (type: object)\n",
            "61: '101AI8601A' (type: object)\n",
            "62: '101AI8601B' (type: object)\n",
            "63: '101AI8601C' (type: object)\n",
            "64: '101AI8601D' (type: object)\n",
            "65: '101AI8701A' (type: object)\n",
            "66: '101AI8701B' (type: object)\n",
            "67: '101AI8701C' (type: object)\n",
            "68: '101AI8701D' (type: object)\n",
            "69: '101FI1401' (type: object)\n",
            "70: 'Train_A_Raw_Crude_Flow.1' (type: object)\n",
            "71: 'Train_B_Raw_Crude_Flow.1' (type: object)\n",
            "72: '101FI3201' (type: object)\n",
            "73: '101FI3202' (type: object)\n",
            "74: '101FI3702' (type: object)\n",
            "75: '101FI6502' (type: object)\n",
            "76: '101FI8102' (type: object)\n",
            "77: '101FI8103' (type: object)\n",
            "78: '101FI8110' (type: object)\n",
            "79: '101FI8202' (type: object)\n",
            "80: '101FI8203' (type: object)\n",
            "81: '101FI8210' (type: object)\n",
            "82: '101FI8302' (type: object)\n",
            "83: '101FI8303' (type: object)\n",
            "84: '101FI8310' (type: object)\n",
            "85: '101FI8402' (type: object)\n",
            "86: '101FI8403' (type: object)\n",
            "87: '101FI8410' (type: object)\n",
            "88: '101FI8802' (type: object)\n",
            "89: '101FI8803' (type: object)\n",
            "90: '101FI8810' (type: object)\n",
            "91: '101FI8902' (type: object)\n",
            "92: '101FI8903' (type: object)\n",
            "93: '101FI8910' (type: object)\n",
            "94: '101FIC1404' (type: object)\n",
            "95: '101FIC1405' (type: object)\n",
            "96: '101FIC1701' (type: object)\n",
            "97: '101FIC2002' (type: object)\n",
            "98: '101FIC2004' (type: object)\n",
            "99: '101FIC2101' (type: object)\n",
            "100: '101FIC2201' (type: object)\n",
            "101: '101FIC2301' (type: object)\n",
            "102: '101FIC2401' (type: object)\n",
            "103: '101FIC2402' (type: object)\n",
            "104: '101FIC2501' (type: object)\n",
            "105: '101FIC2601' (type: object)\n",
            "106: '101FIC2801' (type: object)\n",
            "107: '101FIC2901' (type: object)\n",
            "108: '101FIC2902' (type: object)\n",
            "109: '101FIC3003' (type: object)\n",
            "110: '101FIC3102' (type: object)\n",
            "111: '101FIC3105' (type: object)\n",
            "112: '101FIC3501' (type: object)\n",
            "113: '101FIC3502' (type: object)\n",
            "114: '101FIC3602' (type: object)\n",
            "115: '101FIC3804' (type: object)\n",
            "116: '101FIC4401' (type: object)\n",
            "117: '101FIC4501' (type: object)\n",
            "118: '101FIC4502A' (type: object)\n",
            "119: '101FIC4502B' (type: object)\n",
            "120: '101FIC4502C' (type: object)\n",
            "121: '101FIC4502D' (type: object)\n",
            "122: '101FIC4502E' (type: object)\n",
            "123: '101FIC4502F' (type: object)\n",
            "124: '101FIC4504A' (type: object)\n",
            "125: '101FIC4504B' (type: object)\n",
            "126: '101FIC4504C' (type: object)\n",
            "127: '101FIC4504D' (type: object)\n",
            "128: '101FIC4504E' (type: object)\n",
            "129: '101FIC4504F' (type: object)\n",
            "130: '101FIC4601' (type: object)\n",
            "131: '101FIC4602A' (type: object)\n",
            "132: '101FIC4602B' (type: object)\n",
            "133: '101FIC4602C' (type: object)\n",
            "134: '101FIC4602D' (type: object)\n",
            "135: '101FIC4602E' (type: object)\n",
            "136: '101FIC4602F' (type: object)\n",
            "137: '101FIC4604A' (type: object)\n",
            "138: '101FIC4604B' (type: object)\n",
            "139: '101FIC4604C' (type: object)\n",
            "140: '101FIC4604D' (type: object)\n",
            "141: '101FIC4604E' (type: object)\n",
            "142: '101FIC4604F' (type: object)\n",
            "143: '101FIC4701' (type: object)\n",
            "144: '101FIC4703' (type: object)\n",
            "145: '101FIC4801' (type: object)\n",
            "146: '101FIC4903' (type: object)\n",
            "147: '101FIC5101' (type: object)\n",
            "148: '101FIC5202' (type: object)\n",
            "149: '101FIC5302' (type: object)\n",
            "150: '101FIC5402' (type: object)\n",
            "151: '101FIC5803' (type: object)\n",
            "152: '101FIC5804' (type: object)\n",
            "153: '101FIC5805' (type: object)\n",
            "154: '101FIC6102' (type: object)\n",
            "155: '101FIC6104' (type: object)\n",
            "156: '101FIC6301' (type: object)\n",
            "157: '101FIC6401' (type: object)\n",
            "158: '101FIC6704' (type: object)\n",
            "159: '101FIC6801' (type: object)\n",
            "160: '101FIC6802A' (type: object)\n",
            "161: '101FIC6802B' (type: object)\n",
            "162: '101FIC6802C' (type: object)\n",
            "163: '101FIC6802D' (type: object)\n",
            "164: '101FIC6802E' (type: object)\n",
            "165: '101FIC6802F' (type: object)\n",
            "166: '101FIC6804A' (type: object)\n",
            "167: '101FIC6804B' (type: object)\n",
            "168: '101FIC6804C' (type: object)\n",
            "169: '101FIC6804D' (type: object)\n",
            "170: '101FIC6804E' (type: object)\n",
            "171: '101FIC6804F' (type: object)\n",
            "172: '101FIC8101' (type: object)\n",
            "173: '101FIC8109' (type: object)\n",
            "174: '101FIC8201' (type: object)\n",
            "175: '101FIC8209' (type: object)\n",
            "176: '101FIC8301' (type: object)\n",
            "177: '101FIC8309' (type: object)\n",
            "178: '101FIC8401' (type: object)\n",
            "179: '101FIC8409' (type: object)\n",
            "180: '101FIC8801' (type: object)\n",
            "181: '101FIC8809' (type: object)\n",
            "182: '101FIC8901' (type: object)\n",
            "183: '101FIC8909' (type: object)\n",
            "184: '101FV8103' (type: object)\n",
            "185: '101FV8109' (type: object)\n",
            "186: '101FV8203' (type: object)\n",
            "187: '101FV8209' (type: object)\n",
            "188: '101FV8303' (type: object)\n",
            "189: '101FV8309' (type: object)\n",
            "190: '101FV8403' (type: object)\n",
            "191: '101FV8409' (type: object)\n",
            "192: '101FV8803' (type: object)\n",
            "193: '101FV8809' (type: object)\n",
            "194: '101FV8903' (type: object)\n",
            "195: '101FV8909' (type: object)\n",
            "196: '101PDI4903' (type: object)\n",
            "197: '101PDI5101' (type: object)\n",
            "198: '101PDIC2201' (type: object)\n",
            "199: '101PDIC2501' (type: object)\n",
            "200: '101PDIC2601' (type: object)\n",
            "201: '101PDIC2801' (type: object)\n",
            "202: '101PI4702' (type: object)\n",
            "203: '101PI4904' (type: object)\n",
            "204: '101PI8104' (type: object)\n",
            "205: '101PI8204' (type: object)\n",
            "206: '101PI8304' (type: object)\n",
            "207: '101PI8404' (type: object)\n",
            "208: '101PI8804' (type: object)\n",
            "209: '101PI8904' (type: object)\n",
            "210: '101PIC3201' (type: object)\n",
            "211: '101PIC3202' (type: object)\n",
            "212: '101PIC3602' (type: object)\n",
            "213: '101PIC3701' (type: object)\n",
            "214: '101PIC5701' (type: object)\n",
            "215: '101PIC6402' (type: object)\n",
            "216: '101PIC6501' (type: object)\n",
            "217: '101PIC8103' (type: object)\n",
            "218: '101PIC8109' (type: object)\n",
            "219: '101PIC8203' (type: object)\n",
            "220: '101PIC8209' (type: object)\n",
            "221: '101PIC8303' (type: object)\n",
            "222: '101PIC8309' (type: object)\n",
            "223: '101PIC8403' (type: object)\n",
            "224: '101PIC8409' (type: object)\n",
            "225: '101PIC8803' (type: object)\n",
            "226: '101PIC8809' (type: object)\n",
            "227: '101PIC8903' (type: object)\n",
            "228: '101PIC8909' (type: object)\n",
            "229: '101PV3701B' (type: object)\n",
            "230: '101PV5701B' (type: object)\n",
            "231: '101PV6501B' (type: object)\n",
            "232: '101TI1402' (type: object)\n",
            "233: '101TI1404' (type: object)\n",
            "234: '101TI1505' (type: object)\n",
            "235: '101TI1511' (type: object)\n",
            "236: '101TI1606' (type: object)\n",
            "237: '101TI1612' (type: object)\n",
            "238: '101TI1702' (type: object)\n",
            "239: '101TI1703' (type: object)\n",
            "240: '101TI1708' (type: object)\n",
            "241: '101TI1712' (type: object)\n",
            "242: '101TI1802' (type: object)\n",
            "243: '101TI1804' (type: object)\n",
            "244: '101TI1810' (type: object)\n",
            "245: '101TI1812' (type: object)\n",
            "246: '101TI1902' (type: object)\n",
            "247: '101TI1905' (type: object)\n",
            "248: '101TI1910' (type: object)\n",
            "249: '101TI1912' (type: object)\n",
            "250: '101TI2002' (type: object)\n",
            "251: '101TI2005' (type: object)\n",
            "252: '101TI2102' (type: object)\n",
            "253: '101TI2201' (type: object)\n",
            "254: '101TI2504' (type: object)\n",
            "255: '101TI2601' (type: object)\n",
            "256: '101TI2805' (type: object)\n",
            "257: '101TI2902' (type: object)\n",
            "258: '101TI2904' (type: object)\n",
            "259: '101TI3001' (type: object)\n",
            "260: '101TI3005' (type: object)\n",
            "261: '101TI3101' (type: object)\n",
            "262: '101TI3104' (type: object)\n",
            "263: '101TI3105' (type: object)\n",
            "264: '101TI3106' (type: object)\n",
            "265: '101TI3112' (type: object)\n",
            "266: '101TI3114' (type: object)\n",
            "267: '101TI3130' (type: object)\n",
            "268: '101TI3201' (type: object)\n",
            "269: '101TI3203' (type: object)\n",
            "270: '101TI3207' (type: object)\n",
            "271: '101TI3211' (type: object)\n",
            "272: '101TI3212' (type: object)\n",
            "273: '101TI3302' (type: object)\n",
            "274: '101TI3304' (type: object)\n",
            "275: '101TI3306' (type: object)\n",
            "276: '101TI3309' (type: object)\n",
            "277: '101TI3311' (type: object)\n",
            "278: '101TI3402' (type: object)\n",
            "279: '101TI3404' (type: object)\n",
            "280: '101TI3408' (type: object)\n",
            "281: '101TI3410' (type: object)\n",
            "282: '101TI3412' (type: object)\n",
            "283: '101TI3414' (type: object)\n",
            "284: '101TI3418' (type: object)\n",
            "285: '101TI3422' (type: object)\n",
            "286: '101TI3424' (type: object)\n",
            "287: '101TI3602' (type: object)\n",
            "288: '101TI3604' (type: object)\n",
            "289: '101TI3704' (type: object)\n",
            "290: '101TI3904' (type: object)\n",
            "291: '101TI3912' (type: object)\n",
            "292: '101TI4002' (type: object)\n",
            "293: '101TI4004' (type: object)\n",
            "294: '101TI4008' (type: object)\n",
            "295: '101TI4010' (type: object)\n",
            "296: '101TI4110' (type: object)\n",
            "297: '101TI4112' (type: object)\n",
            "298: '101TI4115' (type: object)\n",
            "299: '101TI4204' (type: object)\n",
            "300: '101TI4207' (type: object)\n",
            "301: '101TI4209' (type: object)\n",
            "302: '101TI4211' (type: object)\n",
            "303: '101TI4213' (type: object)\n",
            "304: '101TI4306' (type: object)\n",
            "305: '101TI4307' (type: object)\n",
            "306: '101TI4312' (type: object)\n",
            "307: '101TI4313' (type: object)\n",
            "308: '101TI4323' (type: object)\n",
            "309: '101TI4325' (type: object)\n",
            "310: '101TI4401' (type: object)\n",
            "311: '101TI4410' (type: object)\n",
            "312: '101TI4412' (type: object)\n",
            "313: '101TI4421' (type: object)\n",
            "314: '101TI4424' (type: object)\n",
            "315: '101TI4425' (type: object)\n",
            "316: '101TI4427' (type: object)\n",
            "317: '101TI4703' (type: object)\n",
            "318: '101TI4704' (type: object)\n",
            "319: '101TI4706' (type: object)\n",
            "320: '101TI4708' (type: object)\n",
            "321: '101TI4718' (type: object)\n",
            "322: '101TI4808' (type: object)\n",
            "323: '101TI4903' (type: object)\n",
            "324: '101TI4904' (type: object)\n",
            "325: '101TI4905' (type: object)\n",
            "326: '101TI4906' (type: object)\n",
            "327: '101TI4909' (type: object)\n",
            "328: '101TI4912' (type: object)\n",
            "329: '101TI5202' (type: object)\n",
            "330: '101TI5301' (type: object)\n",
            "331: '101TI5401' (type: object)\n",
            "332: '101TI5501' (type: object)\n",
            "333: '101TI5704' (type: object)\n",
            "334: '101TI6105' (type: object)\n",
            "335: '101TI6201' (type: object)\n",
            "336: '101TI6301' (type: object)\n",
            "337: '101TI6304' (type: object)\n",
            "338: '101TI6306' (type: object)\n",
            "339: '101TI6402' (type: object)\n",
            "340: '101TI6404' (type: object)\n",
            "341: '101TI6504' (type: object)\n",
            "342: '101TI8507' (type: object)\n",
            "343: '101TI8514' (type: object)\n",
            "344: '101TI8607' (type: object)\n",
            "345: '101TI8614' (type: object)\n",
            "346: '101TI8707' (type: object)\n",
            "347: '101TI8714' (type: object)\n",
            "348: '101TIC4542' (type: object)\n",
            "349: '101TIC4544' (type: object)\n",
            "350: '101TIC4622' (type: object)\n",
            "351: '101TIC4644' (type: object)\n",
            "352: '101TIC6822' (type: object)\n",
            "353: '101TIC6844' (type: object)\n",
            "354: '101TXI4505A' (type: object)\n",
            "355: '101TXI4505B' (type: object)\n",
            "356: '101TXI4505C' (type: object)\n",
            "357: '101TXI4505D' (type: object)\n",
            "358: '101TXI4505E' (type: object)\n",
            "359: '101TXI4505F' (type: object)\n",
            "360: '101TXI4506A' (type: object)\n",
            "361: '101TXI4506B' (type: object)\n",
            "362: '101TXI4506C' (type: object)\n",
            "363: '101TXI4506D' (type: object)\n",
            "364: '101TXI4506E' (type: object)\n",
            "365: '101TXI4506F' (type: object)\n",
            "366: '101TXI4507A' (type: object)\n",
            "367: '101TXI4507B' (type: object)\n",
            "368: '101TXI4507C' (type: object)\n",
            "369: '101TXI4507D' (type: object)\n",
            "370: '101TXI4507E' (type: object)\n",
            "371: '101TXI4507F' (type: object)\n",
            "372: '101TXI4508A' (type: object)\n",
            "373: '101TXI4508B' (type: object)\n",
            "374: '101TXI4508C' (type: object)\n",
            "375: '101TXI4508D' (type: object)\n",
            "376: '101TXI4508E' (type: object)\n",
            "377: '101TXI4508F' (type: object)\n",
            "378: '101TXI4509A' (type: object)\n",
            "379: '101TXI4509B' (type: object)\n",
            "380: '101TXI4509C' (type: object)\n",
            "381: '101TXI4509D' (type: object)\n",
            "382: '101TXI4509E' (type: object)\n",
            "383: '101TXI4509F' (type: object)\n",
            "384: '101TXI4510A' (type: object)\n",
            "385: '101TXI4510B' (type: object)\n",
            "386: '101TXI4510C' (type: object)\n",
            "387: '101TXI4510D' (type: object)\n",
            "388: '101TXI4510E' (type: object)\n",
            "389: '101TXI4510F' (type: object)\n",
            "390: '101TXI4605A' (type: object)\n",
            "391: '101TXI4605B' (type: object)\n",
            "392: '101TXI4605C' (type: object)\n",
            "393: '101TXI4605D' (type: object)\n",
            "394: '101TXI4605E' (type: object)\n",
            "395: '101TXI4605F' (type: object)\n",
            "396: '101TXI4606A' (type: object)\n",
            "397: '101TXI4606B' (type: object)\n",
            "398: '101TXI4606C' (type: object)\n",
            "399: '101TXI4606D' (type: object)\n",
            "400: '101TXI4606E' (type: object)\n",
            "401: '101TXI4606F' (type: object)\n",
            "402: '101TXI4607A' (type: object)\n",
            "403: '101TXI4607B' (type: object)\n",
            "404: '101TXI4607C' (type: object)\n",
            "405: '101TXI4607D' (type: object)\n",
            "406: '101TXI4607E' (type: object)\n",
            "407: '101TXI4607F' (type: object)\n",
            "408: '101TXI4608A' (type: object)\n",
            "409: '101TXI4608B' (type: object)\n",
            "410: '101TXI4608C' (type: object)\n",
            "411: '101TXI4608D' (type: object)\n",
            "412: '101TXI4608E' (type: object)\n",
            "413: '101TXI4608F' (type: object)\n",
            "414: '101TXI4609A' (type: object)\n",
            "415: '101TXI4609B' (type: object)\n",
            "416: '101TXI4609C' (type: object)\n",
            "417: '101TXI4609D' (type: object)\n",
            "418: '101TXI4609E' (type: object)\n",
            "419: '101TXI4609F' (type: object)\n",
            "420: '101TXI4610A' (type: object)\n",
            "421: '101TXI4610B' (type: object)\n",
            "422: '101TXI4610C' (type: object)\n",
            "423: '101TXI4610D' (type: object)\n",
            "424: '101TXI4610E' (type: object)\n",
            "425: '101TXI4610F' (type: object)\n",
            "426: '101TXI6805A' (type: object)\n",
            "427: '101TXI6805B' (type: object)\n",
            "428: '101TXI6805C' (type: object)\n",
            "429: '101TXI6805D' (type: object)\n",
            "430: '101TXI6805E' (type: object)\n",
            "431: '101TXI6805F' (type: object)\n",
            "432: '101TXI6806A' (type: object)\n",
            "433: '101TXI6806B' (type: object)\n",
            "434: '101TXI6806C' (type: object)\n",
            "435: '101TXI6806D' (type: object)\n",
            "436: '101TXI6806E' (type: object)\n",
            "437: '101TXI6806F' (type: object)\n",
            "438: '101TXI6807A' (type: object)\n",
            "439: '101TXI6807B' (type: object)\n",
            "440: '101TXI6807C' (type: object)\n",
            "441: '101TXI6807D' (type: object)\n",
            "442: '101TXI6807E' (type: object)\n",
            "443: '101TXI6807F' (type: object)\n",
            "444: '101TXI6808A' (type: object)\n",
            "445: '101TXI6808B' (type: object)\n",
            "446: '101TXI6808C' (type: object)\n",
            "447: '101TXI6808D' (type: object)\n",
            "448: '101TXI6808E' (type: object)\n",
            "449: '101TXI6808F' (type: object)\n",
            "450: '101TXI6809A' (type: object)\n",
            "451: '101TXI6809B' (type: object)\n",
            "452: '101TXI6809C' (type: object)\n",
            "453: '101TXI6809D' (type: object)\n",
            "454: '101TXI6809E' (type: object)\n",
            "455: '101TXI6809F' (type: object)\n",
            "456: '101TXI6810A' (type: object)\n",
            "457: '101TXI6810B' (type: object)\n",
            "458: '101TXI6810C' (type: object)\n",
            "459: '101TXI6810D' (type: object)\n",
            "460: '101TXI6810E' (type: object)\n",
            "461: '101TXI6810F' (type: object)\n",
            "462: '102FI1201' (type: object)\n",
            "463: '102FI1501' (type: object)\n",
            "464: '102FI1701' (type: object)\n",
            "465: '102FI1803' (type: object)\n",
            "466: '102FI2302' (type: object)\n",
            "467: '102FI2401' (type: object)\n",
            "468: '102FI2901' (type: object)\n",
            "469: '102FIC2101' (type: object)\n",
            "470: '102FIC2501' (type: object)\n",
            "471: 'Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU.1' (type: object)\n",
            "472: 'Rich_Gas_Knockout_Drum_Liquid_to_CDU.1' (type: object)\n",
            "473: '102FIC2801' (type: object)\n",
            "474: '102FIC2802' (type: object)\n",
            "475: '102FIC2804' (type: object)\n",
            "476: '102FIC2902' (type: object)\n",
            "477: '102FIC3001' (type: object)\n",
            "478: '102FIC3101' (type: object)\n",
            "479: '102FIC3301' (type: object)\n",
            "480: '102FIC3401' (type: object)\n",
            "481: '102FIC3801' (type: object)\n",
            "482: '102FIC3802' (type: object)\n",
            "483: '102FIC3803' (type: object)\n",
            "484: '102FIC4201' (type: object)\n",
            "485: '102FIC4202' (type: object)\n",
            "486: '102FIC4401' (type: object)\n",
            "487: '102PI1701' (type: object)\n",
            "488: '102PI1801' (type: object)\n",
            "489: '102PI1803' (type: object)\n",
            "490: '102PI2001' (type: object)\n",
            "491: '102PI2002' (type: object)\n",
            "492: '102PI2901' (type: object)\n",
            "493: '102PI3201' (type: object)\n",
            "494: '102PIC2402' (type: object)\n",
            "495: '102PV2402B' (type: object)\n",
            "496: '102TI1601' (type: object)\n",
            "497: '102TI1616' (type: object)\n",
            "498: '102TI1801' (type: object)\n",
            "499: '102TI1802' (type: object)\n",
            "500: '102TI1902' (type: object)\n",
            "501: '102TI1905' (type: object)\n",
            "502: '102TI1910' (type: object)\n",
            "503: '102TI1913' (type: object)\n",
            "504: '102TI2106' (type: object)\n",
            "505: '102TI2108' (type: object)\n",
            "506: '102TI2112' (type: object)\n",
            "507: '102TI2202' (type: object)\n",
            "508: '102TI2205' (type: object)\n",
            "509: '102TI2301' (type: object)\n",
            "510: '102TI2604' (type: object)\n",
            "511: '102TI2822' (type: object)\n",
            "512: '102TI2901' (type: object)\n",
            "513: '102TI2902' (type: object)\n",
            "514: '102TI2903' (type: object)\n",
            "515: '102TI3001' (type: object)\n",
            "516: '102TI3003' (type: object)\n",
            "517: '102TI3101' (type: object)\n",
            "518: '102TI3103' (type: object)\n",
            "519: '102TI3201' (type: object)\n",
            "520: '102TI3205' (type: object)\n",
            "521: '102TI3901' (type: object)\n",
            "522: '103FI1503A' (type: object)\n",
            "523: '103FI1802' (type: object)\n",
            "524: '103FIC1601' (type: object)\n",
            "525: '103FIC1801' (type: object)\n",
            "526: '103FIC1803' (type: object)\n",
            "527: '103FIC1804' (type: object)\n",
            "528: '103FIC1901' (type: object)\n",
            "529: '103FIC2102' (type: object)\n",
            "530: '103LIC1803' (type: object)\n",
            "531: '103PDI1603' (type: object)\n",
            "532: '103TI1601' (type: object)\n",
            "533: '103TI1602' (type: object)\n",
            "534: '103TI1901' (type: object)\n",
            "535: '103TI1905' (type: object)\n",
            "536: '104FI1501' (type: object)\n",
            "537: '104FI2201' (type: object)\n",
            "538: '104FI2202' (type: object)\n",
            "539: '104FI2301' (type: object)\n",
            "540: '104FI2401' (type: object)\n",
            "541: '104FIC1503' (type: object)\n",
            "542: '104FIC1701' (type: object)\n",
            "543: '104FIC1801' (type: object)\n",
            "544: '104FIC2901' (type: object)\n",
            "545: '104II1701' (type: object)\n",
            "546: '104II1801' (type: object)\n",
            "547: '104LIC1703' (type: object)\n",
            "548: '104LIC1803' (type: object)\n",
            "549: '104LIC2301' (type: object)\n",
            "550: '104LIC2401' (type: object)\n",
            "551: '104PDI1501' (type: object)\n",
            "552: '104PDI1601' (type: object)\n",
            "553: '104PDI2001' (type: object)\n",
            "554: '104PDI2101' (type: object)\n",
            "555: '104PDI2301' (type: object)\n",
            "556: '104PDI2401' (type: object)\n",
            "557: '104PDI2701' (type: object)\n",
            "558: '104PDI2702' (type: object)\n",
            "559: '104PDI2801' (type: object)\n",
            "560: '104PDI2802' (type: object)\n",
            "561: '104TI1201' (type: object)\n",
            "562: '104VI1701' (type: object)\n",
            "563: '104VI1801' (type: object)\n",
            "564: '112FI1301' (type: object)\n",
            "565: '114FIC1406' (type: object)\n",
            "566: '114FIC1702' (type: object)\n",
            "567: '121FI1502' (type: object)\n",
            "568: '121FIC10501' (type: object)\n",
            "569: '151FIC2801' (type: object)\n",
            "570: '152FIC3402' (type: object)\n",
            "571: '101AI8502A' (type: object)\n",
            "572: '101AI8502B' (type: object)\n",
            "573: '101AI8502C' (type: object)\n",
            "574: '101AI8502D' (type: object)\n",
            "575: '101AI8602A' (type: object)\n",
            "576: '101AI8602B' (type: object)\n",
            "577: '101AI8602C' (type: object)\n",
            "578: '101AI8602D' (type: object)\n",
            "579: '101AI8702A' (type: object)\n",
            "580: '101AI8702B' (type: object)\n",
            "581: '101AI8702D' (type: object)\n",
            "582: '101AI8703C' (type: object)\n",
            "583: '101FQ1401' (type: object)\n",
            "584: '101FQ3001' (type: object)\n",
            "585: '101FQ3101' (type: object)\n",
            "586: '101FQ4201' (type: object)\n",
            "587: '101FQ5303' (type: object)\n",
            "588: '102FQ2801' (type: object)\n",
            "589: '102FQ4201' (type: object)\n",
            "590: '102FQ4401' (type: object)\n",
            "591: 'crude_Egina' (type: object)\n",
            "592: 'crude_Utapate' (type: object)\n",
            "593: 'crude_Okono' (type: object)\n",
            "594: 'crude_SAB' (type: object)\n",
            "595: 'crude_Buzios' (type: object)\n",
            "596: 'Raw_Crude_Pump_Discharge_Flow' (type: object)\n",
            "597: '100FI3201' (type: object)\n",
            "598: '100FI3301' (type: object)\n",
            "599: '100FIC8101' (type: object)\n",
            "600: '100TI8102' (type: object)\n",
            "601: '100TIC8101' (type: object)\n",
            "602: 'Raw_Crude_Pump_Discharge_Flow.1' (type: object)\n",
            "603: '101TI1814' (type: object)\n",
            "604: '101TI1817' (type: object)\n",
            "605: '101TI1914' (type: object)\n",
            "606: '101TI2309' (type: object)\n",
            "607: '101TI2311' (type: object)\n",
            "608: '101TI2313' (type: object)\n",
            "609: '101TI2707' (type: object)\n",
            "610: '101TI2710' (type: object)\n",
            "611: '101TI2712' (type: object)\n",
            "612: '101TI2905' (type: object)\n",
            "613: '101TI3702' (type: object)\n",
            "614: '101TI5702' (type: object)\n",
            "615: '101TI6502' (type: object)\n",
            "616: '101TIC3605' (type: object)\n",
            "617: '101TIC6405' (type: object)\n",
            "618: '102TI1201' (type: object)\n",
            "619: '102TI1205' (type: object)\n",
            "620: '102TI1603' (type: object)\n",
            "621: '102TI1617' (type: object)\n",
            "622: '102TI2403' (type: object)\n",
            "623: '102TI2405' (type: object)\n",
            "624: '102TI2701' (type: object)\n",
            "625: '102TI2704' (type: object)\n",
            "626: '102TI2801' (type: object)\n",
            "627: '102TI2804' (type: object)\n",
            "628: '102TI2810' (type: object)\n",
            "629: '102TI3802' (type: object)\n",
            "630: '102TI3804' (type: object)\n",
            "631: '102TI3903' (type: object)\n",
            "632: '102TI3905' (type: object)\n",
            "633: '102TI3908' (type: object)\n",
            "634: '102TI4018' (type: object)\n",
            "635: '102TI4203' (type: object)\n",
            "636: '102TI4205' (type: object)\n",
            "637: '102TI4403' (type: object)\n",
            "638: '102TI4405' (type: object)\n",
            "639: '102TI4501' (type: object)\n",
            "640: '102TI4503' (type: object)\n",
            "641: '103TI2501' (type: object)\n",
            "642: '103TIC2504' (type: object)\n",
            "643: '104TI2001' (type: object)\n",
            "644: '104TI2003' (type: object)\n",
            "645: '104TI2101' (type: object)\n",
            "646: '104TI2103' (type: object)\n",
            "647: 'crude_TUL' (type: object)\n",
            "648: 'crude_CEI' (type: object)\n",
            "649: 'crude_IRC' (type: object)\n",
            "650: 'crude_MEO' (type: object)\n",
            "651: 'crude_Cieba' (type: object)\n",
            "652: '100FQ4001' (type: object)\n",
            "653: '100FQ4002' (type: object)\n",
            "654: '100FQ4003' (type: object)\n",
            "655: '101FI8101' (type: object)\n",
            "656: '101FI8201' (type: object)\n",
            "657: '101FI8301' (type: object)\n",
            "658: '101FI8401' (type: object)\n",
            "659: '101FI8801' (type: object)\n",
            "660: '101FI8901' (type: object)\n",
            "661: 'crude_WTI Midland' (type: object)\n",
            "662: 'crude_MERO' (type: object)\n",
            "663: '101FIC3802' (type: object)\n",
            "664: '101FIC5801' (type: object)\n",
            "665: '101FIC6702' (type: object)\n",
            "666: 'MW' (type: object)\n",
            "‚úÖ Successfully configured:\n",
            "   - enc_in, dec_in: 0\n",
            "   - c_out (output dimension): 1\n",
            "   - Primary target column: 'Total_Stabilized_Naphtha_Product_Flowrate'\n",
            "   - All numeric columns (0): []\n",
            "   - Dataset shape: (0, 0)\n",
            "üîç Verifying data before training...\n",
            "‚úÖ Target column 'Total_Stabilized_Naphtha_Product_Flowrate' verified in dataset\n",
            "‚ö†Ô∏è  Warning: No 'date' column found. Available columns:\n",
            "['Total_Stabilized_Naphtha_Product_Flowrate', 'Total_Kerosene_Product_Flowrate', 'Jet_Fuel_Product_Train1_Flowrate', 'Total_Light_Diesel_Product_Flowrate', 'Total_Heavy_Diesel_Product_Flowrate', 'Total_Atmospheric_Residue_Flowrate', 'Blend_Yield_Gas & LPG', 'Blend_Yield_Kerosene', 'Blend_Yield_Light Diesel', 'Blend_Yield_Heavy Diesel', 'Blend_Yield_RCO', 'Light_Diesel_to_DHT_Unit_Flowrate', 'Kerosene_to_Light_Diesel_DHT_Flowrate', 'Atm_Residue_to_RFCC_Unit_Flowrate', 'Atm_Residue_to_Storage_Flowrate', 'Crude_Column_Naphtha_to_SGCU_Flowrate', 'Heavy_Diesel_to_MHC_Flowrate', 'Atm_Residue_to_Storage_EE1027_Flowrate', 'blend_id', 'API', 'Sulphur', 'crude_AGB', 'crude_BOL', 'crude_CJB', 'crude_WCD', 'crude_QUI', 'crude_AME', 'crude_FOR', 'crude_ABO', 'crude_OK2', 'crude_ESC', 'crude_BOC', 'crude_BRL', 'crude_AKP', 'crude_ASC', 'crude_AGO', 'crude_YOH', 'crude_OKW', 'crude_ERH', 'crude_PAZ', 'crude_Nembe', 'crude_Total', 'crude_Gas & LPG', 'crude_Naptha ', 'crude_Kerosene', 'crude_Light Diesel', 'crude_Heavy Diesel', 'crude_RCO', 'Train_A_Raw_Crude_Flow', 'Train_B_Raw_Crude_Flow', 'Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU', 'Rich_Gas_Knockout_Drum_Liquid_to_CDU', '100FI2901', '100FI2902', '100FI4001', '100FI4002', '100FI4003', '101AI8501A', '101AI8501B', '101AI8501C', '101AI8501D', '101AI8601A', '101AI8601B', '101AI8601C', '101AI8601D', '101AI8701A', '101AI8701B', '101AI8701C', '101AI8701D', '101FI1401', 'Train_A_Raw_Crude_Flow.1', 'Train_B_Raw_Crude_Flow.1', '101FI3201', '101FI3202', '101FI3702', '101FI6502', '101FI8102', '101FI8103', '101FI8110', '101FI8202', '101FI8203', '101FI8210', '101FI8302', '101FI8303', '101FI8310', '101FI8402', '101FI8403', '101FI8410', '101FI8802', '101FI8803', '101FI8810', '101FI8902', '101FI8903', '101FI8910', '101FIC1404', '101FIC1405', '101FIC1701', '101FIC2002', '101FIC2004', '101FIC2101', '101FIC2201', '101FIC2301', '101FIC2401', '101FIC2402', '101FIC2501', '101FIC2601', '101FIC2801', '101FIC2901', '101FIC2902', '101FIC3003', '101FIC3102', '101FIC3105', '101FIC3501', '101FIC3502', '101FIC3602', '101FIC3804', '101FIC4401', '101FIC4501', '101FIC4502A', '101FIC4502B', '101FIC4502C', '101FIC4502D', '101FIC4502E', '101FIC4502F', '101FIC4504A', '101FIC4504B', '101FIC4504C', '101FIC4504D', '101FIC4504E', '101FIC4504F', '101FIC4601', '101FIC4602A', '101FIC4602B', '101FIC4602C', '101FIC4602D', '101FIC4602E', '101FIC4602F', '101FIC4604A', '101FIC4604B', '101FIC4604C', '101FIC4604D', '101FIC4604E', '101FIC4604F', '101FIC4701', '101FIC4703', '101FIC4801', '101FIC4903', '101FIC5101', '101FIC5202', '101FIC5302', '101FIC5402', '101FIC5803', '101FIC5804', '101FIC5805', '101FIC6102', '101FIC6104', '101FIC6301', '101FIC6401', '101FIC6704', '101FIC6801', '101FIC6802A', '101FIC6802B', '101FIC6802C', '101FIC6802D', '101FIC6802E', '101FIC6802F', '101FIC6804A', '101FIC6804B', '101FIC6804C', '101FIC6804D', '101FIC6804E', '101FIC6804F', '101FIC8101', '101FIC8109', '101FIC8201', '101FIC8209', '101FIC8301', '101FIC8309', '101FIC8401', '101FIC8409', '101FIC8801', '101FIC8809', '101FIC8901', '101FIC8909', '101FV8103', '101FV8109', '101FV8203', '101FV8209', '101FV8303', '101FV8309', '101FV8403', '101FV8409', '101FV8803', '101FV8809', '101FV8903', '101FV8909', '101PDI4903', '101PDI5101', '101PDIC2201', '101PDIC2501', '101PDIC2601', '101PDIC2801', '101PI4702', '101PI4904', '101PI8104', '101PI8204', '101PI8304', '101PI8404', '101PI8804', '101PI8904', '101PIC3201', '101PIC3202', '101PIC3602', '101PIC3701', '101PIC5701', '101PIC6402', '101PIC6501', '101PIC8103', '101PIC8109', '101PIC8203', '101PIC8209', '101PIC8303', '101PIC8309', '101PIC8403', '101PIC8409', '101PIC8803', '101PIC8809', '101PIC8903', '101PIC8909', '101PV3701B', '101PV5701B', '101PV6501B', '101TI1402', '101TI1404', '101TI1505', '101TI1511', '101TI1606', '101TI1612', '101TI1702', '101TI1703', '101TI1708', '101TI1712', '101TI1802', '101TI1804', '101TI1810', '101TI1812', '101TI1902', '101TI1905', '101TI1910', '101TI1912', '101TI2002', '101TI2005', '101TI2102', '101TI2201', '101TI2504', '101TI2601', '101TI2805', '101TI2902', '101TI2904', '101TI3001', '101TI3005', '101TI3101', '101TI3104', '101TI3105', '101TI3106', '101TI3112', '101TI3114', '101TI3130', '101TI3201', '101TI3203', '101TI3207', '101TI3211', '101TI3212', '101TI3302', '101TI3304', '101TI3306', '101TI3309', '101TI3311', '101TI3402', '101TI3404', '101TI3408', '101TI3410', '101TI3412', '101TI3414', '101TI3418', '101TI3422', '101TI3424', '101TI3602', '101TI3604', '101TI3704', '101TI3904', '101TI3912', '101TI4002', '101TI4004', '101TI4008', '101TI4010', '101TI4110', '101TI4112', '101TI4115', '101TI4204', '101TI4207', '101TI4209', '101TI4211', '101TI4213', '101TI4306', '101TI4307', '101TI4312', '101TI4313', '101TI4323', '101TI4325', '101TI4401', '101TI4410', '101TI4412', '101TI4421', '101TI4424', '101TI4425', '101TI4427', '101TI4703', '101TI4704', '101TI4706', '101TI4708', '101TI4718', '101TI4808', '101TI4903', '101TI4904', '101TI4905', '101TI4906', '101TI4909', '101TI4912', '101TI5202', '101TI5301', '101TI5401', '101TI5501', '101TI5704', '101TI6105', '101TI6201', '101TI6301', '101TI6304', '101TI6306', '101TI6402', '101TI6404', '101TI6504', '101TI8507', '101TI8514', '101TI8607', '101TI8614', '101TI8707', '101TI8714', '101TIC4542', '101TIC4544', '101TIC4622', '101TIC4644', '101TIC6822', '101TIC6844', '101TXI4505A', '101TXI4505B', '101TXI4505C', '101TXI4505D', '101TXI4505E', '101TXI4505F', '101TXI4506A', '101TXI4506B', '101TXI4506C', '101TXI4506D', '101TXI4506E', '101TXI4506F', '101TXI4507A', '101TXI4507B', '101TXI4507C', '101TXI4507D', '101TXI4507E', '101TXI4507F', '101TXI4508A', '101TXI4508B', '101TXI4508C', '101TXI4508D', '101TXI4508E', '101TXI4508F', '101TXI4509A', '101TXI4509B', '101TXI4509C', '101TXI4509D', '101TXI4509E', '101TXI4509F', '101TXI4510A', '101TXI4510B', '101TXI4510C', '101TXI4510D', '101TXI4510E', '101TXI4510F', '101TXI4605A', '101TXI4605B', '101TXI4605C', '101TXI4605D', '101TXI4605E', '101TXI4605F', '101TXI4606A', '101TXI4606B', '101TXI4606C', '101TXI4606D', '101TXI4606E', '101TXI4606F', '101TXI4607A', '101TXI4607B', '101TXI4607C', '101TXI4607D', '101TXI4607E', '101TXI4607F', '101TXI4608A', '101TXI4608B', '101TXI4608C', '101TXI4608D', '101TXI4608E', '101TXI4608F', '101TXI4609A', '101TXI4609B', '101TXI4609C', '101TXI4609D', '101TXI4609E', '101TXI4609F', '101TXI4610A', '101TXI4610B', '101TXI4610C', '101TXI4610D', '101TXI4610E', '101TXI4610F', '101TXI6805A', '101TXI6805B', '101TXI6805C', '101TXI6805D', '101TXI6805E', '101TXI6805F', '101TXI6806A', '101TXI6806B', '101TXI6806C', '101TXI6806D', '101TXI6806E', '101TXI6806F', '101TXI6807A', '101TXI6807B', '101TXI6807C', '101TXI6807D', '101TXI6807E', '101TXI6807F', '101TXI6808A', '101TXI6808B', '101TXI6808C', '101TXI6808D', '101TXI6808E', '101TXI6808F', '101TXI6809A', '101TXI6809B', '101TXI6809C', '101TXI6809D', '101TXI6809E', '101TXI6809F', '101TXI6810A', '101TXI6810B', '101TXI6810C', '101TXI6810D', '101TXI6810E', '101TXI6810F', '102FI1201', '102FI1501', '102FI1701', '102FI1803', '102FI2302', '102FI2401', '102FI2901', '102FIC2101', '102FIC2501', 'Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU.1', 'Rich_Gas_Knockout_Drum_Liquid_to_CDU.1', '102FIC2801', '102FIC2802', '102FIC2804', '102FIC2902', '102FIC3001', '102FIC3101', '102FIC3301', '102FIC3401', '102FIC3801', '102FIC3802', '102FIC3803', '102FIC4201', '102FIC4202', '102FIC4401', '102PI1701', '102PI1801', '102PI1803', '102PI2001', '102PI2002', '102PI2901', '102PI3201', '102PIC2402', '102PV2402B', '102TI1601', '102TI1616', '102TI1801', '102TI1802', '102TI1902', '102TI1905', '102TI1910', '102TI1913', '102TI2106', '102TI2108', '102TI2112', '102TI2202', '102TI2205', '102TI2301', '102TI2604', '102TI2822', '102TI2901', '102TI2902', '102TI2903', '102TI3001', '102TI3003', '102TI3101', '102TI3103', '102TI3201', '102TI3205', '102TI3901', '103FI1503A', '103FI1802', '103FIC1601', '103FIC1801', '103FIC1803', '103FIC1804', '103FIC1901', '103FIC2102', '103LIC1803', '103PDI1603', '103TI1601', '103TI1602', '103TI1901', '103TI1905', '104FI1501', '104FI2201', '104FI2202', '104FI2301', '104FI2401', '104FIC1503', '104FIC1701', '104FIC1801', '104FIC2901', '104II1701', '104II1801', '104LIC1703', '104LIC1803', '104LIC2301', '104LIC2401', '104PDI1501', '104PDI1601', '104PDI2001', '104PDI2101', '104PDI2301', '104PDI2401', '104PDI2701', '104PDI2702', '104PDI2801', '104PDI2802', '104TI1201', '104VI1701', '104VI1801', '112FI1301', '114FIC1406', '114FIC1702', '121FI1502', '121FIC10501', '151FIC2801', '152FIC3402', '101AI8502A', '101AI8502B', '101AI8502C', '101AI8502D', '101AI8602A', '101AI8602B', '101AI8602C', '101AI8602D', '101AI8702A', '101AI8702B', '101AI8702D', '101AI8703C', '101FQ1401', '101FQ3001', '101FQ3101', '101FQ4201', '101FQ5303', '102FQ2801', '102FQ4201', '102FQ4401', 'crude_Egina', 'crude_Utapate', 'crude_Okono', 'crude_SAB', 'crude_Buzios', 'Raw_Crude_Pump_Discharge_Flow', '100FI3201', '100FI3301', '100FIC8101', '100TI8102', '100TIC8101', 'Raw_Crude_Pump_Discharge_Flow.1', '101TI1814', '101TI1817', '101TI1914', '101TI2309', '101TI2311', '101TI2313', '101TI2707', '101TI2710', '101TI2712', '101TI2905', '101TI3702', '101TI5702', '101TI6502', '101TIC3605', '101TIC6405', '102TI1201', '102TI1205', '102TI1603', '102TI1617', '102TI2403', '102TI2405', '102TI2701', '102TI2704', '102TI2801', '102TI2804', '102TI2810', '102TI3802', '102TI3804', '102TI3903', '102TI3905', '102TI3908', '102TI4018', '102TI4203', '102TI4205', '102TI4403', '102TI4405', '102TI4501', '102TI4503', '103TI2501', '103TIC2504', '104TI2001', '104TI2003', '104TI2101', '104TI2103', 'crude_TUL', 'crude_CEI', 'crude_IRC', 'crude_MEO', 'crude_Cieba', '100FQ4001', '100FQ4002', '100FQ4003', '101FI8101', '101FI8201', '101FI8301', '101FI8401', '101FI8801', '101FI8901', 'crude_WTI Midland', 'crude_MERO', '101FIC3802', '101FIC5801', '101FIC6702', 'MW']\n",
            "\n",
            "üöÄ Starting Autoformer experiment...\n",
            "Use GPU: cuda:0\n",
            "üìä Start training...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "list.remove(x): x not in list",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-299914997.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExp_Main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìä Start training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'autoformer_custom_multivariate_targets'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Updated setting name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üíæ Training completed! Saving model to drive...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/exp/exp_main.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mvali_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvali_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/exp/exp_main.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/data_provider/data_factory.py\u001b[0m in \u001b[0;36mdata_provider\u001b[0;34m(args, flag)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     data_set = Data(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/data_provider/data_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_path, flag, size, features, data_path, target, scale, timeenc, freq)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__read_data__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__read_data__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/data_provider/data_loader.py\u001b[0m in \u001b[0;36m__read_data__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# print(cols)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
          ]
        }
      ],
      "source": [
        "from exp.exp_main import Exp_Main\n",
        "import argparse\n",
        "import torch\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd # Import pandas to read the CSV and get column count\n",
        "import numpy as np # Import numpy for numeric check\n",
        "\n",
        "# First, let's check the dataset size to determine appropriate parameters\n",
        "print(\"üîç Checking dataset size and determining optimal parameters...\")\n",
        "df_size_check = pd.read_csv('./dataset/custom/custom.csv')\n",
        "dataset_size = len(df_size_check)\n",
        "print(f\"Dataset size: {dataset_size} rows\")\n",
        "\n",
        "# Calculate optimal parameters based on dataset size\n",
        "# Rule of thumb: seq_len + label_len + pred_len should be < dataset_size\n",
        "# Leave some buffer for train/val/test split\n",
        "max_sequence_length = max(1, min(60, dataset_size // 4))  # Use 1/4 of dataset or max 60\n",
        "max_label_length = max(1, min(24, dataset_size // 8))     # Use 1/8 of dataset or max 24\n",
        "max_pred_length = max(1, min(7, dataset_size // 12))      # Use 1/12 of dataset or max 7\n",
        "\n",
        "# Ensure we have enough data for training\n",
        "min_required_samples = max_sequence_length + max_label_length + max_pred_length + 10  # +10 buffer\n",
        "if dataset_size < min_required_samples:\n",
        "    print(f\"‚ö†Ô∏è  Dataset size ({dataset_size}) is too small for the calculated parameters.\")\n",
        "    print(f\"   Minimum required: {min_required_samples}\")\n",
        "    print(f\"   Adjusting parameters to fit available data...\")\n",
        "    \n",
        "    # Use more conservative parameters\n",
        "    max_sequence_length = max(1, dataset_size // 6)\n",
        "    max_label_length = max(1, dataset_size // 12)\n",
        "    max_pred_length = max(1, dataset_size // 20)\n",
        "    \n",
        "    print(f\"   Adjusted seq_len: {max_sequence_length}\")\n",
        "    print(f\"   Adjusted label_len: {max_label_length}\")\n",
        "    print(f\"   Adjusted pred_len: {max_pred_length}\")\n",
        "\n",
        "print(f\"‚úÖ Using parameters:\")\n",
        "print(f\"   - seq_len: {max_sequence_length}\")\n",
        "print(f\"   - label_len: {max_label_length}\")\n",
        "print(f\"   - pred_len: {max_pred_length}\")\n",
        "print(f\"   - Total sequence requirement: {max_sequence_length + max_label_length + max_pred_length}\")\n",
        "\n",
        "# Arguments for Autoformer training and prediction\n",
        "args = argparse.Namespace(\n",
        "    is_training=1,\n",
        "    model_id='autoformer_custom_multivariate_targets', # Changed model_id for clarity\n",
        "    model='Autoformer',\n",
        "    data='custom',\n",
        "    root_path='./dataset/custom/',\n",
        "    data_path='custom.csv',\n",
        "    features='M', # Multivariate features\n",
        "    seq_len=max_sequence_length,         # Dynamically calculated based on dataset size\n",
        "    label_len=max_label_length,          # Dynamically calculated based on dataset size\n",
        "    pred_len=max_pred_length,           # Dynamically calculated based on dataset size\n",
        "    e_layers=1,\n",
        "    d_layers=1,\n",
        "    factor=3,\n",
        "    # enc_in, dec_in will be determined dynamically based on all columns except date and non-numeric\n",
        "    # c_out will be set to the number of target columns for multivariate output\n",
        "    d_model=64,\n",
        "    n_heads=8,\n",
        "    d_ff=128,\n",
        "    activation='gelu',\n",
        "    description='autoformer multivariate target forecasting', # Updated description\n",
        "    itr=1,\n",
        "    train_epochs=10,     # keep small for demo\n",
        "    batch_size=min(16, dataset_size // 10),  # Adjust batch size based on dataset size\n",
        "    learning_rate=0.001,\n",
        "    moving_avg=25,\n",
        "    freq='d',\n",
        "    dropout=0.3,\n",
        "    embed='timeF',\n",
        "    patience=3,\n",
        "    checkpoint='./checkpoints/',\n",
        "    output_attention=False,\n",
        "    do_predict=True,\n",
        "    # Add GPU arguments\n",
        "    use_gpu=torch.cuda.is_available(), # Check if GPU is available\n",
        "    gpu=0, # Specify GPU id, 0 by default\n",
        "    use_multi_gpu=False, # Set to True if using multiple GPUs\n",
        "    devices=[0], # Specify device ids if using multi-gpu\n",
        "    # Add the 'target' argument back, set to the first target column name.\n",
        "    # The library's data_provider might still need this, even for multivariate features.\n",
        "    target='Total_Stabilized_Naphtha_Product_Flowrate',\n",
        "    num_workers=0, # Add num_workers argument\n",
        "    # Add checkpoints attribute\n",
        "    checkpoints= './checkpoints/',\n",
        "    # Add use_amp attribute\n",
        "    use_amp=False,\n",
        "    # Add lradj attribute\n",
        "    lradj='type1'\n",
        ")\n",
        "\n",
        "# Determine input/output dimensions dynamically\n",
        "try:\n",
        "    # First, check if the file exists and is readable\n",
        "    file_path = os.path.join(args.root_path, args.data_path)\n",
        "    print(f\"üîç Checking dataset file: {file_path}\")\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
        "    \n",
        "    # Check file size\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    print(f\"   - File size: {file_size} bytes\")\n",
        "    \n",
        "    if file_size == 0:\n",
        "        raise ValueError(f\"Dataset file is empty: {file_path}\")\n",
        "    \n",
        "    # Try to read the file\n",
        "    print(f\"   - Reading dataset...\")\n",
        "    df_temp = pd.read_csv(file_path)\n",
        "    print(f\"   - Successfully loaded dataset with {len(df_temp)} rows and {len(df_temp.columns)} columns\")\n",
        "\n",
        "    # Print all column names to debug\n",
        "    print(\"All columns in the dataset:\")\n",
        "    for i, col in enumerate(df_temp.columns):\n",
        "        print(f\"{i}: '{col}' (type: {df_temp[col].dtype})\")\n",
        "\n",
        "    # Verify that the dataset has the expected structure\n",
        "    if 'date' not in df_temp.columns:\n",
        "        raise ValueError(\"‚ùå Dataset must contain a 'date' column for Autoformer to work properly!\")\n",
        "    \n",
        "    print(f\"‚úÖ Found 'date' column in dataset\")\n",
        "\n",
        "    # Identify target columns from the list used in the data preparation cell\n",
        "    target_cols_list = [\n",
        "        'Total_Stabilized_Naphtha_Product_Flowrate',\n",
        "        'Total_Kerosene_Product_Flowrate',\n",
        "        'Jet_Fuel_Product_Train1_Flowrate',\n",
        "        'Total_Light_Diesel_Product_Flowrate',\n",
        "        'Total_Heavy_Diesel_Product_Flowrate',\n",
        "        'Total_Atmospheric_Residue_Flowrate',\n",
        "        'Blend_Yield_Gas & LPG',\n",
        "        'Blend_Yield_Kerosene',\n",
        "        'Blend_Yield_Light Diesel',\n",
        "        'Blend_Yield_Heavy Diesel',\n",
        "        'Blend_Yield_RCO'\n",
        "    ]\n",
        "\n",
        "    # Check if all specified target columns exist in the DataFrame\n",
        "    missing_targets = [col for col in target_cols_list if col not in df_temp.columns]\n",
        "    if missing_targets:\n",
        "        print(f\"‚ùå The following specified target columns were not found in the dataset: {missing_targets}\")\n",
        "        print(\"Available columns that might be similar:\")\n",
        "        for missing_col in missing_targets:\n",
        "            similar_cols = [col for col in df_temp.columns if missing_col.lower().replace('_', '').replace(' ', '') in col.lower().replace('_', '').replace(' ', '')]\n",
        "            if similar_cols:\n",
        "                print(f\"  For '{missing_col}', similar columns found: {similar_cols}\")\n",
        "\n",
        "        # Use only the target columns that exist\n",
        "        existing_targets = [col for col in target_cols_list if col in df_temp.columns]\n",
        "        if not existing_targets:\n",
        "            raise ValueError(\"No target columns found in the dataset!\")\n",
        "        target_cols_list = existing_targets\n",
        "        print(f\"Using existing target columns: {target_cols_list}\")\n",
        "\n",
        "    # CRITICAL FIX: Convert all columns to numeric types (except date)\n",
        "    print(\"\\nüîß Converting all columns to numeric types...\")\n",
        "    non_numeric_cols = []\n",
        "    for col in df_temp.columns:\n",
        "        if col != 'date':  # Skip date column\n",
        "            try:\n",
        "                # Convert to numeric, coercing errors to NaN\n",
        "                df_temp[col] = pd.to_numeric(df_temp[col], errors='coerce')\n",
        "            except Exception as e:\n",
        "                non_numeric_cols.append(col)\n",
        "                print(f\"‚ö†Ô∏è  Could not convert column '{col}' to numeric: {e}\")\n",
        "\n",
        "    if non_numeric_cols:\n",
        "        print(f\"‚ö†Ô∏è  Columns that could not be converted to numeric: {non_numeric_cols}\")\n",
        "        # Drop these columns if they can't be converted\n",
        "        df_temp = df_temp.drop(columns=non_numeric_cols)\n",
        "        print(f\"‚úÖ Dropped {len(non_numeric_cols)} non-numeric columns\")\n",
        "\n",
        "    # Handle missing values after conversion\n",
        "    print(\"üîß Handling missing values...\")\n",
        "    df_temp = df_temp.fillna(method='ffill').fillna(method='bfill')\n",
        "    \n",
        "    # Remove any rows that still have NaN values\n",
        "    initial_rows = len(df_temp)\n",
        "    df_temp = df_temp.dropna()\n",
        "    final_rows = len(df_temp)\n",
        "    if initial_rows != final_rows:\n",
        "        print(f\"‚ö†Ô∏è  Removed {initial_rows - final_rows} rows with NaN values\")\n",
        "\n",
        "    # Verify that the primary target column exists and set it correctly\n",
        "    if args.target not in df_temp.columns:\n",
        "        print(f\"Warning: Primary target column '{args.target}' not found in dataset.\")\n",
        "        if target_cols_list:\n",
        "            args.target = target_cols_list[0]\n",
        "            print(f\"Setting primary target to: '{args.target}'\")\n",
        "        else:\n",
        "            # Find the first numeric column as fallback\n",
        "            numeric_cols = df_temp.select_dtypes(include=np.number).columns.tolist()\n",
        "            if 'date' in numeric_cols:\n",
        "                numeric_cols.remove('date')\n",
        "            if numeric_cols:\n",
        "                args.target = numeric_cols[0]\n",
        "                print(f\"Using first numeric column as target: '{args.target}'\")\n",
        "            else:\n",
        "                raise ValueError(\"No suitable target column found!\")\n",
        "\n",
        "    # Identify columns to be used as features and targets (excluding 'date' and non-numeric, non-target columns)\n",
        "    # Also ensure target columns are numeric\n",
        "    numeric_cols = df_temp.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Remove 'date' from numeric columns if it exists there\n",
        "    if 'date' in numeric_cols:\n",
        "        numeric_cols.remove('date')\n",
        "\n",
        "    # Create list of all relevant columns: date + all numeric columns\n",
        "    all_relevant_cols = ['date'] + numeric_cols\n",
        "\n",
        "    # Filter df_temp to only include relevant columns that exist\n",
        "    existing_cols = [col for col in all_relevant_cols if col in df_temp.columns]\n",
        "    df_temp_filtered = df_temp[existing_cols].copy()\n",
        "\n",
        "    # The number of features (enc_in, dec_in) is the total number of numeric columns\n",
        "    num_features = len(numeric_cols)\n",
        "\n",
        "    args.enc_in = num_features\n",
        "    args.dec_in = num_features # Decoder input size matches encoder input size for multivariate features\n",
        "\n",
        "    # CRITICAL FIX: Configure for multivariate target prediction\n",
        "    # For multivariate targets, c_out should be the number of target columns\n",
        "    num_target_columns = len(target_cols_list)\n",
        "    args.c_out = num_target_columns  # Set to number of target columns for multivariate output\n",
        "    \n",
        "    print(f\"üîß Configuring for multivariate prediction:\")\n",
        "    print(f\"   - Number of target columns: {num_target_columns}\")\n",
        "    print(f\"   - Target columns: {target_cols_list}\")\n",
        "    print(f\"   - c_out (output dimension): {args.c_out}\")\n",
        "\n",
        "    print(f\"‚úÖ Successfully configured:\")\n",
        "    print(f\"   - enc_in, dec_in: {num_features}\")\n",
        "    print(f\"   - c_out (output dimension): {args.c_out}\")\n",
        "    print(f\"   - Primary target column: '{args.target}'\")\n",
        "    print(f\"   - All numeric columns ({len(numeric_cols)}): {numeric_cols[:10]}... (showing first 10)\")\n",
        "    print(f\"   - Dataset shape: {df_temp_filtered.shape}\")\n",
        "    \n",
        "    # Verify data types are correct\n",
        "    print(f\"\\nüîç Final data type verification:\")\n",
        "    print(f\"   - All columns are numeric: {all(df_temp[numeric_cols].dtypes.apply(lambda x: np.issubdtype(x, np.number)))}\")\n",
        "    print(f\"   - No NaN values: {not df_temp[numeric_cols].isnull().any().any()}\")\n",
        "    print(f\"   - Sample values from target column '{args.target}': {df_temp[args.target].head().tolist()}\")\n",
        "    \n",
        "    # CRITICAL: Validate that we have enough data for the sequence parameters\n",
        "    print(f\"\\nüîç Validating sequence parameters against dataset size:\")\n",
        "    total_sequence_need = args.seq_len + args.label_len + args.pred_len\n",
        "    available_samples = len(df_temp_filtered)\n",
        "    \n",
        "    print(f\"   - Required total sequence length: {total_sequence_need}\")\n",
        "    print(f\"   - Available samples: {available_samples}\")\n",
        "    print(f\"   - Buffer remaining: {available_samples - total_sequence_need}\")\n",
        "    \n",
        "    # Debug: Check if df_temp_filtered is empty\n",
        "    if available_samples == 0:\n",
        "        print(f\"‚ùå CRITICAL ERROR: df_temp_filtered is empty!\")\n",
        "        print(f\"   - Original df_temp shape: {df_temp.shape}\")\n",
        "        print(f\"   - df_temp columns: {list(df_temp.columns)}\")\n",
        "        print(f\"   - df_temp_filtered shape: {df_temp_filtered.shape}\")\n",
        "        print(f\"   - existing_cols: {existing_cols}\")\n",
        "        print(f\"   - numeric_cols: {numeric_cols}\")\n",
        "        \n",
        "        # Try to identify the issue\n",
        "        if len(existing_cols) == 0:\n",
        "            print(f\"   - Issue: No existing columns found!\")\n",
        "        if len(numeric_cols) == 0:\n",
        "            print(f\"   - Issue: No numeric columns found!\")\n",
        "        \n",
        "        # Try to recover by using all columns except date\n",
        "        print(f\"   - Attempting recovery by using all columns except date...\")\n",
        "        all_cols_except_date = [col for col in df_temp.columns if col != 'date']\n",
        "        if len(all_cols_except_date) > 0:\n",
        "            df_temp_filtered = df_temp[['date'] + all_cols_except_date].copy()\n",
        "            available_samples = len(df_temp_filtered)\n",
        "            print(f\"   - Recovery successful: {available_samples} samples available\")\n",
        "        else:\n",
        "            raise ValueError(\"Cannot recover: No columns available for training!\")\n",
        "    \n",
        "    if available_samples < total_sequence_need:\n",
        "        print(f\"‚ùå ERROR: Not enough data for the specified sequence parameters!\")\n",
        "        print(f\"   Need at least {total_sequence_need} samples, but only have {available_samples}\")\n",
        "        print(f\"   Please reduce seq_len, label_len, or pred_len parameters\")\n",
        "        raise ValueError(f\"Insufficient data: need {total_sequence_need} samples, have {available_samples}\")\n",
        "    \n",
        "    # Additional validation for train/val/test split\n",
        "    # Autoformer typically uses 70% train, 20% val, 10% test\n",
        "    train_samples = int(available_samples * 0.7)\n",
        "    val_samples = int(available_samples * 0.2)\n",
        "    test_samples = available_samples - train_samples - val_samples\n",
        "    \n",
        "    print(f\"   - Expected train samples: {train_samples}\")\n",
        "    print(f\"   - Expected val samples: {val_samples}\")\n",
        "    print(f\"   - Expected test samples: {test_samples}\")\n",
        "    \n",
        "    # Ensure we have enough samples for training after splitting\n",
        "    min_train_samples = total_sequence_need + 10  # +10 buffer\n",
        "    if train_samples < min_train_samples:\n",
        "        print(f\"‚ö†Ô∏è  WARNING: Training set may be too small after splitting!\")\n",
        "        print(f\"   Expected train samples: {train_samples}\")\n",
        "        print(f\"   Minimum recommended: {min_train_samples}\")\n",
        "        print(f\"   Consider reducing sequence parameters or using more data\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error determining input/output dimensions: {e}\")\n",
        "    # Fallback to default or raise error if necessary\n",
        "    args.enc_in = 1 # Fallback to 1 if unable to read file\n",
        "    args.dec_in = 1\n",
        "    args.c_out = 1\n",
        "    args.target = 'Unknown' # Set target to a placeholder\n",
        "    print(f\"Using default enc_in, dec_in, c_out: {args.enc_in}, {args.dec_in}, {args.c_out}\")\n",
        "    raise e  # Re-raise the error to stop execution\n",
        "\n",
        "\n",
        "def save_model_to_drive(exp, model_name=\"autoformer_model\", save_path=\"./saved_models/\"):\n",
        "    \"\"\"\n",
        "    Save the trained model and related information to drive\n",
        "\n",
        "    Args:\n",
        "        exp: Experiment object containing the trained model\n",
        "        model_name: Name for the saved model\n",
        "        save_path: Directory path to save the model\n",
        "    \"\"\"\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Create timestamped filename\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model_filename = f\"{model_name}_{timestamp}\"\n",
        "\n",
        "    # Save model state dict\n",
        "    model_path = os.path.join(save_path, f\"{model_filename}.pth\")\n",
        "    torch.save(exp.model.state_dict(), model_path)\n",
        "    print(f\"Model state dict saved to: {model_path}\")\n",
        "\n",
        "    # Save complete model (including architecture)\n",
        "    complete_model_path = os.path.join(save_path, f\"{model_filename}_complete.pth\")\n",
        "    torch.save(exp.model, complete_model_path)\n",
        "    print(f\"Complete model saved to: {complete_model_path}\")\n",
        "\n",
        "    # Save model configuration (args)\n",
        "    config_path = os.path.join(save_path, f\"{model_filename}_config.json\")\n",
        "    config_dict = vars(args)\n",
        "    # Convert non-serializable objects to strings\n",
        "    for key, value in config_dict.items():\n",
        "        if not isinstance(value, (int, float, str, bool, list, dict, type(None))):\n",
        "            config_dict[key] = str(value)\n",
        "\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config_dict, f, indent=4)\n",
        "    print(f\"Model configuration saved to: {config_path}\")\n",
        "\n",
        "    # Save experiment object (if needed for later use)\n",
        "    exp_path = os.path.join(save_path, f\"{model_filename}_experiment.pkl\")\n",
        "    try:\n",
        "        with open(exp_path, 'wb') as f:\n",
        "            pickle.dump(exp, f)\n",
        "        print(f\"Experiment object saved to: {exp_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save experiment object: {e}\")\n",
        "\n",
        "    # Create a summary file\n",
        "    summary_path = os.path.join(save_path, f\"{model_filename}_summary.txt\")\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(f\"Autoformer Model Summary\\n\")\n",
        "        f.write(f\"========================\\n\")\n",
        "        f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Model ID: {args.model_id}\\n\")\n",
        "        f.write(f\"Model Type: {args.model}\\n\")\n",
        "        f.write(f\"Training Epochs: {args.train_epochs}\\n\")\n",
        "        f.write(f\"Sequence Length: {args.seq_len}\\n\")\n",
        "        f.write(f\"Prediction Length: {args.pred_len}\\n\")\n",
        "        f.write(f\"Learning Rate: {args.learning_rate}\\n\")\n",
        "        f.write(f\"Batch Size: {args.batch_size}\\n\")\n",
        "        f.write(f\"GPU Used: {args.use_gpu}\\n\")\n",
        "        f.write(f\"Primary Target: {args.target}\\n\")\n",
        "        f.write(f\"Input Features: {args.enc_in}\\n\")\n",
        "        f.write(f\"Output Dimension: {args.c_out}\\n\")\n",
        "        f.write(f\"\\nFiles saved:\\n\")\n",
        "        f.write(f\"- Model state dict: {model_filename}.pth\\n\")\n",
        "        f.write(f\"- Complete model: {model_filename}_complete.pth\\n\")\n",
        "        f.write(f\"- Configuration: {model_filename}_config.json\\n\")\n",
        "        f.write(f\"- Summary: {model_filename}_summary.txt\\n\")\n",
        "\n",
        "    print(f\"Model summary saved to: {summary_path}\")\n",
        "    print(f\"\\nAll model files saved successfully in: {save_path}\")\n",
        "\n",
        "    return {\n",
        "        'model_path': model_path,\n",
        "        'complete_model_path': complete_model_path,\n",
        "        'config_path': config_path,\n",
        "        'summary_path': summary_path\n",
        "    }\n",
        "\n",
        "def load_saved_model(model_path, config_path, device=None):\n",
        "    \"\"\"\n",
        "    Load a previously saved model\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model state dict\n",
        "        config_path: Path to the saved configuration\n",
        "        device: Device to load the model on\n",
        "\n",
        "    Returns:\n",
        "        Loaded model and configuration\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load configuration\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Recreate args namespace\n",
        "    loaded_args = argparse.Namespace(**config)\n",
        "\n",
        "    # Create experiment object\n",
        "    exp = Exp_Main(loaded_args)\n",
        "\n",
        "    # Load model state dict\n",
        "    model_state = torch.load(model_path, map_location=device)\n",
        "    exp.model.load_state_dict(model_state)\n",
        "    exp.model.eval()\n",
        "\n",
        "    print(f\"Model loaded successfully from: {model_path}\")\n",
        "    return exp, loaded_args\n",
        "\n",
        "# Verify the target column exists before proceeding\n",
        "try:\n",
        "    print(\"üîç Verifying data before training...\")\n",
        "    df_verify = pd.read_csv(os.path.join(args.root_path, args.data_path))\n",
        "    if args.target not in df_verify.columns:\n",
        "        raise ValueError(f\"Target column '{args.target}' not found in the dataset columns: {list(df_verify.columns)}\")\n",
        "    print(f\"‚úÖ Target column '{args.target}' verified in dataset\")\n",
        "\n",
        "    # Check if there's a 'date' column\n",
        "    if 'date' not in df_verify.columns:\n",
        "        print(\"‚ö†Ô∏è  Warning: No 'date' column found. Available columns:\")\n",
        "        print(list(df_verify.columns))\n",
        "    else:\n",
        "        print(\"‚úÖ Date column found in dataset\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error verifying data: {e}\")\n",
        "    raise e\n",
        "\n",
        "# Run training and prediction\n",
        "print(\"\\nüöÄ Starting Autoformer experiment...\")\n",
        "exp = Exp_Main(args)\n",
        "print(\"üìä Start training...\")\n",
        "exp.train(setting='autoformer_custom_multivariate_targets') # Updated setting name\n",
        "\n",
        "print(\"üíæ Training completed! Saving model to drive...\")\n",
        "saved_files = save_model_to_drive(exp, model_name=\"autoformer_custom_multivariate_targets\", save_path=\"./saved_models/\") # Updated model name\n",
        "\n",
        "print(\"üîÆ Start predicting...\")\n",
        "# Modify the predict call if needed based on Autoformer's prediction output\n",
        "exp.predict(setting='autoformer_custom_multivariate_targets', load=True) # Updated setting name\n",
        "\n",
        "print(\"üéâ Training and prediction completed!\")\n",
        "print(f\"üìÅ Model saved to: {saved_files['model_path']}\")\n",
        "\n",
        "# Example of how to load the model later:\n",
        "# exp_loaded, args_loaded = load_saved_model(\n",
        "#     saved_files['model_path'],\n",
        "#     saved_files['config_path']\n",
        "# )\n",
        "# print(\"Model loaded successfully for future use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOx51FBrQr2i"
      },
      "source": [
        "MODEL EVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYECWUhbQugW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def get_test_predictions_manual(exp, args):\n",
        "    \"\"\"\n",
        "    Manual method to get test predictions when data_provider is not available\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a simple data loader for the test data\n",
        "        import pandas as pd\n",
        "        from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv(os.path.join(args.root_path, args.data_path))\n",
        "\n",
        "        # Simple train/test split (you may need to adjust this based on your data)\n",
        "        train_size = int(len(df) * 0.8)\n",
        "        test_df = df[train_size:]\n",
        "\n",
        "        print(f\"‚úì Manual data loading: {len(test_df)} test samples\")\n",
        "\n",
        "        # Simple dataset class\n",
        "        class SimpleDataset(Dataset):\n",
        "            def __init__(self, data, seq_len, pred_len):\n",
        "                self.data = data.values\n",
        "                self.seq_len = seq_len\n",
        "                self.pred_len = pred_len\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.data) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                seq_x = self.data[idx:idx + self.seq_len]\n",
        "                seq_y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len]\n",
        "                return torch.FloatTensor(seq_x), torch.FloatTensor(seq_y)\n",
        "\n",
        "        # Create dataset and dataloader\n",
        "        test_dataset = SimpleDataset(test_df, args.seq_len, args.pred_len)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = []\n",
        "        true_values = []\n",
        "\n",
        "        exp.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in test_loader:\n",
        "                if args.use_gpu:\n",
        "                    batch_x = batch_x.cuda()\n",
        "                    batch_y = batch_y.cuda()\n",
        "\n",
        "                # Simple prediction (you may need to adjust based on your model)\n",
        "                pred = exp.model(batch_x)\n",
        "\n",
        "                predictions.append(pred.cpu().numpy())\n",
        "                true_values.append(batch_y.cpu().numpy())\n",
        "\n",
        "        if predictions and true_values:\n",
        "            y_pred = np.concatenate(predictions, axis=0)\n",
        "            y_true = np.concatenate(true_values, axis=0)\n",
        "            return y_pred, y_true\n",
        "        else:\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Manual prediction method failed: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive evaluation metrics\n",
        "\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays if they're tensors\n",
        "    if torch.is_tensor(y_true):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "    if torch.is_tensor(y_pred):\n",
        "        y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "    # Flatten arrays if multidimensional\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "    # Remove any NaN values\n",
        "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
        "    y_true = y_true[mask]\n",
        "    y_pred = y_pred[mask]\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    # MAPE (Mean Absolute Percentage Error)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
        "\n",
        "    # MSPE (Mean Squared Percentage Error)\n",
        "    mspe = np.mean(((y_true - y_pred) / (y_true + 1e-8)) ** 2) * 100\n",
        "\n",
        "    # R¬≤ Score\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Directional Accuracy (for time series)\n",
        "    if len(y_true) > 1:\n",
        "        true_direction = np.sign(np.diff(y_true))\n",
        "        pred_direction = np.sign(np.diff(y_pred))\n",
        "        directional_accuracy = np.mean(true_direction == pred_direction) * 100\n",
        "    else:\n",
        "        directional_accuracy = 0\n",
        "\n",
        "    return {\n",
        "        'MSE': float(mse),\n",
        "        'RMSE': float(rmse),\n",
        "        'MAE': float(mae),\n",
        "        'MAPE': float(mape),\n",
        "        'MSPE': float(mspe),\n",
        "        'R2': float(r2),\n",
        "        'Directional_Accuracy': float(directional_accuracy)\n",
        "    }\n",
        "\n",
        "def create_evaluation_plots(y_true, y_pred, save_path=\"./evaluation_plots/\"):\n",
        "    \"\"\"\n",
        "    Create comprehensive evaluation plots\n",
        "\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "        save_path: Directory to save plots\n",
        "    \"\"\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Convert to numpy arrays if they're tensors\n",
        "    if torch.is_tensor(y_true):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "    if torch.is_tensor(y_pred):\n",
        "        y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "    # Flatten arrays if multidimensional\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Model Evaluation Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Actual vs Predicted (Time Series)\n",
        "    axes[0, 0].plot(y_true, label='Actual', color='blue', alpha=0.7)\n",
        "    axes[0, 0].plot(y_pred, label='Predicted', color='red', alpha=0.7)\n",
        "    axes[0, 0].set_title('Actual vs Predicted Values (Time Series)')\n",
        "    axes[0, 0].set_xlabel('Time Steps')\n",
        "    axes[0, 0].set_ylabel('Values')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Scatter Plot\n",
        "    axes[0, 1].scatter(y_true, y_pred, alpha=0.6, color='green')\n",
        "    axes[0, 1].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],\n",
        "                    'r--', lw=2, label='Perfect Prediction')\n",
        "    axes[0, 1].set_title('Scatter Plot: Actual vs Predicted')\n",
        "    axes[0, 1].set_xlabel('Actual Values')\n",
        "    axes[0, 1].set_ylabel('Predicted Values')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Residuals\n",
        "    residuals = y_true - y_pred\n",
        "    axes[1, 0].scatter(y_pred, residuals, alpha=0.6, color='orange')\n",
        "    axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[1, 0].set_title('Residual Plot')\n",
        "    axes[1, 0].set_xlabel('Predicted Values')\n",
        "    axes[1, 0].set_ylabel('Residuals')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Residuals Distribution\n",
        "    axes[1, 1].hist(residuals, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "    axes[1, 1].set_title('Distribution of Residuals')\n",
        "    axes[1, 1].set_xlabel('Residuals')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    plot_filename = os.path.join(save_path, f\"evaluation_plots_{timestamp}.png\")\n",
        "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Evaluation plots saved to: {plot_filename}\")\n",
        "    return plot_filename\n",
        "\n",
        "def comprehensive_model_evaluation(exp, args, save_results=True):\n",
        "    \"\"\"\n",
        "    Perform comprehensive evaluation of the trained model\n",
        "\n",
        "    Args:\n",
        "        exp: Experiment object\n",
        "        args: Arguments namespace\n",
        "        save_results: Whether to save results to files\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing evaluation results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    exp.model.eval()\n",
        "\n",
        "    # Set is_training to 0 for evaluation\n",
        "    args.is_training = 0\n",
        "\n",
        "    # Create evaluation results directory\n",
        "    eval_save_path = './evaluation_results/'\n",
        "    os.makedirs(eval_save_path, exist_ok=True)\n",
        "\n",
        "    # Initialize results dictionary\n",
        "    eval_results = {\n",
        "        'model_id': args.model_id,\n",
        "        'model_type': args.model,\n",
        "        'setting': 'autoformer_custom_autoformer_custom',\n",
        "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
        "        'evaluation_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "    print(\"1. Running test method for standard evaluation...\")\n",
        "    # Run the standard test method\n",
        "    try:\n",
        "        exp.test(setting='autoformer_custom_autoformer_custom')\n",
        "        print(\"‚úì Standard test method completed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Warning: Standard test method failed: {e}\")\n",
        "\n",
        "    print(\"\\n2. Performing detailed evaluation...\")\n",
        "\n",
        "    # Get predictions and true values for detailed analysis\n",
        "    try:\n",
        "        # Method 1: Use the built-in data provider from Exp_Main\n",
        "        from data_provider.data_factory import data_provider\n",
        "\n",
        "        # Get test data loader\n",
        "        test_data, test_loader = data_provider(args, 'test')\n",
        "\n",
        "        print(f\"‚úì Test data loaded successfully. Dataset size: {len(test_data)}\")\n",
        "\n",
        "        # Get predictions using the test data loader\n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            true_values = []\n",
        "\n",
        "            exp.model.eval()\n",
        "\n",
        "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
        "                # Move to device if GPU is available\n",
        "                if args.use_gpu:\n",
        "                    batch_x = batch_x.float().cuda()\n",
        "                    batch_y = batch_y.float().cuda()\n",
        "                    batch_x_mark = batch_x_mark.float().cuda()\n",
        "                    batch_y_mark = batch_y_mark.float().cuda()\n",
        "                else:\n",
        "                    batch_x = batch_x.float()\n",
        "                    batch_y = batch_y.float()\n",
        "                    batch_x_mark = batch_x_mark.float()\n",
        "                    batch_y_mark = batch_y_mark.float()\n",
        "\n",
        "                # Decoder input\n",
        "                dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
        "                dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()\n",
        "\n",
        "                if args.use_gpu:\n",
        "                    dec_inp = dec_inp.cuda()\n",
        "\n",
        "                # Get prediction\n",
        "                if args.output_attention:\n",
        "                    outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                else:\n",
        "                    outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "\n",
        "                # Extract the prediction part (last pred_len time steps)\n",
        "                pred = outputs[:, -args.pred_len:, :]\n",
        "                true = batch_y[:, -args.pred_len:, :]\n",
        "\n",
        "                predictions.append(pred.cpu().numpy())\n",
        "                true_values.append(true.cpu().numpy())\n",
        "\n",
        "                # Print progress for large datasets\n",
        "                if i > 0 and i % 50 == 0:\n",
        "                    print(f\"   Processed {i}/{len(test_loader)} batches...\")\n",
        "\n",
        "            if predictions and true_values:\n",
        "                y_pred = np.concatenate(predictions, axis=0)\n",
        "                y_true = np.concatenate(true_values, axis=0)\n",
        "\n",
        "                print(f\"‚úì Predictions generated. Shape: {y_pred.shape}\")\n",
        "\n",
        "                # Calculate detailed metrics\n",
        "                metrics = calculate_metrics(y_true, y_pred)\n",
        "                eval_results.update(metrics)\n",
        "\n",
        "                # Print metrics\n",
        "                print(\"\\n3. Detailed Evaluation Metrics:\")\n",
        "                print(\"-\" * 40)\n",
        "                for metric, value in metrics.items():\n",
        "                    print(f\"{metric:<20}: {value:.6f}\")\n",
        "\n",
        "                # Create plots\n",
        "                print(\"\\n4. Creating evaluation plots...\")\n",
        "                plot_filename = create_evaluation_plots(y_true, y_pred)\n",
        "                eval_results['plot_filename'] = plot_filename\n",
        "\n",
        "                print(\"‚úì Detailed evaluation completed successfully\")\n",
        "\n",
        "                # Save predictions for further analysis\n",
        "                pred_save_path = './evaluation_results/'\n",
        "                os.makedirs(pred_save_path, exist_ok=True)\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "                # Save predictions as numpy arrays\n",
        "                np.save(os.path.join(pred_save_path, f\"predictions_{timestamp}.npy\"), y_pred)\n",
        "                np.save(os.path.join(pred_save_path, f\"true_values_{timestamp}.npy\"), y_true)\n",
        "\n",
        "                print(f\"‚úì Predictions saved to: {pred_save_path}\")\n",
        "\n",
        "            else:\n",
        "                print(\"‚ö† Warning: Could not extract predictions for detailed analysis\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö† Warning: Could not import data_provider: {e}\")\n",
        "        print(\"Trying alternative method...\")\n",
        "\n",
        "        # Method 2: Alternative approach using exp object's data loading\n",
        "        try:\n",
        "            # Set up data loader manually\n",
        "            args.is_training = 0  # Ensure we're in test mode\n",
        "\n",
        "            # Try to recreate the experiment for testing\n",
        "            from exp.exp_main import Exp_Main\n",
        "            exp_test = Exp_Main(args)\n",
        "\n",
        "            # Get test predictions using the vali method (which is often used for testing)\n",
        "            print(\"Using validation method for evaluation...\")\n",
        "\n",
        "            # The vali method in Exp_Main typically returns metrics\n",
        "            # We'll capture the model's predictions during validation\n",
        "            vali_results = exp_test.vali(test_loader=None, criterion=None)\n",
        "\n",
        "            print(\"‚úì Validation-based evaluation completed\")\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ö† Warning: Alternative evaluation method failed: {e2}\")\n",
        "            print(\"Trying manual prediction method...\")\n",
        "\n",
        "            # Method 3: Manual prediction method\n",
        "            y_pred, y_true = get_test_predictions_manual(exp, args)\n",
        "\n",
        "            if y_pred is not None and y_true is not None:\n",
        "                print(\"‚úì Manual prediction method succeeded\")\n",
        "\n",
        "                # Calculate detailed metrics\n",
        "                metrics = calculate_metrics(y_true, y_pred)\n",
        "                eval_results.update(metrics)\n",
        "\n",
        "                # Print metrics\n",
        "                print(\"\\n3. Detailed Evaluation Metrics:\")\n",
        "                print(\"-\" * 40)\n",
        "                for metric, value in metrics.items():\n",
        "                    print(f\"{metric:<20}: {value:.6f}\")\n",
        "\n",
        "                # Create plots\n",
        "                print(\"\\n4. Creating evaluation plots...\")\n",
        "                plot_filename = create_evaluation_plots(y_true, y_pred)\n",
        "                eval_results['plot_filename'] = plot_filename\n",
        "\n",
        "                print(\"‚úì Detailed evaluation completed successfully\")\n",
        "            else:\n",
        "                print(\"‚ö† All prediction methods failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Warning: Detailed evaluation failed: {e}\")\n",
        "        print(\"Please ensure your data_provider module is properly configured.\")\n",
        "\n",
        "        # Add basic model information even if detailed evaluation fails\n",
        "        eval_results.update({\n",
        "            'MSE': 'N/A',\n",
        "            'RMSE': 'N/A',\n",
        "            'MAE': 'N/A',\n",
        "            'MAPE': 'N/A',\n",
        "            'MSPE': 'N/A',\n",
        "            'R2': 'N/A',\n",
        "            'Directional_Accuracy': 'N/A'\n",
        "        })\n",
        "\n",
        "    # Add model configuration to results\n",
        "    eval_results['model_config'] = {\n",
        "        'seq_len': args.seq_len,\n",
        "        'pred_len': args.pred_len,\n",
        "        'train_epochs': args.train_epochs,\n",
        "        'batch_size': args.batch_size,\n",
        "        'learning_rate': args.learning_rate,\n",
        "        'model_layers': f\"e_layers: {args.e_layers}, d_layers: {args.d_layers}\",\n",
        "        'model_dim': args.d_model,\n",
        "        'n_heads': args.n_heads\n",
        "    }\n",
        "\n",
        "    # Save results if requested\n",
        "    if save_results:\n",
        "        print(\"\\n5. Saving evaluation results...\")\n",
        "        eval_filename = os.path.join(eval_save_path, f\"eval_results_{eval_results['timestamp']}.json\")\n",
        "\n",
        "        with open(eval_filename, 'w') as f:\n",
        "            json.dump(eval_results, f, indent=4)\n",
        "\n",
        "        print(f\"‚úì Evaluation results saved to: {eval_filename}\")\n",
        "\n",
        "        # Create summary report\n",
        "        summary_filename = os.path.join(eval_save_path, f\"eval_summary_{eval_results['timestamp']}.txt\")\n",
        "        with open(summary_filename, 'w') as f:\n",
        "            f.write(\"AUTOFORMER MODEL EVALUATION SUMMARY\\n\")\n",
        "            f.write(\"=\"*50 + \"\\n\")\n",
        "            f.write(f\"Evaluation Date: {eval_results['evaluation_date']}\\n\")\n",
        "            f.write(f\"Model ID: {eval_results['model_id']}\\n\")\n",
        "            f.write(f\"Model Type: {eval_results['model_type']}\\n\")\n",
        "            f.write(f\"Setting: {eval_results['setting']}\\n\\n\")\n",
        "\n",
        "            f.write(\"MODEL CONFIGURATION:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            for key, value in eval_results['model_config'].items():\n",
        "                f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "            f.write(\"\\nEVALUATION METRICS:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            metrics = ['MSE', 'RMSE', 'MAE', 'MAPE', 'MSPE', 'R2', 'Directional_Accuracy']\n",
        "            for metric in metrics:\n",
        "                if metric in eval_results:\n",
        "                    f.write(f\"{metric}: {eval_results[metric]}\\n\")\n",
        "\n",
        "        print(f\"‚úì Evaluation summary saved to: {summary_filename}\")\n",
        "        eval_results['summary_filename'] = summary_filename\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EVALUATION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "# MAIN EVALUATION EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    # This assumes the 'exp' and 'args' objects are available from the training script\n",
        "    # If running separately, you would need to load the model first:\n",
        "    # exp, args = load_saved_model(model_path, config_path)\n",
        "\n",
        "    print(\"\\nStarting comprehensive model evaluation...\")\n",
        "\n",
        "    # Perform comprehensive evaluation\n",
        "    evaluation_results = comprehensive_model_evaluation(exp, args, save_results=True)\n",
        "\n",
        "    # Print final summary\n",
        "    print(\"\\nFINAL EVALUATION SUMMARY:\")\n",
        "    print(\"-\" * 30)\n",
        "    if 'MSE' in evaluation_results and evaluation_results['MSE'] != 'N/A':\n",
        "        print(f\"Mean Squared Error (MSE): {evaluation_results['MSE']:.6f}\")\n",
        "        print(f\"Root Mean Squared Error (RMSE): {evaluation_results['RMSE']:.6f}\")\n",
        "        print(f\"Mean Absolute Error (MAE): {evaluation_results['MAE']:.6f}\")\n",
        "        print(f\"Mean Absolute Percentage Error (MAPE): {evaluation_results['MAPE']:.2f}%\")\n",
        "        print(f\"R¬≤ Score: {evaluation_results['R2']:.6f}\")\n",
        "\n",
        "    print(f\"\\nAll evaluation files saved in: ./evaluation_results/\")\n",
        "    print(\"Evaluation completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2cdaffd"
      },
      "outputs": [],
      "source": [
        "# Fix the numpy.Inf deprecation error in utils/tools.py\n",
        "!sed -i 's/np.Inf/np.inf/g' ./utils/tools.py\n",
        "\n",
        "print(\"Fixed np.Inf in utils/tools.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40d46c8d"
      },
      "source": [
        "## üîç Post-Hoc Interpretation with Explainable Boosting Machine (EBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1ZZhqmqS9BL"
      },
      "source": [
        "Expalainable Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da2da9a6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Prepare Static Covariates and Forecast Averages\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming original dataset is loaded as 'df' and forecast saved to 'forecast_df'\n",
        "static_columns= [col for col in df.columns if col.startswith('static_') or col.startswith('')]  # Replace with your static features\n",
        "target_column = \"RCO_Yield\"  # Example target; modify as needed\n",
        "\n",
        "# Extract static features (latest available per batch)\n",
        "df['crude_batch_id'] = df['future_Blend_Num'].astype(str)\n",
        "latest_static = df.groupby(\"crude_batch_id\").last()\n",
        "static_covariates = latest_static[static_columns]\n",
        "\n",
        "# Compute average predictions per batch\n",
        "forecast_df['crude_batch_id'] = forecast_df['future_Blend_Num'].astype(str)\n",
        "target_avg = forecast_df.groupby(\"crude_batch_id\")[[target_column]].mean()\n",
        "\n",
        "# Merge into EBM dataset\n",
        "ebm_data = static_covariates.join(target_avg, how=\"inner\").dropna()\n",
        "X = ebm_data[static_columns]\n",
        "y = ebm_data[target_column]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkmbNzkHTGhu"
      },
      "source": [
        "EBM Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ed0076c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 2: Train the EBM Model\n",
        "from interpret.glassbox import ExplainableBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ebm = ExplainableBoostingRegressor(interactions=10)\n",
        "ebm.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wABGYImSTOFb"
      },
      "source": [
        "Global Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6d23ebf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 3: Interpret Feature Importance and Sample Explanation\n",
        "from interpret import show\n",
        "\n",
        "# Global explanation\n",
        "ebm_global = ebm.explain_global()\n",
        "show(ebm_global)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahJnSAYTT-E"
      },
      "source": [
        "Local Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3c24d8e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optional: Local explanation for one prediction\n",
        "sample = X_test.iloc[[0]]\n",
        "ebm_local = ebm.explain_local(sample, y_test.iloc[[0]])\n",
        "show(ebm_local)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSrXQdVpTXOz"
      },
      "source": [
        "Feature Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "647a8f73"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optional: Bar Plot of Feature Importances\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(X.columns, ebm.feature_importances_)\n",
        "plt.title(\"EBM Feature Importances on RCO_Yield\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a021ab6"
      },
      "outputs": [],
      "source": [
        "print(\"Columns available in the df DataFrame:\")\n",
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff5667ab"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/processed/final_concatenated_data_mice_imputed.csv'\n",
        "\n",
        "try:\n",
        "    df_columns = pd.read_csv(file_path, nrows=0).columns.tolist()\n",
        "    print(\"Column names in the file:\")\n",
        "    for col in df_columns:\n",
        "        print(col)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
