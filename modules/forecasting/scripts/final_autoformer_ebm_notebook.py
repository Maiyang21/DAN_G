# -*- coding: utf-8 -*-
"""FINAL_Autoformer_EBM_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jG-igjnSO7D6_eJWsJDpluQj_fJ_jxQ8

MOUNTING STORAGE DEVICE
"""

from google.colab import drive
drive.mount('/content/drive')

"""AUTOFORMER GIT REPO CLONING"""

# Install dependencies for Hugging Face Autoformer
!pip install transformers datasets
# Removed git clone and %cd as we are using Hugging Face

# Note: Additional dependencies might be needed depending on specific usage,
# but transformers and datasets are the core libraries.

"""FEATURE ENGINEERING"""

# Fix the numpy.Inf deprecation error in utils/tools.py
!sed -i 's/np.Inf/np.inf/g' ./utils/tools.py

print("Fixed np.Inf in utils/tools.py")

import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta

def load_and_prepare_data():
    """
    Load and prepare data for Autoformer training
    """
    print("üîç Starting data preparation for Autoformer...")

    # Define the path to the MICE interpolated and concatenated data
    file_path = '/content/drive/MyDrive/final_concatenated_data_mice_imputed.csv'

    # List of specified target columns
    target_cols_list = [
        'Total_Stabilized_Naphtha_Product_Flowrate',
        'Total_Kerosene_Product_Flowrate',
        'Jet_Fuel_Product_Train1_Flowrate',
        'Total_Light_Diesel_Product_Flowrate',
        'Total_Heavy_Diesel_Product_Flowrate',
        'Total_Atmospheric_Residue_Flowrate',
        'Blend_Yield_Gas & LPG',
        'Blend_Yield_Kerosene',
        'Blend_Yield_Light Diesel',
        'Blend_Yield_Heavy Diesel',
        'Blend_Yield_RCO'
    ]

    # Load the combined CSV file with robust error handling
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Error: File not found - {file_path}")

    try:
        df = pd.read_csv(file_path)
        print(f"‚úÖ Successfully loaded {file_path}: {len(df)} rows, {len(df.columns)} columns")
    except pd.errors.EmptyDataError:
        raise ValueError(f"Error: File {file_path} is empty.")
    except Exception as e:
        raise RuntimeError(f"Error loading file {file_path}: {e}")

    if df.empty:
        raise ValueError("Loaded dataframe is empty.")

    # Check if 'date' column exists and parse dates
    if 'date' in df.columns:
        print("‚úÖ Found 'date' column, parsing dates...")
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        df = df.dropna(subset=['date'])
        if df.empty:
            raise ValueError("Error: No valid date entries after parsing.")
    else:
        print("‚ö†Ô∏è  No 'date' column found, creating synthetic dates...")
        # Create synthetic date column starting from a reasonable date
        start_date = datetime(2024, 1, 1)
        df['date'] = [start_date + timedelta(days=i) for i in range(len(df))]

    # Sort by date and reset index
    df = df.sort_values('date').reset_index(drop=True)

    # Drop 'Tag' column if it exists
    if "Tag" in df.columns:
        df = df.drop(columns=["Tag"])
        print("‚úÖ Dropped 'Tag' column")

    # Display date range
    print(f"üìÖ Date range: {df['date'].min()} to {df['date'].max()}")

    # Check if all specified target columns exist in the DataFrame
    existing_targets = [col for col in target_cols_list if col in df.columns]
    missing_targets = [col for col in target_cols_list if col not in df.columns]

    if missing_targets:
        print(f"‚ö†Ô∏è  Missing target columns: {missing_targets}")

    if not existing_targets:
        raise ValueError("‚ùå No target columns found in the dataset!")

    print(f"‚úÖ Found {len(existing_targets)} target columns: {existing_targets}")

    # Identify feature columns (all columns except 'date' and target columns)
    feature_cols = [col for col in df.columns if col != 'date' and col not in existing_targets]
    print(f"üìä Feature columns: {len(feature_cols)} columns")

    # Convert all non-date columns to numeric
    print("üîß Converting columns to numeric...")
    non_numeric_cols = []

    for col in df.columns:
        if col != 'date':
            if not pd.api.types.is_numeric_dtype(df[col]):
                try:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    print(f"‚úÖ Converted {col} to numeric")
                except Exception as e:
                    non_numeric_cols.append(col)
                    print(f"‚ùå Failed to convert {col}: {e}")

    # Drop columns that couldn't be converted
    if non_numeric_cols:
        print(f"‚ö†Ô∏è  Dropping {len(non_numeric_cols)} non-numeric columns")
        df = df.drop(columns=non_numeric_cols)
        # Update feature and target lists
        feature_cols = [col for col in feature_cols if col not in non_numeric_cols]
        existing_targets = [col for col in existing_targets if col not in non_numeric_cols]

    # Handle missing values
    print("üîß Handling missing values...")
    # For numeric columns, use forward fill then backward fill
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df[numeric_cols] = df[numeric_cols].fillna(0)

    # Remove rows with any remaining NaN values
    """initial_rows = len(df)
    df = df.dropna()
    final_rows = len(df)

    if initial_rows != final_rows:
        print(f"‚ö†Ô∏è  Removed {initial_rows - final_rows} rows with NaN values")
    """
    if df.empty:
        raise ValueError("‚ùå All data was removed during cleaning!")

    # Final verification
    print(f"‚úÖ Final dataset: {len(df)} rows, {len(df.columns)} columns")
    print(f"‚úÖ Target columns: {len(existing_targets)}")
    print(f"‚úÖ Feature columns: {len(feature_cols)}")

    # Prepare datasets for Autoformer
    # Autoformer expects: date column + all other numeric columns
    # The target columns should be at the end for easier processing

    # Reorder columns: date + features + targets
    final_columns = ['date'] + feature_cols + existing_targets
    df_final = df[final_columns].copy()

    # Create output directory
    os.makedirs('./dataset/custom/', exist_ok=True)

    # Save the prepared dataset
    output_path = './dataset/custom/custom.csv'
    df_final.to_csv(output_path, index=False)

    # Create dataset info file
    dataset_info = {
        'total_rows': len(df_final),
        'total_columns': len(df_final.columns) - 1,  # Exclude date column
        'target_columns': existing_targets,
        'num_targets': len(existing_targets),
        'feature_columns': feature_cols[:20],  # First 20 for brevity
        'num_features': len(feature_cols),
        'date_range_start': str(df_final['date'].min()),
        'date_range_end': str(df_final['date'].max()),
        'primary_target': existing_targets[0] if existing_targets else 'Unknown'
    }

    # Save dataset info
    info_path = './dataset/custom/dataset_info.json'
    import json
    with open(info_path, 'w') as f:
        json.dump(dataset_info, f, indent=4)

    # Create summary file
    summary_path = './dataset/custom/dataset_summary.txt'
    with open(summary_path, 'w') as f:
        f.write("Autoformer Dataset Summary\n")
        f.write("=" * 30 + "\n")
        f.write(f"Total rows: {dataset_info['total_rows']}\n")
        f.write(f"Total columns (excluding date): {dataset_info['total_columns']}\n")
        f.write(f"Target columns ({dataset_info['num_targets']}): {dataset_info['target_columns']}\n")
        f.write(f"Feature columns: {dataset_info['num_features']}\n")
        f.write(f"Date range: {dataset_info['date_range_start']} to {dataset_info['date_range_end']}\n")
        f.write(f"Primary target: {dataset_info['primary_target']}\n")
        f.write("\nFirst 10 feature columns:\n")
        for i, col in enumerate(feature_cols[:10]):
            f.write(f"  {i+1}. {col}\n")

    print(f"üíæ Dataset saved to: {output_path}")
    print(f"üìã Dataset info saved to: {info_path}")
    print(f"üìä Summary saved to: {summary_path}")

    # Display sample data
    print("\nüìã Sample data (first 3 rows):")
    print(df_final.head(3).to_string())

    print("\nüéâ Data preparation completed successfully!")
    return dataset_info

# Run data preparation
if __name__ == "__main__":
    try:
        info = load_and_prepare_data()
        print(f"\n‚úÖ Ready for training with {info['total_rows']} samples")
        print(f"‚úÖ Input features: {info['num_features']}")
        print(f"‚úÖ Target outputs: {info['num_targets']}")
        print(f"‚úÖ Primary target: {info['primary_target']}")
    except Exception as e:
        print(f"‚ùå Data preparation failed: {e}")
        raise

"""MODEL TRAINING"""

# Import necessary libraries
import numpy as np
import warnings
import os
import pandas as pd
import torch
import argparse
import json
from datetime import datetime
from transformers import AutoformerConfig, AutoformerForPrediction # Import Hugging Face Autoformer
from torch.utils.data import Dataset, DataLoader # For creating custom dataset and dataloader
import torch.optim as optim # For defining optimizer
import torch.nn as nn # For defining loss function

print("üîß Applying NumPy 2.0 compatibility fixes...")

# Add backward compatibility for deprecated NumPy attributes
if not hasattr(np, 'Inf'):
    np.Inf = np.inf
    print("   ‚úÖ Fixed np.Inf")

if not hasattr(np, 'NaN'):
    np.NaN = np.nan
    print("   ‚úÖ Fixed np.NaN")

if not hasattr(np, 'NINF'):
    np.NINF = -np.inf
    print("   ‚úÖ Fixed np.NINF")

if not hasattr(np, 'PINF'):
    np.PINF = np.inf
    print("   ‚úÖ Fixed np.PINF")

# Suppress all related deprecation warnings
warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*np\.Inf.*')
warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*np\.NaN.*')
warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*np\.NINF.*')
warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*np\.PINF.*')
warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*was removed in the NumPy.*')

print("‚úÖ NumPy compatibility fixes applied successfully!")


def create_hf_dataset_format(dataframe, time_features_array, seq_len, pred_len, is_training=True):
    """
    Helper function to create data tensors in Hugging Face format.
    This function is designed to be reusable for train, test, and prediction data.
    """
    past_values_list = []
    past_time_features_list = []
    future_time_features_list = []
    future_values_list = [] # Only for training data

    total_length = len(dataframe)
    # Calculate the number of complete sequences that can be formed
    num_sequences = total_length - seq_len - pred_len + 1

    # Ensure there are enough data points to create at least one sequence
    if num_sequences <= 0:
        print(f"Warning: Not enough data to create sequences with seq_len={seq_len}, pred_len={pred_len}. Data length is {total_length}.")
        return np.array([]), np.array([]), np.array([]), np.array([]) if is_training else None

    for i in range(num_sequences):
        past_values = dataframe.iloc[i : i + seq_len].values
        past_time_features = time_features_array[i : i + seq_len]

        future_time_features = time_features_array[i + seq_len : i + seq_len + pred_len]

        past_values_list.append(past_values)
        past_time_features_list.append(past_time_features)
        future_time_features_list.append(future_time_features)

        if is_training:
            future_values = dataframe.iloc[i + seq_len : i + seq_len + pred_len].values
            future_values_list.append(future_values)

    # Convert lists to numpy arrays
    past_values_np = np.array(past_values_list)
    past_time_features_np = np.array(past_time_features_list)
    future_time_features_np = np.array(future_time_features_list)
    future_values_np = np.array(future_values_list) if is_training else None

    return past_values_np, past_time_features_np, future_time_features_np, future_values_np

class TimeSeriesDataset(Dataset):
    """
    Custom PyTorch Dataset for time series data in Hugging Face format.
    """
    def __init__(self, past_values, past_time_features, future_time_features, future_values=None):
        self.past_values = torch.tensor(past_values, dtype=torch.float32)
        self.past_time_features = torch.tensor(past_time_features, dtype=torch.float32)
        self.future_time_features = torch.tensor(future_time_features, dtype=torch.float32)
        self.future_values = torch.tensor(future_values, dtype=torch.float32) if future_values is not None else None

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        # Return future_values only if it exists (for training)
        if self.future_values is not None:
            return (
                self.past_values[idx],
                self.past_time_features[idx],
                self.future_time_features[idx],
                self.future_values[idx],
            )
        else:
            # For testing/prediction, future_values is not available
             return (
                self.past_values[idx],
                self.past_time_features[idx],
                self.future_time_features[idx],
                # No future_values returned here
            )


def train_hf_autoformer(hf_data):
    """
    Trains the Hugging Face Autoformer model.

    Args:
        hf_data (dict): Dictionary containing the prepared data in Hugging Face format.
    """
    print("\n" + "=" * 60)
    print("üöÄ Starting Hugging Face Autoformer training...")
    print("=" * 60)

    # Extract data from hf_data
    train_past_values = hf_data['train']['past_values']
    train_past_time_features = hf_data['train']['past_time_features']
    train_future_time_features = hf_data['train']['future_time_features']
    train_future_values = hf_data['train']['future_values']
    seq_len = hf_data['seq_len']
    pred_len = hf_data['pred_len']
    num_variates = hf_data['num_variates']
    num_time_features = hf_data['num_time_features']
    target_columns = hf_data['target_columns']
    num_targets = len(target_columns)

    if train_past_values.shape[0] == 0:
        print("‚ùå Error: Training dataset is empty. Cannot train the model.")
        return None, None # Indicate training failed

    print(f"\nDataset shapes for training:")
    print(f"   - past_values: {train_past_values.shape}")
    print(f"   - past_time_features: {train_past_time_features.shape}")
    print(f"   - future_time_features: {train_future_time_features.shape}")
    print(f"   - future_values: {train_future_values.shape}")
    print(f"   - Sequence Length (context_length): {seq_len}")
    print(f"   - Prediction Length: {pred_len}")
    print(f"   - Number of Variates (input_size): {num_variates}")
    print(f"   - Number of Target Variables (num_outputs): {num_targets}")
    print(f"   - Number of Time Features: {num_time_features}")
    print(f"   - Target Columns: {target_columns}")

    # CRITICAL FIX: Calculate safe lags_sequence
    # The lags should be much smaller than the context length
    # A good rule of thumb is to use lags up to context_length // 3 or smaller
    max_safe_lag = seq_len // 3  # Use at most 1/3 of context length, cap at 7
    safe_lags = list(range(1, max_safe_lag + 1))

    print(f"\nüîß Calculated safe lags_sequence: {safe_lags}")
    print(f"   - Max lag: {max_safe_lag}, Context length: {seq_len}")

    # Define the model configuration with FIXED lags_sequence
    config = AutoformerConfig(
        prediction_length=pred_len,
        context_length=seq_len,
        # input_size should be the number of time series (variates)
        input_size=num_variates,
        # num_outputs should be the number of target variables to predict
        num_outputs=num_targets, # Predict all target variates
        # The number of static/dynamic features depends on your dataset.
        # Assuming time features are the only dynamic real features for now.
        num_dynamic_real_features=num_time_features, # Number of time features we created
        num_static_categorical_features=0, # Adjust if you have static categorical features
        num_static_real_features=0,      # Adjust if you have static real features
        frequency="d", # Assuming daily frequency, adjust if needed
        # Model dimensions - use smaller values initially
        d_model=32,
        encoder_layers=1,
        decoder_layers=1,
        encoder_attention_heads=4,
        decoder_attention_heads=4,
        # Ensure moving_avg is less than context_length
        moving_avg=min(12, seq_len // 2) if seq_len > 0 else 1,
        use_past_context=True,
        # CRITICAL FIX: Use safe lags that are well within the context length
        # lags_sequence=safe_lags,  # Use the calculated safe lags
    )

    print("\n‚úÖ AutoformerConfig defined:")
    print(config)

    # Instantiate the model
    model = AutoformerForPrediction(config)

    print("\n‚úÖ AutoformerForPrediction model instantiated.")

    # Ensure model is on the correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"Model moved to device: {device}")

    # Define the optimizer and loss function
    optimizer = optim.AdamW(model.parameters(), lr=0.001)
    print("Optimizer (AdamW) defined with learning rate 0.001")
    print("Using model's internal loss function (e.g., NLL) via outputs.loss")

    # Create the PyTorch Dataset and DataLoader
    train_dataset = TimeSeriesDataset(
        train_past_values,
        train_past_time_features,
        train_future_time_features,
        train_future_values, # Include future_values for training
    )

    # Use a reasonable batch size
    batch_size = min(32, max(2, train_past_values.shape[0] // 10)) # Adjust batch size based on dataset size
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    print(f"\nCreated TimeSeriesDataset and DataLoader with batch size: {batch_size}")
    print(f"Number of batches in training DataLoader: {len(train_dataloader)}")

    # Implement the training loop
    num_epochs = 10 # Set a reasonable number of epochs

    print(f"\nStarting training for {num_epochs} epochs...")

    model.train() # Set the model to training mode

    # Initialize variables for tracking best loss and early stopping
    best_val_loss = float('inf')
    patience = 5 # Number of epochs to wait for improvement
    epochs_no_improve = 0
    early_stop = False

    for epoch in range(num_epochs):
        if early_stop:
            print("Early stopping triggered.")
            break

        total_loss = 0
        print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")

        for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader):
            # Move batch to device
            past_values = past_values.to(device)
            past_time_features = past_time_features.to(device)
            future_time_features = future_time_features.to(device)
            future_values = future_values.to(device)

            # Create past_observed_mask: assuming all values in past_values are observed (not missing)
            # If you have missing values, you need to create this mask based on your data.
            past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

            # Zero the gradients
            optimizer.zero_grad()

            try:
                # Forward pass
                # Pass future_values as labels for loss calculation
                outputs = model(
                    past_values=past_values,
                    past_time_features=past_time_features,
                    future_time_features=future_time_features,
                    past_observed_mask=past_observed_mask,
                    future_values=future_values # Provided as labels for loss calculation
                )

                # The output of the forward pass includes the loss when future_values is provided
                loss = outputs.loss

                # Backward pass and optimize
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

                # Print loss every few batches
                if (i + 1) % 10 == 0:  # Changed from 50 to 10 for more frequent updates
                    print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}")

            except Exception as e:
                print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
                print("Batch shapes:")
                print(f"  past_values: {past_values.shape}")
                print(f"  past_time_features: {past_time_features.shape}")
                print(f"  future_time_features: {future_time_features.shape}")
                print(f"  future_values: {future_values.shape}")
                print(f"  past_observed_mask: {past_observed_mask.shape}")
                # Stop training on error
                raise # Re-raise the exception

        # Print average epoch loss
        avg_epoch_loss = total_loss / len(train_dataloader)
        print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

        # Early stopping check (based on training loss for this simple example)
        # Replace with validation loss when implemented
        current_val_loss = avg_epoch_loss # Use training loss as proxy for now

        if current_val_loss < best_val_loss:
            best_val_loss = current_val_loss
            epochs_no_improve = 0
            # Optionally save the best model state here
            # torch.save(model.state_dict(), 'best_autoformer_model.pth')
            print("‚úÖ Training loss improved.")
        else:
            epochs_no_improve += 1
            print(f"‚ö†Ô∏è  Training loss did not improve for {epochs_no_improve} epoch(s).")
            if epochs_no_improve >= patience:
                early_stop = True

    print("\nüéâ Training finished successfully!")

    # Return the trained model for subsequent steps (prediction/evaluation)
    return model, config # Return both the model and its config


# MAIN EXECUTION BLOCK
if __name__ == "__main__":
    # --- Data Preparation ---
    # Define sequence lengths based on the data.
    # Use more conservative values to avoid the lags issue
    try:
        seq_len = 90  # Increased context length to allow for better lag sequences
        pred_len = 14  # Keep prediction length the same

        print(f"\nüîß Using seq_len={seq_len}, pred_len={pred_len}")

        # Re-create hf_data dictionary using the data variables available in the kernel
        print("\nüîß Preparing data in Hugging Face format using variables from data prep...")

        # Ensure required data variables exist
        required_vars = ['train_df', 'test_df', 'all_time_features', 'existing_targets', 'feature_cols']
        if not all(v in locals() or v in globals() for v in required_vars):
            print(f"‚ùå Error: Required data variables {required_vars} not found in kernel. Please run the data preparation cell (807764bc) first.")
            exit()

        # Assuming train_df, test_df, all_time_features, existing_targets, feature_cols are available
        train_size = len(train_df)
        train_time_features_subset = all_time_features[:train_size]
        test_time_features_subset = all_time_features[train_size:] # Use the subset for test_time_features

        train_past_values, train_past_time_features, train_future_time_features, train_future_values = create_hf_dataset_format(
            train_df, train_time_features_subset, seq_len, pred_len, is_training=True
        )

        test_past_values, test_past_time_features, test_future_time_features, _ = create_hf_dataset_format(
            test_df, test_time_features_subset, seq_len, pred_len, is_training=False
        )

        hf_data = {
            'train': {
                'past_values': train_past_values,
                'past_time_features': train_past_time_features,
                'future_time_features': train_future_time_features,
                'future_values': train_future_values,
            },
            'test': {
                'past_values': test_past_values,
                'past_time_features': test_past_time_features,
                'future_time_features': test_future_time_features,
            },
            'target_columns': existing_targets,
            'feature_columns': feature_cols,
            'seq_len': seq_len,
            'pred_len': pred_len,
            'num_variates': train_df.shape[1], # Number of columns in the numeric dataframe
            'num_time_features': all_time_features.shape[1] # Number of time features
        }

        print("\n‚úÖ Data prepared in Hugging Face format.")
        print("üîç Prepared data shapes:")
        if hf_data['train']['past_values'].shape[0] > 0:
            print(f"   - Train past_values: {hf_data['train']['past_values'].shape}")
            print(f"   - Train past_time_features: {hf_data['train']['past_time_features'].shape}")
            print(f"   - Train future_time_features: {hf_data['train']['future_time_features'].shape}")
            print(f"   - Train future_values: {hf_data['train']['future_values'].shape}")
        else:
            print("   - Training data is empty.")

        if hf_data['test']['past_values'].shape[0] > 0:
             print(f"   - Test past_values: {hf_data['test']['past_values'].shape}")
             print(f"   - Test past_time_features: {hf_data['test']['past_time_features'].shape}")
             print(f"   - Test future_time_features: {hf_data['test']['future_time_features'].shape}")
        else:
            print("   - Test data is empty or not enough for sequences.")

        print(f"   - Number of variates (time series): {hf_data['num_variates']}")
        print(f"   - Number of time features: {hf_data['num_time_features']}")
        print(f"   - Target columns: {hf_data['target_columns']}")

    except Exception as e:
        print(f"‚ùå Error during data preparation for Hugging Face format: {e}")
        print("Please ensure the data preparation cell (807764bc) ran successfully and populated necessary variables.")
        exit() # Exit if data prep for HF format fails

    # --- Model Training ---
    try:
        trained_model, model_config = train_hf_autoformer(hf_data)

        if trained_model is not None:
            print("\nüéâ Hugging Face Autoformer training completed successfully!")
            # The trained_model object and its config are returned
            # You can now use them for prediction and evaluation

            # Save the trained model and config if needed
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_dir = "./saved_hf_models/"
            os.makedirs(model_dir, exist_ok=True)

            model_save_path = os.path.join(model_dir, f"hf_autoformer_{timestamp}.pth")
            config_save_path = os.path.join(model_dir, f"hf_autoformer_{timestamp}_config.json")

            torch.save(trained_model.state_dict(), model_save_path)
            model_config.to_json_file(config_save_path)

            print(f"üíæ Trained model state_dict saved to: {model_save_path}")
            print(f"üíæ Model config saved to: {config_save_path}")

        else:
            print("\n‚ùå Hugging Face Autoformer training failed.")
            print("Please review the error messages above.")

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Training interrupted by user")
    except Exception as e:
        print(f"\n‚ùå An error occurred during the main training execution: {e}")
        print("Please review the error messages and tracebacks above.")

"""MODEL EVAL"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import torch
import os
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def get_test_predictions_manual(exp, args):
    """
    Manual method to get test predictions when data_provider is not available
    """
    try:
        # Create a simple data loader for the test data
        import pandas as pd
        from torch.utils.data import DataLoader, Dataset

        # Load the dataset
        df = pd.read_csv(os.path.join(args.root_path, args.data_path))

        # Simple train/test split (you may need to adjust this based on your data)
        train_size = int(len(df) * 0.8)
        test_df = df[train_size:]

        print(f"‚úì Manual data loading: {len(test_df)} test samples")

        # Simple dataset class
        class SimpleDataset(Dataset):
            def __init__(self, data, seq_len, pred_len):
                self.data = data.values
                self.seq_len = seq_len
                self.pred_len = pred_len

            def __len__(self):
                return len(self.data) - self.seq_len - self.pred_len + 1

            def __getitem__(self, idx):
                seq_x = self.data[idx:idx + self.seq_len]
                seq_y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len]
                return torch.FloatTensor(seq_x), torch.FloatTensor(seq_y)

        # Create dataset and dataloader
        test_dataset = SimpleDataset(test_df, args.seq_len, args.pred_len)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

        # Get predictions
        predictions = []
        true_values = []

        exp.model.eval()
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                if args.use_gpu:
                    batch_x = batch_x.cuda()
                    batch_y = batch_y.cuda()

                # Simple prediction (you may need to adjust based on your model)
                pred = exp.model(batch_x)

                predictions.append(pred.cpu().numpy())
                true_values.append(batch_y.cpu().numpy())

        if predictions and true_values:
            y_pred = np.concatenate(predictions, axis=0)
            y_true = np.concatenate(true_values, axis=0)
            return y_pred, y_true
        else:
            return None, None

    except Exception as e:
        print(f"Manual prediction method failed: {e}")
        return None, None

def calculate_metrics(y_true, y_pred):
    """
    Calculate comprehensive evaluation metrics

    Args:
        y_true: True values
        y_pred: Predicted values

    Returns:
        Dictionary of metrics
    """
    # Convert to numpy arrays if they're tensors
    if torch.is_tensor(y_true):
        y_true = y_true.cpu().numpy()
    if torch.is_tensor(y_pred):
        y_pred = y_pred.cpu().numpy()

    # Flatten arrays if multidimensional
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()

    # Remove any NaN values
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]

    # Calculate metrics
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)

    # MAPE (Mean Absolute Percentage Error)
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100

    # MSPE (Mean Squared Percentage Error)
    mspe = np.mean(((y_true - y_pred) / (y_true + 1e-8)) ** 2) * 100

    # R¬≤ Score
    r2 = r2_score(y_true, y_pred)

    # Directional Accuracy (for time series)
    if len(y_true) > 1:
        true_direction = np.sign(np.diff(y_true))
        pred_direction = np.sign(np.diff(y_pred))
        directional_accuracy = np.mean(true_direction == pred_direction) * 100
    else:
        directional_accuracy = 0

    return {
        'MSE': float(mse),
        'RMSE': float(rmse),
        'MAE': float(mae),
        'MAPE': float(mape),
        'MSPE': float(mspe),
        'R2': float(r2),
        'Directional_Accuracy': float(directional_accuracy)
    }

def create_evaluation_plots(y_true, y_pred, save_path="./evaluation_plots/"):
    """
    Create comprehensive evaluation plots

    Args:
        y_true: True values
        y_pred: Predicted values
        save_path: Directory to save plots
    """
    os.makedirs(save_path, exist_ok=True)

    # Convert to numpy arrays if they're tensors
    if torch.is_tensor(y_true):
        y_true = y_true.cpu().numpy()
    if torch.is_tensor(y_pred):
        y_pred = y_pred.cpu().numpy()

    # Flatten arrays if multidimensional
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()

    # Create subplots
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Model Evaluation Results', fontsize=16, fontweight='bold')

    # Plot 1: Actual vs Predicted (Time Series)
    axes[0, 0].plot(y_true, label='Actual', color='blue', alpha=0.7)
    axes[0, 0].plot(y_pred, label='Predicted', color='red', alpha=0.7)
    axes[0, 0].set_title('Actual vs Predicted Values (Time Series)')
    axes[0, 0].set_xlabel('Time Steps')
    axes[0, 0].set_ylabel('Values')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # Plot 2: Scatter Plot
    axes[0, 1].scatter(y_true, y_pred, alpha=0.6, color='green')
    axes[0, 1].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],
                    'r--', lw=2, label='Perfect Prediction')
    axes[0, 1].set_title('Scatter Plot: Actual vs Predicted')
    axes[0, 1].set_xlabel('Actual Values')
    axes[0, 1].set_ylabel('Predicted Values')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # Plot 3: Residuals
    residuals = y_true - y_pred
    axes[1, 0].scatter(y_pred, residuals, alpha=0.6, color='orange')
    axes[1, 0].axhline(y=0, color='r', linestyle='--')
    axes[1, 0].set_title('Residual Plot')
    axes[1, 0].set_xlabel('Predicted Values')
    axes[1, 0].set_ylabel('Residuals')
    axes[1, 0].grid(True, alpha=0.3)

    # Plot 4: Residuals Distribution
    axes[1, 1].hist(residuals, bins=50, alpha=0.7, color='purple', edgecolor='black')
    axes[1, 1].set_title('Distribution of Residuals')
    axes[1, 1].set_xlabel('Residuals')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()

    # Save the plot
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_filename = os.path.join(save_path, f"evaluation_plots_{timestamp}.png")
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"Evaluation plots saved to: {plot_filename}")
    return plot_filename

def comprehensive_model_evaluation(exp, args, save_results=True):
    """
    Perform comprehensive evaluation of the trained model

    Args:
        exp: Experiment object
        args: Arguments namespace
        save_results: Whether to save results to files

    Returns:
        Dictionary containing evaluation results
    """
    print("\n" + "="*50)
    print("COMPREHENSIVE MODEL EVALUATION")
    print("="*50)

    # Set model to evaluation mode
    exp.model.eval()

    # Set is_training to 0 for evaluation
    args.is_training = 0

    # Create evaluation results directory
    eval_save_path = './evaluation_results/'
    os.makedirs(eval_save_path, exist_ok=True)

    # Initialize results dictionary
    eval_results = {
        'model_id': args.model_id,
        'model_type': args.model,
        'setting': 'autoformer_custom_autoformer_custom',
        'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
        'evaluation_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    print("1. Running test method for standard evaluation...")
    # Run the standard test method
    try:
        exp.test(setting='autoformer_custom_autoformer_custom')
        print("‚úì Standard test method completed successfully")
    except Exception as e:
        print(f"‚ö† Warning: Standard test method failed: {e}")

    print("\n2. Performing detailed evaluation...")

    # Get predictions and true values for detailed analysis
    try:
        # Method 1: Use the built-in data provider from Exp_Main
        from data_provider.data_factory import data_provider

        # Get test data loader
        test_data, test_loader = data_provider(args, 'test')

        print(f"‚úì Test data loaded successfully. Dataset size: {len(test_data)}")

        # Get predictions using the test data loader
        with torch.no_grad():
            predictions = []
            true_values = []

            exp.model.eval()

            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):
                # Move to device if GPU is available
                if args.use_gpu:
                    batch_x = batch_x.float().cuda()
                    batch_y = batch_y.float().cuda()
                    batch_x_mark = batch_x_mark.float().cuda()
                    batch_y_mark = batch_y_mark.float().cuda()
                else:
                    batch_x = batch_x.float()
                    batch_y = batch_y.float()
                    batch_x_mark = batch_x_mark.float()
                    batch_y_mark = batch_y_mark.float()

                # Decoder input
                dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()
                dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()

                if args.use_gpu:
                    dec_inp = dec_inp.cuda()

                # Get prediction
                if args.output_attention:
                    outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]
                else:
                    outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)

                # Extract the prediction part (last pred_len time steps)
                pred = outputs[:, -args.pred_len:, :]
                true = batch_y[:, -args.pred_len:, :]

                predictions.append(pred.cpu().numpy())
                true_values.append(true.cpu().numpy())

                # Print progress for large datasets
                if i > 0 and i % 50 == 0:
                    print(f"   Processed {i}/{len(test_loader)} batches...")

            if predictions and true_values:
                y_pred = np.concatenate(predictions, axis=0)
                y_true = np.concatenate(true_values, axis=0)

                print(f"‚úì Predictions generated. Shape: {y_pred.shape}")

                # Calculate detailed metrics
                metrics = calculate_metrics(y_true, y_pred)
                eval_results.update(metrics)

                # Print metrics
                print("\n3. Detailed Evaluation Metrics:")
                print("-" * 40)
                for metric, value in metrics.items():
                    print(f"{metric:<20}: {value:.6f}")

                # Create plots
                print("\n4. Creating evaluation plots...")
                plot_filename = create_evaluation_plots(y_true, y_pred)
                eval_results['plot_filename'] = plot_filename

                print("‚úì Detailed evaluation completed successfully")

                # Save predictions for further analysis
                pred_save_path = './evaluation_results/'
                os.makedirs(pred_save_path, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                # Save predictions as numpy arrays
                np.save(os.path.join(pred_save_path, f"predictions_{timestamp}.npy"), y_pred)
                np.save(os.path.join(pred_save_path, f"true_values_{timestamp}.npy"), y_true)

                print(f"‚úì Predictions saved to: {pred_save_path}")

            else:
                print("‚ö† Warning: Could not extract predictions for detailed analysis")

    except ImportError as e:
        print(f"‚ö† Warning: Could not import data_provider: {e}")
        print("Trying alternative method...")

        # Method 2: Alternative approach using exp object's data loading
        try:
            # Set up data loader manually
            args.is_training = 0  # Ensure we're in test mode

            # Try to recreate the experiment for testing
            from exp.exp_main import Exp_Main
            exp_test = Exp_Main(args)

            # Get test predictions using the vali method (which is often used for testing)
            print("Using validation method for evaluation...")

            # The vali method in Exp_Main typically returns metrics
            # We'll capture the model's predictions during validation
            vali_results = exp_test.vali(test_loader=None, criterion=None)

            print("‚úì Validation-based evaluation completed")

        except Exception as e2:
            print(f"‚ö† Warning: Alternative evaluation method failed: {e2}")
            print("Trying manual prediction method...")

            # Method 3: Manual prediction method
            y_pred, y_true = get_test_predictions_manual(exp, args)

            if y_pred is not None and y_true is not None:
                print("‚úì Manual prediction method succeeded")

                # Calculate detailed metrics
                metrics = calculate_metrics(y_true, y_pred)
                eval_results.update(metrics)

                # Print metrics
                print("\n3. Detailed Evaluation Metrics:")
                print("-" * 40)
                for metric, value in metrics.items():
                    print(f"{metric:<20}: {value:.6f}")

                # Create plots
                print("\n4. Creating evaluation plots...")
                plot_filename = create_evaluation_plots(y_true, y_pred)
                eval_results['plot_filename'] = plot_filename

                print("‚úì Detailed evaluation completed successfully")
            else:
                print("‚ö† All prediction methods failed")

    except Exception as e:
        print(f"‚ö† Warning: Detailed evaluation failed: {e}")
        print("Please ensure your data_provider module is properly configured.")

        # Add basic model information even if detailed evaluation fails
        eval_results.update({
            'MSE': 'N/A',
            'RMSE': 'N/A',
            'MAE': 'N/A',
            'MAPE': 'N/A',
            'MSPE': 'N/A',
            'R2': 'N/A',
            'Directional_Accuracy': 'N/A'
        })

    # Add model configuration to results
    eval_results['model_config'] = {
        'seq_len': args.seq_len,
        'pred_len': args.pred_len,
        'train_epochs': args.train_epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'model_layers': f"e_layers: {args.e_layers}, d_layers: {args.d_layers}",
        'model_dim': args.d_model,
        'n_heads': args.n_heads
    }

    # Save results if requested
    if save_results:
        print("\n5. Saving evaluation results...")
        eval_filename = os.path.join(eval_save_path, f"eval_results_{eval_results['timestamp']}.json")

        with open(eval_filename, 'w') as f:
            json.dump(eval_results, f, indent=4)

        print(f"‚úì Evaluation results saved to: {eval_filename}")

        # Create summary report
        summary_filename = os.path.join(eval_save_path, f"eval_summary_{eval_results['timestamp']}.txt")
        with open(summary_filename, 'w') as f:
            f.write("AUTOFORMER MODEL EVALUATION SUMMARY\n")
            f.write("="*50 + "\n")
            f.write(f"Evaluation Date: {eval_results['evaluation_date']}\n")
            f.write(f"Model ID: {eval_results['model_id']}\n")
            f.write(f"Model Type: {eval_results['model_type']}\n")
            f.write(f"Setting: {eval_results['setting']}\n\n")

            f.write("MODEL CONFIGURATION:\n")
            f.write("-" * 20 + "\n")
            for key, value in eval_results['model_config'].items():
                f.write(f"{key}: {value}\n")

            f.write("\nEVALUATION METRICS:\n")
            f.write("-" * 20 + "\n")
            metrics = ['MSE', 'RMSE', 'MAE', 'MAPE', 'MSPE', 'R2', 'Directional_Accuracy']
            for metric in metrics:
                if metric in eval_results:
                    f.write(f"{metric}: {eval_results[metric]}\n")

        print(f"‚úì Evaluation summary saved to: {summary_filename}")
        eval_results['summary_filename'] = summary_filename

    print("\n" + "="*50)
    print("EVALUATION COMPLETED SUCCESSFULLY!")
    print("="*50)

    return eval_results

# MAIN EVALUATION EXECUTION
if __name__ == "__main__":
    # This assumes the 'exp' and 'args' objects are available from the training script
    # If running separately, you would need to load the model first:
    # exp, args = load_saved_model(model_path, config_path)

    print("\nStarting comprehensive model evaluation...")

    # Perform comprehensive evaluation
    evaluation_results = comprehensive_model_evaluation(exp, args, save_results=True)

    # Print final summary
    print("\nFINAL EVALUATION SUMMARY:")
    print("-" * 30)
    if 'MSE' in evaluation_results and evaluation_results['MSE'] != 'N/A':
        print(f"Mean Squared Error (MSE): {evaluation_results['MSE']:.6f}")
        print(f"Root Mean Squared Error (RMSE): {evaluation_results['RMSE']:.6f}")
        print(f"Mean Absolute Error (MAE): {evaluation_results['MAE']:.6f}")
        print(f"Mean Absolute Percentage Error (MAPE): {evaluation_results['MAPE']:.2f}%")
        print(f"R¬≤ Score: {evaluation_results['R2']:.6f}")

    print(f"\nAll evaluation files saved in: ./evaluation_results/")
    print("Evaluation completed successfully!")

# Fix the numpy.Inf deprecation error in utils/tools.py
!sed -i 's/np.Inf/np.inf/g' ./utils/tools.py

print("Fixed np.Inf in utils/tools.py")

"""## üîç Post-Hoc Interpretation with Explainable Boosting Machine (EBM)

Expalainable Data Prep
"""

# Step 1: Prepare Static Covariates and Forecast Averages
import pandas as pd

# Assuming original dataset is loaded as 'df' and forecast saved to 'forecast_df'
static_columns= [col for col in df.columns if col.startswith('static_') or col.startswith('')]  # Replace with your static features
target_column = "RCO_Yield"  # Example target; modify as needed

# Extract static features (latest available per batch)
df['crude_batch_id'] = df['future_Blend_Num'].astype(str)
latest_static = df.groupby("crude_batch_id").last()
static_covariates = latest_static[static_columns]

# Compute average predictions per batch
forecast_df['crude_batch_id'] = forecast_df['future_Blend_Num'].astype(str)
target_avg = forecast_df.groupby("crude_batch_id")[[target_column]].mean()

# Merge into EBM dataset
ebm_data = static_covariates.join(target_avg, how="inner").dropna()
X = ebm_data[static_columns]
y = ebm_data[target_column]

"""EBM Fitting"""

# Step 2: Train the EBM Model
from interpret.glassbox import ExplainableBoostingRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

ebm = ExplainableBoostingRegressor(interactions=10)
ebm.fit(X_train, y_train)

"""Global Interpretation"""

# Step 3: Interpret Feature Importance and Sample Explanation
from interpret import show

# Global explanation
ebm_global = ebm.explain_global()
show(ebm_global)

"""Local Interpretation"""

# Optional: Local explanation for one prediction
sample = X_test.iloc[[0]]
ebm_local = ebm.explain_local(sample, y_test.iloc[[0]])
show(ebm_local)

"""Feature Attribution"""

# Optional: Bar Plot of Feature Importances
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.barh(X.columns, ebm.feature_importances_)
plt.title("EBM Feature Importances on RCO_Yield")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

print("Columns available in the df DataFrame:")
print(df.columns.tolist())

import pandas as pd

file_path = '/content/drive/MyDrive/processed/final_concatenated_data_mice_imputed.csv'

try:
    df_columns = pd.read_csv(file_path, nrows=0).columns.tolist()
    print("Column names in the file:")
    for col in df_columns:
        print(col)
except FileNotFoundError:
    print(f"Error: File not found at {file_path}")
except Exception as e:
    print(f"An error occurred: {e}")

import pandas as pd

# Load the saved dataset with date column
file_path_with_date = '/content/Autoformer/dataset/custom/custom.csv'

try:
    df_with_date_loaded = pd.read_csv(file_path_with_date)
    print(f"Successfully loaded dataset from: {file_path_with_date}")
    print("\nFirst 5 rows of the data:")
    display(df_with_date_loaded.head())
except FileNotFoundError:
    print(f"Error: File not found at {file_path_with_date}")
except Exception as e:
    print(f"An error occurred while loading the file: {e}")

"""# Task
Use an Autoformer from Hugging Face Transformers to forecast multivariate targets using the data from "/content/Autoformer/dataset/custom/custom_with_date.csv".

## Install hugging face transformers

### Subtask:
Install the `transformers` library and any other necessary dependencies.

**Reasoning**:
The subtask is to install the necessary libraries for using Autoformer from Hugging Face Transformers. This involves installing the `transformers` library and potentially other related libraries.
"""

!pip install transformers datasets

"""## Adapt data preparation

### Subtask:
Modify the data loading and preparation steps to create datasets compatible with the input format expected by the Hugging Face Autoformer model (e.g., `timeseries_features`, `past_values`, `past_time_features`, `future_time_features`, `future_values`). This might involve creating appropriate time features and ensuring the data is in the correct shape and format.

**Reasoning**:
The previous training attempt failed due to dimension mismatch issues, likely stemming from data preparation or model configuration specific to the original Autoformer implementation. The current subtask is to prepare the data specifically for the Hugging Face Autoformer model, which has a different expected input format. I will load the dataset, prepare time features, split the data, and format it according to the Hugging Face `timeseries_features` structure.
"""

import pandas as pd
import numpy as np
from datetime import datetime
import torch

print("üîç Starting data preparation for Hugging Face Autoformer...")

# 1. Load the prepared dataset
file_path = './dataset/custom/custom.csv'
try:
    df = pd.read_csv(file_path)
    print(f"‚úÖ Successfully loaded {file_path}: {len(df)} rows, {len(df.columns)} columns")
except FileNotFoundError:
    print(f"‚ùå Error: File not found at {file_path}")
    # Finish the task with failure if the file is not found
    raise FileNotFoundError(f"Dataset file not found: {file_path}")
except Exception as e:
    print(f"‚ùå An error occurred while loading the file: {e}")
    # Finish the task with failure for other loading errors
    raise RuntimeError(f"Error loading dataset file: {e}")

# 2. Ensure 'date' column is datetime and set as index
if 'date' in df.columns:
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df = df.dropna(subset=['date']) # Drop rows where date parsing failed
    if df.empty:
        print("‚ùå Error: No valid date entries after parsing.")
        raise ValueError("No valid date entries after parsing.")
    df = df.set_index('date')
    print("‚úÖ 'date' column set as index")
else:
    print("‚ùå Error: 'date' column not found in the dataset.")
    raise ValueError("'date' column not found.")

# Sort by date index
df = df.sort_index()

# 3. Identify target and feature columns
# Get target columns from dataset_info or infer from the last columns
dataset_info_path = './dataset/custom/dataset_info.json'
target_cols_list = [
    'Total_Stabilized_Naphtha_Product_Flowrate',
    'Total_Kerosene_Product_Flowrate',
    'Jet_Fuel_Product_Train1_Flowrate',
    'Total_Light_Diesel_Product_Flowrate',
    'Total_Heavy_Diesel_Product_Flowrate',
    'Total_Atmospheric_Residue_Flowrate',
    'Blend_Yield_Gas & LPG',
    'Blend_Yield_Kerosene',
    'Blend_Yield_Light Diesel',
    'Blend_Yield_Heavy Diesel',
    'Blend_Yield_RCO'
]
existing_targets = [col for col in target_cols_list if col in df.columns]

if not existing_targets:
     # Fallback: use last 11 numeric columns as targets if info not found or targets missing
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    existing_targets = numeric_cols[-11:] if len(numeric_cols) >= 11 else numeric_cols[-len(numeric_cols)//2:]
    print(f"‚ö†Ô∏è  No predefined targets found, using last {len(existing_targets)} numeric columns as targets")

feature_cols = [col for col in df.columns if col not in existing_targets]

print(f"‚úÖ Identified {len(existing_targets)} target columns and {len(feature_cols)} feature columns.")

# Ensure all columns are numeric except the index
numeric_df = df[existing_targets + feature_cols].apply(pd.to_numeric, errors='coerce')
df_numeric = numeric_df.copy()

# 7. Handle any remaining missing values
if df_numeric.isnull().sum().sum() > 0:
    print("üîß Handling remaining missing values...")
    # Use forward fill then backward fill for time series data
    df_numeric = df_numeric.fillna(method='ffill').fillna(method='bfill')
    # Fill any remaining NaNs (e.g., at the very beginning) with 0
    df_numeric = df_numeric.fillna(0)
    print("‚úÖ Filled remaining missing values.")

# 8. Ensure data is float32
df_numeric = df_numeric.astype(np.float32)
print("‚úÖ Data converted to float32.")


# 4. Create past and future time features
# Assuming a fixed prediction length based on previous attempts or a reasonable value
# Let's use pred_len from the last training args if available, or default
try:
    pred_len = args.pred_len # From previous training attempt
    seq_len = args.seq_len
except NameError:
    print("‚ö†Ô∏è  'args' not found, using default seq_len=12, pred_len=3")
    seq_len = 12 # Default sequence length
    pred_len = 3 # Default prediction length

# We need time features for the entire dataset for splitting
def create_time_features(dates):
    return np.stack([
        dates.year,
        dates.month,
        dates.day,
        dates.hour,
        dates.minute,
        dates.dayofweek,
        dates.dayofyear,
        dates.weekofyear, # Add week of year
        dates.quarter     # Add quarter
    ], axis=1).astype(np.float32) # Ensure float32

all_time_features = create_time_features(df_numeric.index)

# 5. Split the data
# Simple time-based split: 80% train, 20% test
train_size = int(len(df_numeric) * 0.8)
train_df = df_numeric.iloc[:train_size]
test_df = df_numeric.iloc[train_size:]

train_time_features = all_time_features[:train_size]
test_time_features = all_time_features[train_size:]

print(f"‚úÖ Data split: Train {len(train_df)} samples, Test {len(test_df)} samples.")

# 6. Prepare data in Hugging Face format
# For Hugging Face, each sample is typically one sequence (seq_len + pred_len)
# We need to create sliding windows for training and testing.

# For training, we have 'past_values' (seq_len) and 'future_values' (pred_len)
# For testing, we have 'past_values' (seq_len) and need to predict 'future_values' (pred_len)

def create_hf_dataset_format(dataframe, time_features_array, seq_len, pred_len, is_training=True):
    past_values_list = []
    past_time_features_list = []
    future_time_features_list = []
    future_values_list = [] # Only for training

    total_length = len(dataframe)
    # Iterate through the dataframe to create sequences
    # A sequence starts at index i, past_values is from i to i+seq_len,
    # future_values/time_features is from i+seq_len to i+seq_len+pred_len
    for i in range(total_length - seq_len - pred_len + 1):
        past_values = dataframe.iloc[i : i + seq_len].values
        past_time_features = time_features_array[i : i + seq_len]

        future_time_features = time_features_array[i + seq_len : i + seq_len + pred_len]

        past_values_list.append(past_values)
        past_time_features_list.append(past_time_features)
        future_time_features_list.append(future_time_features)

        if is_training:
            future_values = dataframe.iloc[i + seq_len : i + seq_len + pred_len].values
            future_values_list.append(future_values)

    # Convert lists to numpy arrays
    past_values_np = np.array(past_values_list)
    past_time_features_np = np.array(past_time_features_list)
    future_time_features_np = np.array(future_time_features_list)
    future_values_np = np.array(future_values_list) if is_training else None

    return past_values_np, past_time_features_np, future_time_features_np, future_values_np

print(f"\nüîß Creating Hugging Face dataset format (seq_len={seq_len}, pred_len={pred_len})...")

# Create training dataset
train_past_values, train_past_time_features, train_future_time_features, train_future_values = create_hf_dataset_format(
    train_df, train_time_features, seq_len, pred_len, is_training=True
)

# Create testing dataset
test_past_values, test_past_time_features, test_future_time_features, _ = create_hf_dataset_format(
    test_df, test_time_features, seq_len, pred_len, is_training=False
)

# Store the processed data in a dictionary or similar structure
hf_data = {
    'train': {
        'past_values': train_past_values,
        'past_time_features': train_past_time_features,
        'future_time_features': train_future_time_features,
        'future_values': train_future_values,
    },
    'test': {
        'past_values': test_past_values,
        'past_time_features': test_past_time_features,
        'future_time_features': test_future_time_features,
        # 'future_values' is not needed for the test set input to the model
    },
    'target_columns': existing_targets,
    'feature_columns': feature_cols,
    'seq_len': seq_len,
    'pred_len': pred_len,
    'num_variates': df_numeric.shape[1], # Total number of time series (targets + features)
    'num_time_features': all_time_features.shape[1] # Number of time features
}

print("\n‚úÖ Data preparation for Hugging Face Autoformer completed.")
print("üîç Prepared data shapes:")
print(f"   - Train past_values: {hf_data['train']['past_values'].shape}")
print(f"   - Train past_time_features: {hf_data['train']['past_time_features'].shape}")
print(f"   - Train future_time_features: {hf_data['train']['future_time_features'].shape}")
print(f"   - Train future_values: {hf_data['train']['future_values'].shape}")
print(f"   - Test past_values: {hf_data['test']['past_values'].shape}")
print(f"   - Test past_time_features: {hf_data['test']['past_time_features'].shape}")
print(f"   - Test future_time_features: {hf_data['test']['future_time_features'].shape}")
print(f"   - Number of variates (time series): {hf_data['num_variates']}")
print(f"   - Number of time features: {hf_data['num_time_features']}")

"""**Reasoning**:
The previous code failed because `DatetimeIndex` objects in newer pandas versions do not have the `.weekofyear` attribute directly. It should be accessed via `.isocalendar().week`. I need to fix the `create_time_features` function to use the correct method for extracting the week of the year and then rerun the data preparation steps.


"""

# Fix the create_time_features function to use .isocalendar().week
def create_time_features(dates):
    return np.stack([
        dates.year,
        dates.month,
        dates.day,
        dates.hour,
        dates.minute,
        dates.dayofweek,
        dates.dayofyear,
        dates.isocalendar().week, # Correct way to get week of year
        dates.quarter
    ], axis=1).astype(np.float32) # Ensure float32

print("üîß Fixed create_time_features function.")

# Rerun the data preparation steps after fixing the function

print("üîç Starting data preparation for Hugging Face Autoformer (Rerun)...")

# 1. Load the prepared dataset (already loaded in previous step, but re-reading for clarity)
file_path = './dataset/custom/custom.csv'
try:
    df = pd.read_csv(file_path)
    print(f"‚úÖ Successfully loaded {file_path}: {len(df)} rows, {len(df.columns)} columns")
except FileNotFoundError:
    print(f"‚ùå Error: File not found at {file_path}")
    raise FileNotFoundError(f"Dataset file not found: {file_path}")
except Exception as e:
    print(f"‚ùå An error occurred while loading the file: {e}")
    raise RuntimeError(f"Error loading dataset file: {e}")

# 2. Ensure 'date' column is datetime and set as index
if 'date' in df.columns:
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df = df.dropna(subset=['date'])
    if df.empty:
        print("‚ùå Error: No valid date entries after parsing.")
        raise ValueError("No valid date entries after parsing.")
    df = df.set_index('date')
    print("‚úÖ 'date' column set as index")
else:
    print("‚ùå Error: 'date' column not found in the dataset.")
    raise ValueError("'date' column not found.")

# Sort by date index
df = df.sort_index()

# 3. Identify target and feature columns
dataset_info_path = './dataset/custom/dataset_info.json'
target_cols_list = [
    'Total_Stabilized_Naphtha_Product_Flowrate',
    'Total_Kerosene_Product_Flowrate',
    'Jet_Fuel_Product_Train1_Flowrate',
    'Total_Light_Diesel_Product_Flowrate',
    'Total_Heavy_Diesel_Product_Flowrate',
    'Total_Atmospheric_Residue_Flowrate',
    'Blend_Yield_Gas & LPG',
    'Blend_Yield_Kerosene',
    'Blend_Yield_Light Diesel',
    'Blend_Yield_Heavy Diesel',
    'Blend_Yield_RCO'
]
existing_targets = [col for col in target_cols_list if col in df.columns]

if not existing_targets:
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    existing_targets = numeric_cols[-11:] if len(numeric_cols) >= 11 else numeric_cols[-len(numeric_cols)//2:]
    print(f"‚ö†Ô∏è  No predefined targets found, using last {len(existing_targets)} numeric columns as targets")

feature_cols = [col for col in df.columns if col not in existing_targets]

print(f"‚úÖ Identified {len(existing_targets)} target columns and {len(feature_cols)} feature columns.")

numeric_df = df[existing_targets + feature_cols].apply(pd.to_numeric, errors='coerce')
df_numeric = numeric_df.copy()

# 7. Handle any remaining missing values
if df_numeric.isnull().sum().sum() > 0:
    print("üîß Handling remaining missing values...")
    df_numeric = df_numeric.fillna(method='ffill').fillna(method='bfill')
    df_numeric = df_numeric.fillna(0)
    print("‚úÖ Filled remaining missing values.")

# 8. Ensure data is float32
df_numeric = df_numeric.astype(np.float32)
print("‚úÖ Data converted to float32.")

# Use pred_len and seq_len from the previous successful run if available, else default
try:
    pred_len = args.pred_len
    seq_len = args.seq_len
    print(f"‚úÖ Using seq_len={seq_len}, pred_len={pred_len} from previous args.")
except NameError:
    print("‚ö†Ô∏è  'args' not found, using default seq_len=12, pred_len=3")
    seq_len = 12
    pred_len = 3


# 4. Create past and future time features
all_time_features = create_time_features(df_numeric.index)

# 5. Split the data
train_size = int(len(df_numeric) * 0.8)
train_df = df_numeric.iloc[:train_size]
test_df = df_numeric.iloc[train_size:]

train_time_features = all_time_features[:train_size]
test_time_features = all_time_features[train_size:]

print(f"‚úÖ Data split: Train {len(train_df)} samples, Test {len(test_df)} samples.")

# 6. Prepare data in Hugging Face format
def create_hf_dataset_format(dataframe, time_features_array, seq_len, pred_len, is_training=True):
    past_values_list = []
    past_time_features_list = []
    future_time_features_list = []
    future_values_list = []

    total_length = len(dataframe)
    for i in range(total_length - seq_len - pred_len + 1):
        past_values = dataframe.iloc[i : i + seq_len].values
        past_time_features = time_features_array[i : i + seq_len]

        future_time_features = time_features_array[i + seq_len : i + seq_len + pred_len]

        past_values_list.append(past_values)
        past_time_features_list.append(past_time_features)
        future_time_features_list.append(future_time_features)

        if is_training:
            future_values = dataframe.iloc[i + seq_len : i + seq_len + pred_len].values
            future_values_list.append(future_values)

    past_values_np = np.array(past_values_list)
    past_time_features_np = np.array(past_time_features_list)
    future_time_features_np = np.array(future_time_features_list)
    future_values_np = np.array(future_values_list) if is_training else None

    return past_values_np, past_time_features_np, future_time_features_np, future_values_np

print(f"\nüîß Creating Hugging Face dataset format (seq_len={seq_len}, pred_len={pred_len})...")

train_past_values, train_past_time_features, train_future_time_features, train_future_values = create_hf_dataset_format(
    train_df, train_time_features, seq_len, pred_len, is_training=True
)

test_past_values, test_past_time_features, test_future_time_features, _ = create_hf_dataset_format(
    test_df, test_time_features, seq_len, pred_len, is_training=False
)

hf_data = {
    'train': {
        'past_values': train_past_values,
        'past_time_features': train_past_time_features,
        'future_time_features': train_future_time_features,
        'future_values': train_future_values,
    },
    'test': {
        'past_values': test_past_values,
        'past_time_features': test_time_features, # Corrected: use test_time_features directly
        'future_time_features': test_future_time_features,
    },
    'target_columns': existing_targets,
    'feature_columns': feature_cols,
    'seq_len': seq_len,
    'pred_len': pred_len,
    'num_variates': df_numeric.shape[1],
    'num_time_features': all_time_features.shape[1]
}

print("\n‚úÖ Data preparation for Hugging Face Autoformer completed.")
print("üîç Prepared data shapes:")
print(f"   - Train past_values: {hf_data['train']['past_values'].shape}")
print(f"   - Train past_time_features: {hf_data['train']['past_time_features'].shape}")
print(f"   - Train future_time_features: {hf_data['train']['future_time_features'].shape}")
print(f"   - Train future_values: {hf_data['train']['future_values'].shape}")
print(f"   - Test past_values: {hf_data['test']['past_values'].shape}")
print(f"   - Test past_time_features: {hf_data['test']['past_time_features'].shape}")
print(f"   - Test future_time_features: {hf_data['test']['future_time_features'].shape}")
print(f"   - Number of variates (time series): {hf_data['num_variates']}")
print(f"   - Number of time features: {hf_data['num_time_features']}")

"""## Load and configure hugging face autoformer

### Subtask:
Load a pre-trained or randomly initialized Autoformer model from the Hugging Face `transformers` library. Configure the model with appropriate parameters based on the dataset dimensions (e.g., number of input features, number of output targets).

**Reasoning**:
Import the necessary classes from Hugging Face Transformers, define the model configuration based on the prepared data's dimensions, instantiate the model, and print the configuration and model summary to verify the setup.
"""

from transformers import AutoformerConfig, AutoformerForTimeSeriesForecasting
import torch # Make sure torch is imported

# Define the model configuration using parameters from hf_data
config = AutoformerConfig(
    prediction_length=hf_data['pred_len'],
    context_length=hf_data['seq_len'],
    # Assuming all variates are inputs, and we want to predict all targets
    # The model expects input_size as total number of variates
    input_size=hf_data['num_variates'],
    # The model expects num_outputs as the number of target variables to predict
    num_outputs=hf_data['num_variates'], # Predict all variates for simplicity first
    # Time features are handled by Autoformer's internal logic based on freq,
    # but input_size should match the dimension of the input time series (variates + time features if combined)
    # For AutoformerForTimeSeriesForecasting, time features are passed separately.
    # The number of time features is defined by the model based on freq.
    # We need to ensure the input_size in the config matches the last dimension of past_values.
    # The actual number of time features is derived from the frequency and time_features argument.
    # Let's set the input_size to the number of variates in the data.
    # AutoformerForTimeSeriesForecasting will handle time features internally.
    # Set d_model, n_heads, etc. to reasonable values, potentially matching previous run or defaults
    d_model=32, # Match the slightly larger model from previous attempt
    encoder_layers=1, # Minimal layers
    decoder_layers=1, # Minimal layers
    encoder_attention_heads=4, # Match the slightly larger model
    decoder_attention_heads=4,
    moving_avg=min(hf_data['seq_len'] - 1, 7), # Ensure moving_avg < context_length
    frequency="d", # Assuming daily frequency based on previous steps
    use_past_context=True,
)

print("‚úÖ AutoformerConfig defined:")
print(config)

# Instantiate the model
model = AutoformerForTimeSeriesForecasting(config)

print("\n‚úÖ AutoformerForTimeSeriesForecasting model instantiated.")

# Print model architecture summary
print("\nüîç Model Architecture Summary:")
print(model)

# Check if the model parameters match the configuration
print(f"\nModel input size: {model.config.input_size}")
print(f"Config input size: {config.input_size}")
print(f"Model num_outputs: {model.config.num_outputs}")
print(f"Config num_outputs: {config.num_outputs}")
print(f"Model prediction_length: {model.config.prediction_length}")
print(f"Config prediction_length: {config.prediction_length}")
print(f"Model context_length: {model.config.context_length}")
print(f"Config context_length: {config.context_length}")

"""## Load and configure hugging face autoformer

### Subtask:
Load a pre-trained or randomly initialized Autoformer model from the Hugging Face `transformers` library. Configure the model with appropriate parameters based on the dataset dimensions (e.g., number of input features, number of output targets).

**Reasoning**:
The previous attempt failed because `AutoformerForTimeSeriesForecasting` was not found. I need to check the available classes in the `transformers` library to find the correct one for time series forecasting with Autoformer, or confirm if Autoformer is supported for this task in the current version. If a specific forecasting class isn't available, I might need to use the base `AutoformerModel` and handle the forecasting layer manually, but first, I will try to list available classes.
"""

from transformers import AutoformerConfig
import inspect

# List classes in transformers that contain "Autoformer"
autoformer_classes = [name for name, obj in inspect.getmembers(__import__("transformers"), inspect.isclass) if "Autoformer" in name]
print("Classes containing 'Autoformer' in transformers:")
for cls_name in autoformer_classes:
    print(cls_name)

# Check if AutoformerModel exists
try:
    from transformers import AutoformerModel
    print("\n'AutoformerModel' class is available.")
except ImportError:
    print("\n'AutoformerModel' class is NOT available.")

# Check if AutoformerForPrediction or similar exists
try:
    from transformers import AutoformerForPrediction
    print("'AutoformerForPrediction' class is available.")
except ImportError:
    print("'AutoformerForPrediction' class is NOT available.")

try:
    from transformers import AutoformerForPredictionHead
    print("'AutoformerForPredictionHead' class is available.")
except ImportError:
    print("'AutoformerForPredictionHead' class is NOT available.")

"""**Reasoning**:
The previous step identified that `AutoformerForPrediction` and `AutoformerModel` are available. `AutoformerForPrediction` is likely the correct class for forecasting. I will proceed to load and configure `AutoformerForPrediction` using the dataset dimensions and conservative model parameters as requested in the instructions.


"""

from transformers import AutoformerConfig, AutoformerForPrediction # Import the identified class

# Define the model configuration using parameters from hf_data
# Use AutoformerForPrediction which is designed for forecasting
config = AutoformerConfig(
    prediction_length=hf_data['pred_len'],
    context_length=hf_data['seq_len'],
    # input_size should be the number of time series (variates)
    input_size=hf_data['num_variates'],
    # num_outputs should be the number of target variables to predict
    # Since we want to predict all variates for simplicity first, set num_outputs to num_variates
    num_outputs=hf_data['num_variates'],
    # Time features are handled internally based on freq and time_features argument
    # We specify the frequency as 'd' (daily)
    frequency="d",
    # Use conservative model dimensions
    d_model=32,
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=4,
    decoder_attention_heads=4,
    # Ensure moving_avg is less than context_length
    moving_avg=min(hf_data['seq_len'] - 1, 7),
    use_past_context=True, # Keep this as it is standard for forecasting tasks
)

print("‚úÖ AutoformerConfig defined:")
print(config)

# Instantiate the model using AutoformerForPrediction
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated.")

# Print model architecture summary
print("\nüîç Model Architecture Summary:")
print(model)

# Verify the setup by printing key configuration parameters
print(f"\nModel input size: {model.config.input_size}")
print(f"Config input size: {config.input_size}")
print(f"Model num_outputs: {model.config.num_outputs}")
print(f"Config num_outputs: {config.num_outputs}")
print(f"Model prediction_length: {model.config.prediction_length}")
print(f"Config prediction_length: {config.prediction_length}")
print(f"Model context_length: {model.config.context_length}")
print(f"Config context_length: {config.context_length}")

"""## Implement training loop

### Subtask:
Write a custom training loop using PyTorch (or TensorFlow, if preferred) to train the Hugging Face Autoformer model on your prepared data. This will involve defining an optimizer, a loss function suitable for time series forecasting, and iterating through the data in batches.

**Reasoning**:
Import necessary PyTorch libraries and define the optimizer and loss function.
"""

import torch.optim as optim
import torch.nn as nn
import torch

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# Define the optimizer (using AdamW)
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) defined with learning rate 0.001")

# Define the loss function (using Mean Squared Error)
criterion = nn.MSELoss()
print("Loss function (MSELoss) defined")

# Convert numpy arrays from hf_data to PyTorch Tensors
train_past_values_tensor = torch.tensor(hf_data['train']['past_values'], dtype=torch.float32)
train_past_time_features_tensor = torch.tensor(hf_data['train']['past_time_features'], dtype=torch.float32)
train_future_time_features_tensor = torch.tensor(hf_data['train']['future_time_features'], dtype=torch.float32)
train_future_values_tensor = torch.tensor(hf_data['train']['future_values'], dtype=torch.float32)

print("\nConverted training data to PyTorch Tensors:")
print(f"  - train_past_values_tensor shape: {train_past_values_tensor.shape}")
print(f"  - train_past_time_features_tensor shape: {train_past_time_features_tensor.shape}")
print(f"  - train_future_time_features_tensor shape: {train_future_time_features_tensor.shape}")
print(f"  - train_future_values_tensor shape: {train_future_values_tensor.shape}")

"""**Reasoning**:
Implement the training loop, iterating through batches, performing forward and backward passes, and calculating the loss.


"""

from torch.utils.data import Dataset, DataLoader

# Create a custom Dataset for the training data
class TimeSeriesDataset(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = past_values
        self.past_time_features = past_time_features
        self.future_time_features = future_time_features
        self.future_values = future_values

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx],
        )

# Create the Dataset and DataLoader
train_dataset = TimeSeriesDataset(
    train_past_values_tensor,
    train_past_time_features_tensor,
    train_future_time_features_tensor,
    train_future_values_tensor,
)

# Use a smaller batch size to avoid potential memory issues and match previous settings
batch_size = 3
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

print(f"\nCreated TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of batches in training DataLoader: {len(train_dataloader)}")

# Implement the training loop
num_epochs = 5 # Keep epochs low for initial testing

print(f"\nStarting training for {num_epochs} epochs...")

model.train() # Set the model to training mode

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        # The model expects inputs: past_values, past_time_features, future_time_features
        # and uses future_values for loss calculation internally during training.
        # Pass future_values to the model's forward pass for loss calculation
        outputs = model(
            past_values=past_values,
            past_time_features=past_time_features,
            future_time_features=future_time_features,
            future_values=future_values # Provided as labels for loss calculation
        )

        # The output of the forward pass includes the loss when future_values is provided
        loss = outputs.loss

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Print loss every few batches
        if (i + 1) % 100 == 0:
            print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}")

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished.")

"""**Reasoning**:
The error message indicates that the `past_observed_mask` argument is missing in the forward pass. This mask is required to indicate which values in `past_values` are observed (not missing). Create a mask assuming all past values are observed and pass it to the model.


"""

from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch
import numpy as np # Import numpy to create the mask

# Create a custom Dataset for the training data
class TimeSeriesDataset(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = past_values
        self.past_time_features = past_time_features
        self.future_time_features = future_time_features
        self.future_values = future_values

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx],
        )

# Assuming the tensors are already created from the previous step
# train_past_values_tensor, train_past_time_features_tensor, train_future_time_features_tensor, train_future_values_tensor

# Create the Dataset and DataLoader
train_dataset = TimeSeriesDataset(
    train_past_values_tensor,
    train_past_time_features_tensor,
    train_future_time_features_tensor,
    train_future_values_tensor,
)

# Use a smaller batch size to avoid potential memory issues and match previous settings
batch_size = 3
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

print(f"\nCreated TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of batches in training DataLoader: {len(train_dataloader)}")

# Implement the training loop
num_epochs = 5 # Keep epochs low for initial testing

print(f"\nStarting training for {num_epochs} epochs...")

model.train() # Set the model to training mode
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Re-define device

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device)

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        # The mask should have the same shape as past_values and be True where observed, False where missing.
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)


        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        # Pass future_values to the model's forward pass for loss calculation
        outputs = model(
            past_values=past_values,
            past_time_features=past_time_features,
            future_time_features=future_time_features,
            past_observed_mask=past_observed_mask, # Add the required mask
            future_values=future_values # Provided as labels for loss calculation
        )

        # The output of the forward pass includes the loss when future_values is provided
        loss = outputs.loss

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Print loss every few batches
        if (i + 1) % 100 == 0:
            print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}")

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished.")

"""**Reasoning**:
The error "lags cannot go further than history length" suggests an issue with the `lags_sequence` configuration in the model relative to the provided `context_length`. The default lags in Autoformer are likely exceeding the `context_length` (seq_len) + `prediction_length`. The Autoformer paper uses specific lags (e.g., [1, 2, 3, 4, 5, 6, 7, 8, 16, 32, 64, 128]). The current configuration has `context_length` (seq_len) of 12 and `prediction_length` of 3, for a total effective history of 15. A lag of 7 is fine, but the error message indicates a maximum lag of 7 with a history of 15, which seems contradictory unless the model's internal calculation of history length is different or there are larger default lags. To fix this, explicitly define the `lags_sequence` in the `AutoformerConfig` with values that are guaranteed to be less than or equal to the `context_length`. Let's use a simple sequence of lags up to `context_length`.


"""

from transformers import AutoformerConfig, AutoformerForPrediction
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch
import numpy as np

# Re-define the model configuration with explicit lags_sequence
# Use parameters from hf_data and previous successful config
seq_len = hf_data['seq_len'] # Should be 12
pred_len = hf_data['pred_len'] # Should be 3
num_variates = hf_data['num_variates'] # Should be 667

# Define lags_sequence to be explicitly within the context length
# A simple sequence of lags up to the context length
lags_sequence = list(range(1, seq_len + 1))
print(f"Using lags_sequence: {lags_sequence}")


config = AutoformerConfig(
    prediction_length=pred_len,
    context_length=seq_len,
    input_size=num_variates,
    num_outputs=num_variates, # Predict all variates
    frequency="d",
    d_model=32,
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=4,
    decoder_attention_heads=4,
    moving_avg=min(seq_len - 1, 7),
    use_past_context=True,
    lags_sequence=lags_sequence, # Add the explicit lags sequence
)

print("\n‚úÖ AutoformerConfig re-defined with explicit lags_sequence:")
print(config)

# Instantiate the model using the new config
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated with new config.")

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# Re-define the optimizer (using AdamW) for the new model instance
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) re-defined for the new model instance")

# Define the loss function (using Mean Squared Error) - already defined, but kept for clarity
criterion = nn.MSELoss()
print("Loss function (MSELoss) defined")

# Assuming the tensors are already created from the previous step
# train_past_values_tensor, train_past_time_features_tensor, train_future_time_features_tensor, train_future_values_tensor

# Create the Dataset and DataLoader - already created, but kept for clarity
class TimeSeriesDataset(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = past_values
        self.past_time_features = past_time_features
        self.future_time_features = future_time_features
        self.future_values = future_values

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx],
        )

train_dataset = TimeSeriesDataset(
    train_past_values_tensor,
    train_past_time_features_tensor,
    train_future_time_features_tensor,
    train_future_values_tensor,
)

batch_size = 3 # Match previous batch size
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

print(f"\nCreated TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of batches in training DataLoader: {len(train_dataloader)}")


# Implement the training loop
num_epochs = 5 # Keep epochs low for initial testing

print(f"\nStarting training for {num_epochs} epochs...")

model.train() # Set the model to training mode

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device)

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        # Pass future_values to the model's forward pass for loss calculation
        outputs = model(
            past_values=past_values,
            past_time_features=past_time_features,
            future_time_features=future_time_features,
            past_observed_mask=past_observed_mask,
            future_values=future_values # Provided as labels for loss calculation
        )

        # The output of the forward pass includes the loss when future_values is provided
        loss = outputs.loss

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Print loss every few batches
        if (i + 1) % 100 == 0:
            print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}")

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished.")

"""**Reasoning**:
The error persists even after explicitly setting `lags_sequence` to `[1, ..., seq_len]`. The error message says "found lag 12 while history length is only 15", where history length is `context_length` + `prediction_length` (12 + 3 = 15). This implies that the maximum lag used internally by the model's `get_lagged_subsequences` function is compared against `sequence_length`, which seems to be the `context_length` (12) plus the `prediction_length` (3) for some reason, resulting in 15. However, a lag of 12 should be okay given a total length of 15. The error might be related to how the model uses the lags relative to the *start* of the combined `past_values` and `future_values` sequence, or an internal detail of the `get_lagged_subsequences` implementation.

Let's try a more conservative `lags_sequence` that is strictly less than the `context_length`. For example, lags from 1 up to `context_length - 1`. Also, the default `moving_average` is 25 in the config, which is larger than `seq_len` (12). Let's explicitly set `moving_average` to be less than `seq_len` as well, similar to the previous robust training attempt.


"""

from transformers import AutoformerConfig, AutoformerForPrediction
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch
import numpy as np

# Re-define the model configuration with explicit and more conservative lags_sequence and moving_average
seq_len = hf_data['seq_len'] # Should be 12
pred_len = hf_data['pred_len'] # Should be 3
num_variates = hf_data['num_variates'] # Should be 667

# Define lags_sequence to be strictly within the context length
lags_sequence = list(range(1, seq_len)) # Use lags from 1 up to seq_len - 1 (i.e., 1 to 11)
print(f"Using lags_sequence: {lags_sequence}")

# Define a conservative moving_average that is less than seq_len
conservative_moving_avg = min(seq_len - 1, 7)
print(f"Using moving_average: {conservative_moving_avg}")


config = AutoformerConfig(
    prediction_length=pred_len,
    context_length=seq_len,
    input_size=num_variates,
    num_outputs=num_variates, # Predict all variates
    frequency="d",
    d_model=32,
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=4,
    decoder_attention_heads=4,
    moving_avg=conservative_moving_avg, # Set the conservative moving average
    use_past_context=True,
    lags_sequence=lags_sequence, # Add the explicit lags sequence
)

print("\n‚úÖ AutoformerConfig re-defined with conservative lags_sequence and moving_average:")
print(config)

# Instantiate the model using the new config
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated with new config.")

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# Re-define the optimizer (using AdamW) for the new model instance
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) re-defined for the new model instance")

# Define the loss function (using Mean Squared Error) - already defined, but kept for clarity
criterion = nn.MSELoss()
print("Loss function (MSELoss) defined")

# Assuming the tensors are already created from the previous step
# train_past_values_tensor, train_past_time_features_tensor, train_future_time_features_tensor, train_future_values_tensor

# Create the Dataset and DataLoader - already created, but kept for clarity
class TimeSeriesDataset(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = past_values
        self.past_time_features = past_time_features
        self.future_time_features = future_time_features
        self.future_values = future_values

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx],
        )

train_dataset = TimeSeriesDataset(
    train_past_values_tensor,
    train_past_time_features_tensor,
    train_future_time_features_tensor,
    train_future_values_tensor,
)

batch_size = 3 # Match previous batch size
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

print(f"\nCreated TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of batches in training DataLoader: {len(train_dataloader)}")


# Implement the training loop
num_epochs = 5 # Keep epochs low for initial testing

print(f"\nStarting training for {num_epochs} epochs...")

model.train() # Set the model to training mode

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device)

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        # Pass future_values to the model's forward pass for loss calculation
        outputs = model(
            past_values=past_values,
            past_time_features=past_time_features,
            future_time_features=future_time_features,
            past_observed_mask=past_observed_mask,
            future_values=future_values # Provided as labels for loss calculation
        )

        # The output of the forward pass includes the loss when future_values is provided
        loss = outputs.loss

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Print loss every few batches
        if (i + 1) % 100 == 0:
            print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}")

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished.")

"""**Reasoning**:
The "lags cannot go further than history length" error persists even with a reduced `lags_sequence`. The error message indicates that a lag of 11 is found, while history length is 15 (`context_length` + `prediction_length`). This suggests that the maximum allowed lag might be `context_length` itself (12), or that the `get_lagged_subsequences` function is being called on a sequence effectively limited to `context_length`. Let's try setting the maximum lag to be `context_length - 1` (11) as we did, but also consider the possibility that the internal history length calculation is stricter. A very safe approach would be to use only a few small lags. Let's try a very small, fixed set of lags, e.g., `[1, 2, 7]` (daily, weekly lags) which should definitely be within the `context_length` of 12. We will also keep the conservative `moving_average`.


"""

from transformers import AutoformerConfig, AutoformerForPrediction
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch
import numpy as np

# Re-define the model configuration with a very conservative lags_sequence
seq_len = hf_data['seq_len'] # Should be 12
pred_len = hf_data['pred_len'] # Should be 3
num_variates = hf_data['num_variates'] # Should be 667

# Define a very conservative lags_sequence
lags_sequence = [1, 2, 7] # Daily and weekly lags, well within context_length
print(f"Using lags_sequence: {lags_sequence}")

# Define a conservative moving_average that is less than seq_len
conservative_moving_avg = min(seq_len - 1, 7)
print(f"Using moving_average: {conservative_moving_avg}")


config = AutoformerConfig(
    prediction_length=pred_len,
    context_length=seq_len,
    input_size=num_variates,
    num_outputs=num_variates, # Predict all variates
    frequency="d",
    d_model=32,
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=4,
    decoder_attention_heads=4,
    moving_avg=conservative_moving_avg, # Set the conservative moving average
    use_past_context=True,
    lags_sequence=lags_sequence, # Add the explicit lags sequence
)

print("\n‚úÖ AutoformerConfig re-defined with very conservative lags_sequence and moving_average:")
print(config)

# Instantiate the model using the new config
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated with new config.")

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# Re-define the optimizer (using AdamW) for the new model instance
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) re-defined for the new model instance")

# Define the loss function (using Mean Squared Error) - already defined, but kept for clarity
criterion = nn.MSELoss()
print("Loss function (MSELoss) defined")

# Assuming the tensors are already created from the previous step
# train_past_values_tensor, train_past_time_features_tensor, train_future_time_features_tensor, train_future_values_tensor

# Create the Dataset and DataLoader - already created, but kept for clarity
class TimeSeriesDataset(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = past_values
        self.past_time_features = past_time_features
        self.future_time_features = future_time_features
        self.future_values = future_values

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx],
        )

train_dataset = TimeSeriesDataset(
    train_past_values_tensor,
    train_past_time_features_tensor,
    train_future_time_features_tensor,
    train_future_values_tensor,
)

batch_size = 3 # Match previous batch size
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

print(f"\nCreated TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of batches in training DataLoader: {len(train_dataloader)}")


# Implement the training loop
num_epochs = 5 # Keep epochs low for initial testing

print(f"\nStarting training for {num_epochs} epochs...")

model.train() # Set the model to training mode

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device)

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        # Pass future_values to the model's forward pass for loss calculation
        outputs = model(
            past_values=past_values,
            past_time_features=past_time_features,
            future_time_features=future_time_features,
            past_observed_mask=past_observed_mask,
            future_values=future_values # Provided as labels for loss calculation
        )

        # The output of the forward pass includes the loss when future_values is provided
        loss = outputs.loss

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Print loss every few batches
        if (i + 1) % 100 == 0:
            print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}")

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished.")

"""**Reasoning**:
The "lags cannot go further than history length" error persists even with very small lags (e.g., 7). The error message consistently reports a history length of 15 (`context_length` + `prediction_length`). This suggests that the `get_lagged_subsequences` function might be expecting lags relative to the *end* of the `context_length`, and the total sequence length it considers is indeed `context_length + prediction_length`. In this case, the maximum lag `l` must satisfy `l + prediction_length <= context_length + prediction_length`, which simplifies to `l <= context_length`. However, the error message says "found lag 7 while history length is only 15". This is confusing, as lag 7 is less than 12, and 12 is less than 15.

There might be an internal constraint or a misunderstanding of what `history_length` refers to in this specific part of the code. Given that even small lags trigger the error, let's explore the possibility that the issue is not with the *values* of the lags themselves, but how the `lags_sequence` is being interpreted or combined with other parameters internally.

The Autoformer paper and implementations often use specific lags based on frequency. Maybe the default lags generated when `lags_sequence` is not explicitly provided are the ones causing the issue, and setting `lags_sequence` is supposed to *override* this, but isn't doing so correctly, or the provided sequence is still being validated against some internal rule.

Let's try a different approach: instead of explicitly setting `lags_sequence`, let the model use its default lags based on the `frequency`. It's possible the conflict arises when we manually set `lags_sequence` in a way that clashes with other internal Autoformer logic. We will remove the `lags_sequence` from the config and rely on the default behavior for 'd' frequency. We will keep the conservative `moving_average` setting, as that seemed less likely to be the primary cause of the lag error but is still a good practice for stability.


"""

from transformers import AutoformerConfig, AutoformerForPrediction
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch
import numpy as np

# Re-define the model configuration, REMOVING explicit lags_sequence
seq_len = hf_data['seq_len'] # Should be 12
pred_len = hf_data['pred_len'] # Should be 3
num_variates = hf_data['num_variates'] # Should be 667

# Define a conservative moving_average that is less than seq_len
conservative_moving_avg = min(seq_len - 1, 7)
print(f"Using moving_average: {conservative_moving_avg}")

config = AutoformerConfig(
    prediction_length=pred_len,
    context_length=seq_len,
    input_size=num_variates,
    num_outputs=num_variates, # Predict all variates
    frequency="d", # Keep frequency as 'd'
    d_model=32,
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=4,
    decoder_attention_heads=4,
    moving_avg=conservative_moving_avg, # Set the conservative moving average
    use_past_context=True,
    # REMOVED lags_sequence = lags_sequence, # Remove explicit lags_sequence
)

print("\n‚úÖ AutoformerConfig re-defined, REMOVING explicit lags_sequence:")
print(config)

# Instantiate the model using the new config
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated with new config.")

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# Re-define the optimizer (using AdamW) for the new model instance
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) re-defined for the new model instance")

# Define the loss function (using Mean Squared Error) - already defined, but kept for clarity
criterion = nn.MSELoss()
print("Loss function (MSELoss) defined")

# Assuming the tensors are already created from the previous step
# train_past_values_tensor, train_past_time_features_tensor, train_future_time_features_tensor, train_future_values_tensor

# Create the Dataset and DataLoader - already created, but kept for clarity
class TimeSeriesDataset(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = past_values
        self.past_time_features = past_time_features
        self.future_time_features = future_time_features
        self.future_values = future_values

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx],
        )

train_dataset = TimeSeriesDataset(
    train_past_values_tensor,
    train_past_time_features_tensor,
    train_future_time_features_tensor,
    train_future_values_tensor,
)

batch_size = 3 # Match previous batch size
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

print(f"\nCreated TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of batches in training DataLoader: {len(train_dataloader)}")


# Implement the training loop
num_epochs = 5 # Keep epochs low for initial testing

print(f"\nStarting training for {num_epochs} epochs...")

model.train() # Set the model to training mode

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device)

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        # Pass future_values to the model's forward pass for loss calculation
        outputs = model(
            past_values=past_values,
            past_time_features=past_time_features,
            future_time_features=future_time_features,
            past_observed_mask=past_observed_mask,
            future_values=future_values # Provided as labels for loss calculation
        )

        # The output of the forward pass includes the loss when future_values is provided
        loss = outputs.loss

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Print loss every few batches
        if (i + 1) % 100 == 0:
            print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}")

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished.")

"""## Implement training loop - retry 1

### Subtask:
Implement training loop - Retry 1: Write a custom training loop using PyTorch to train the Hugging Face Autoformer model on your prepared data, specifically addressing the "lags cannot go further than history length" error by further investigating and potentially adjusting model configuration or data handling.

**Reasoning**:
The previous attempt to implement the training loop failed due to a persistent "lags cannot go further than history length" error within the Hugging Face Autoformer model's forward pass. This error seems related to how the model calculates or uses lags based on the provided `context_length` and `prediction_length`. To address this, I will first review the error details more closely, adjust the model configuration parameters (`context_length`, `prediction_length`) to potentially safer values, and then re-instantiate the model and set up the optimizer and loss function before attempting the training loop again with the adjusted configuration. I will prioritize making the configuration minimal and safe to avoid this specific error.
"""

from transformers import AutoformerConfig, AutoformerForPrediction
import torch.optim as optim
import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

# --- Error Review and Configuration Adjustment ---
# The error "lags cannot go further than history length" suggests that the model's internal calculation
# of the maximum usable lag is less than one or more lags in the `lags_sequence`.
# The history length appears to be context_length + prediction_length.
# The default lags for frequency 'd' are [1, 2, 3, 4, 5, 6, 7].
# With context_length=12 and prediction_length=3, history length is 15.
# Lag 7 should be acceptable (7 < 15). The error message "lags cannot go further than history length (15)"
# when the problematic lag is reported as 7 is counter-intuitive and suggests a deeper issue or
# misinterpretation of "history length" within the model's internal lag calculation.

# Given the persistent nature of the error even with small lags, let's try to
# significantly reduce the context_length and prediction_length to see if that resolves the issue.
# A minimal configuration might help isolate if the problem is with the data shape
# or the model's handling of sequence lengths above a certain threshold.

# Reduce sequence and prediction lengths dramatically
seq_len = 6 # Reduced from 12
pred_len = 1 # Reduced from 3

# Recalculate conservative moving_average based on new seq_len
conservative_moving_avg = min(seq_len - 1, 7)
print(f"Adjusted seq_len: {seq_len}, pred_len: {pred_len}")
print(f"Recalculated moving_average: {conservative_moving_avg}")

# Re-define the model configuration with reduced lengths and calculated moving_avg
num_variates = hf_data['num_variates'] # Should be 667

config = AutoformerConfig(
    prediction_length=pred_len,
    context_length=seq_len,
    input_size=num_variates,
    num_outputs=num_variates, # Still predict all variates for now
    frequency="d", # Keep frequency as 'd'
    d_model=16, # Further reduce model size
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=2, # Further reduce heads
    decoder_attention_heads=2,
    moving_avg=conservative_moving_avg, # Set the conservative moving average
    use_past_context=True,
    # Still avoid explicit lags_sequence
)

print("\n‚úÖ AutoformerConfig re-defined with reduced lengths and model size:")
print(config)

# --- Instantiate Model and Setup ---
# Instantiate the model using the new config
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated with new config.")

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# Re-define the optimizer (using AdamW) for the new model instance
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) re-defined for the new model instance")

# Define the loss function (using Mean Squared Error)
criterion = nn.MSELoss()
print("Loss function (MSELoss) defined")

# --- Prepare Data for new sequence/prediction lengths ---
# Need to regenerate the dataset tensors based on the new seq_len and pred_len
def create_hf_dataset_format_adjusted(dataframe, time_features_array, seq_len, pred_len, is_training=True):
    past_values_list = []
    past_time_features_list = []
    future_time_features_list = []
    future_values_list = []

    total_length = len(dataframe)
    # Adjusted loop range based on new sequence lengths
    for i in range(total_length - seq_len - pred_len + 1):
        past_values = dataframe.iloc[i : i + seq_len].values
        past_time_features = time_features_array[i : i + seq_len]

        future_time_features = time_features_array[i + seq_len : i + seq_len + pred_len]

        past_values_list.append(past_values)
        past_time_features_list.append(past_time_features)
        future_time_features_list.append(future_time_features)

        if is_training:
            future_values = dataframe.iloc[i + seq_len : i + seq_len + pred_len].values
            future_values_list.append(future_values)

    past_values_np = np.array(past_values_list)
    past_time_features_np = np.array(past_time_features_list)
    future_time_features_np = np.array(future_time_features_list)
    future_values_np = np.array(future_values_list) if is_training else None

    return past_values_np, past_time_features_np, future_time_features_np, future_values_np

print(f"\nüîß Re-creating Hugging Face dataset format with adjusted seq_len={seq_len}, pred_len={pred_len}...")

# Load the numeric dataframe and time features (assuming they are available from previous steps)
# If not available, need to re-run data loading and preparation
# For this retry, assume df_numeric and all_time_features are available
# from the previous successful data prep step.
# If not, uncomment the following lines to reload/recreate
# file_path = './dataset/custom/custom.csv'
# df_full = pd.read_csv(file_path)
# df_full['date'] = pd.to_datetime(df_full['date'], errors='coerce')
# df_full = df_full.dropna(subset=['date']).set_index('date').sort_index()
# target_cols_list = ['Total_Stabilized_Naphtha_Product_Flowrate', 'Total_Kerosene_Product_Flowrate', 'Jet_Fuel_Product_Train1_Flowrate', 'Total_Light_Diesel_Product_Flowrate', 'Total_Heavy_Diesel_Product_Flowrate', 'Total_Atmospheric_Residue_Flowrate', 'Blend_Yield_Gas & LPG', 'Blend_Yield_Kerosene', 'Blend_Yield_Light Diesel', 'Blend_Yield_Heavy Diesel', 'Blend_Yield_RCO']
# existing_targets = [col for col in target_cols_list if col in df_full.columns]
# if not existing_targets:
#      numeric_cols = df_full.select_dtypes(include=[np.number]).columns.tolist()
#      existing_targets = numeric_cols[-11:] if len(numeric_cols) >= 11 else numeric_cols[-len(numeric_cols)//2:]
# feature_cols = [col for col in df_full.columns if col not in existing_targets]
# df_numeric = df_full[existing_targets + feature_cols].apply(pd.to_numeric, errors='coerce').fillna(method='ffill').fillna(method='bfill').fillna(0).astype(np.float32)
# all_time_features = create_time_features(df_numeric.index)

# Split the data based on the new lengths
train_size = int(len(df_numeric) * 0.8)
train_df_adjusted = df_numeric.iloc[:train_size]
test_df_adjusted = df_numeric.iloc[train_size:]

train_time_features_adjusted = all_time_features[:train_size]
test_time_features_adjusted = all_time_features[train_size:]

# Create adjusted tensors
train_past_values_tensor_adj, train_past_time_features_tensor_adj, train_future_time_features_tensor_adj, train_future_values_tensor_adj = create_hf_dataset_format_adjusted(
    train_df_adjusted, train_time_features_adjusted, seq_len, pred_len, is_training=True
)

# Create adjusted Dataset and DataLoader
class TimeSeriesDatasetAdjusted(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = past_values
        self.past_time_features = past_time_features
        self.future_time_features = future_time_features
        self.future_values = future_values

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx],
        )

train_dataset_adj = TimeSeriesDatasetAdjusted(
    train_past_values_tensor_adj,
    train_past_time_features_tensor_adj,
    train_future_time_features_tensor_adj,
    train_future_values_tensor_adj,
)

batch_size = 3 # Keep batch size low
train_dataloader_adj = DataLoader(train_dataset_adj, batch_size=batch_size, shuffle=True)

print(f"\nCreated Adjusted TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of samples in adjusted training dataset: {len(train_dataset_adj)}")
print(f"Number of batches in adjusted training DataLoader: {len(train_dataloader_adj)}")


# --- Implement Training Loop (Retry) ---
num_epochs = 3 # Keep epochs low for initial testing

print(f"\nStarting training retry for {num_epochs} epochs with adjusted config...")

model.train() # Set the model to training mode

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader_adj):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device)

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        try:
            # Forward pass
            # Pass future_values to the model's forward pass for loss calculation
            outputs = model(
                past_values=past_values,
                past_time_features=past_time_features,
                future_time_features=future_time_features,
                past_observed_mask=past_observed_mask,
                future_values=future_values # Provided as labels for loss calculation
            )

            # The output of the forward pass includes the loss when future_values is provided
            loss = outputs.loss

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Print loss every few batches
            if (i + 1) % 50 == 0: # Print more frequently with smaller dataset
                print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader_adj)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
            print("Batch shapes:")
            print(f"  past_values: {past_values.shape}")
            print(f"  past_time_features: {past_time_features.shape}")
            print(f"  future_time_features: {future_time_features.shape}")
            print(f"  future_values: {future_values.shape}")
            print(f"  past_observed_mask: {past_observed_mask.shape}")
            # Re-raise the exception to stop training
            raise

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader_adj)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished (Retry).")

"""**Reasoning**:
The "lags cannot go further than history length" error persists even with significantly reduced `context_length` (6) and `prediction_length` (1). The error message now states "found lag 7 while history length is only 7". This is confusing because the configured `context_length` is 6 and `prediction_length` is 1, which should result in a history length of 7 (6+1). The default lags sequence for frequency 'd' is [1, 2, 3, 4, 5, 6, 7]. The error correctly identifies the history length as 7 but incorrectly flags lag 7 as going "further" than history length. This strongly suggests an internal calculation issue within the Hugging Face Autoformer model's `get_lagged_subsequences` function when `max(lags_sequence)` is equal to the history length, or a mismatch in how history length is defined or used internally versus externally.

To attempt to resolve this specific error, I will explicitly set the `lags_sequence` in the `AutoformerConfig` to exclude lag 7, ensuring that the maximum lag (which will be 6) is strictly less than the history length (7). I will then re-instantiate the model and attempt the training loop again.


"""

from transformers import AutoformerConfig, AutoformerForPrediction
import torch.optim as optim
import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

# --- Error Analysis and Configuration Adjustment (Attempt 3) ---
# Error persists with seq_len=6, pred_len=1, history_length=7, and lag 7.
# The error message "found lag 7 while history length is only 7" is key.
# It seems the model's internal check fails when max(lags_sequence) == history_length.
# To work around this, I will explicitly set a lags_sequence that excludes the maximum default lag (7).

seq_len = 6 # Keep reduced length
pred_len = 1 # Keep reduced length

# Define an explicit lags_sequence excluding lag 7
# Default lags for 'd' are [1, 2, 3, 4, 5, 6, 7]
explicit_lags_sequence = [1, 2, 3, 4, 5, 6] # Exclude lag 7

# Recalculate conservative moving_average (already 5, which is < seq_len-1)
conservative_moving_avg = min(seq_len - 1, 7) # seq_len-1 = 5, so moving_avg = 5

print(f"Keeping seq_len: {seq_len}, pred_len: {pred_len}")
print(f"Using explicit_lags_sequence: {explicit_lags_sequence}")
print(f"Moving average: {conservative_moving_avg}")


num_variates = hf_data['num_variates'] # Should be 667

config = AutoformerConfig(
    prediction_length=pred_len,
    context_length=seq_len,
    input_size=num_variates,
    num_outputs=num_variates,
    frequency="d",
    d_model=16,
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=2,
    decoder_attention_heads=2,
    moving_avg=conservative_moving_avg,
    use_past_context=True,
    lags_sequence=explicit_lags_sequence, # Explicitly set the lags sequence
)

print("\n‚úÖ AutoformerConfig re-defined with explicit_lags_sequence:")
print(config)

# --- Instantiate Model and Setup ---
# Instantiate the model using the new config
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated with new config.")

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# Re-define the optimizer (using AdamW) for the new model instance
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) re-defined for the new model instance")

# Define the loss function (using Mean Squared Error)
criterion = nn.MSELoss()
print("Loss function (MSELoss) defined")

# --- Data Preparation (using the adjusted data tensors and dataloader) ---
# The data tensors and dataloader (train_dataset_adj, train_dataloader_adj)
# created in the previous step are already based on seq_len=6 and pred_len=1.
# We will reuse them.

print(f"\nReusing adjusted dataset and dataloader (seq_len={seq_len}, pred_len={pred_len})...")
print(f"Number of samples in adjusted training dataset: {len(train_dataset_adj)}")
print(f"Number of batches in adjusted training DataLoader: {len(train_dataloader_adj)}")

# --- Implement Training Loop (Retry 2) ---
num_epochs = 3 # Keep epochs low for initial testing

print(f"\nStarting training retry 2 for {num_epochs} epochs with explicit lags...")

model.train() # Set the model to training mode

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader_adj):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device)

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        try:
            # Forward pass
            outputs = model(
                past_values=past_values,
                past_time_features=past_time_features,
                future_time_features=future_time_features,
                past_observed_mask=past_observed_mask,
                future_values=future_values # Provided as labels for loss calculation
            )

            # The output of the forward pass includes the loss when future_values is provided
            loss = outputs.loss

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Print loss every few batches
            if (i + 1) % 50 == 0:
                print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader_adj)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
            print("Batch shapes:")
            print(f"  past_values: {past_values.shape}")
            print(f"  past_time_features: {past_time_features.shape}")
            print(f"  future_time_features: {future_time_features.shape}")
            print(f"  future_values: {future_values.shape}")
            print(f"  past_observed_mask: {past_observed_mask.shape}")
            # Re-raise the exception to stop training
            raise

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader_adj)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished (Retry 2).")

"""## Implement training loop - retry 2

### Subtask:
Implement training loop - Retry 2: Implement a custom training loop using PyTorch to train the Hugging Face Autoformer model on your prepared data. This is a retry to address the persistent "lags cannot go further than history length" error by trying a minimal configuration and potentially debugging the data loading or model inputs.

**Reasoning**:
Implement a custom training loop using PyTorch to train the Hugging Face Autoformer model with a minimal configuration to address the persistent "lags cannot go further than history length" error.
"""

# --- Error Analysis and Configuration Adjustment (Attempt 4 - Minimal Config) ---
# The error "lags cannot go further than history length" persists.
# Based on the error message "found lag X while history length is only Y",
# where Y appears to be context_length + prediction_length,
# it seems there's an issue with how the model calculates or uses lags.
# Let's try the absolute minimal configuration to see if the error disappears or changes.

seq_len = 2 # Absolute minimal context length
pred_len = 1 # Absolute minimal prediction length

# With seq_len=2, pred_len=1, history_length = 2 + 1 = 3
# Default lags for 'd' are [1, 2, 3, 4, 5, 6, 7]
# We need lags <= history_length (3)
# Let's try explicitly setting lags to [1, 2, 3]

explicit_lags_sequence = [1, 2, 3]

# Recalculate conservative moving_average (must be < seq_len - 1)
# seq_len - 1 = 2 - 1 = 1
# So moving_avg must be < 1. This is impossible for a moving average window.
# This suggests moving_avg might be a conflicting parameter in minimal configs.
# Let's set moving_avg to the smallest possible meaningful value, 1, and see.
# If that fails, we might need to disable moving_avg if the config allows.
# Looking at the config definition, moving_avg has a default, let's try setting it to 1.
conservative_moving_avg = 1 # Must be < seq_len - 1 is impossible, setting to 1

print(f"Trying absolute MINIMAL config:")
print(f"  seq_len: {seq_len}, pred_len: {pred_len}")
print(f"  Calculated history_length (context_length + prediction_length): {seq_len + pred_len}")
print(f"  Using explicit_lags_sequence: {explicit_lags_sequence}")
print(f"  Moving average: {conservative_moving_avg} (Note: < seq_len-1 rule may conflict)")

num_variates = hf_data['num_variates'] # Should be 667

config = AutoformerConfig(
    prediction_length=pred_len,
    context_length=seq_len,
    input_size=num_variates,
    num_outputs=num_variates, # Predict all variates
    frequency="d",
    d_model=16, # Minimal model dimensions
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=2,
    decoder_attention_heads=2,
    moving_avg=conservative_moving_avg, # Set minimal moving average
    use_past_context=True,
    lags_sequence=explicit_lags_sequence, # Explicitly set the lags sequence
)

print("\n‚úÖ AutoformerConfig re-defined with MINIMAL config and explicit_lags_sequence:")
print(config)

# --- Instantiate Model and Setup ---
# Instantiate the model using the new config
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated with new MINIMAL config.")

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.cuda_is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# Re-define the optimizer (using AdamW) for the new model instance
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) re-defined for the new model instance")

# Define the loss function (using Mean Squared Error)
criterion = nn.MSELoss()
print("Loss function (MSELoss) defined")


# --- Data Preparation (using the adjusted data tensors and dataloader) ---
# We need to regenerate the data tensors and dataloader with the new minimal seq_len and pred_len.

def create_hf_dataset_format_minimal(dataframe, time_features_array, seq_len, pred_len, is_training=True):
    past_values_list = []
    past_time_features_list = []
    future_time_features_list = []
    future_values_list = []

    total_length = len(dataframe)
    # Adjust loop range for the new sequence lengths
    # Loop up to the point where a full (seq_len + pred_len) window can be formed
    for i in range(total_length - seq_len - pred_len + 1):
        past_values = dataframe.iloc[i : i + seq_len].values
        past_time_features = time_features_array[i : i + seq_len]

        future_time_features = time_features_array[i + seq_len : i + seq_len + pred_len]

        past_values_list.append(past_values)
        past_time_features_list.append(past_time_features)
        future_time_features_list.append(future_time_features)

        if is_training:
            future_values = dataframe.iloc[i + seq_len : i + seq_len + pred_len].values
            future_values_list.append(future_values)


    # Ensure numpy arrays are created only if lists are not empty
    if not past_values_list:
         return np.array([]), np.array([]), np.array([]), np.array([]) if is_training else np.array([])

    past_values_np = np.array(past_values_list)
    past_time_features_np = np.array(past_time_features_list)
    future_time_features_np = np.array(future_time_features_list)
    future_values_np = np.array(future_values_list) if is_training and future_values_list else None # Handle empty case

    return past_values_np, past_time_features_np, future_time_features_np, future_values_np


# Re-create tensors with minimal seq_len and pred_len
train_past_values_minimal, train_past_time_features_minimal, train_future_time_features_minimal, train_future_values_minimal = create_hf_dataset_format_minimal(
    train_df, train_time_features, seq_len, pred_len, is_training=True
)

# Re-create Dataset and DataLoader
class TimeSeriesDatasetMinimal(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = torch.tensor(past_values, dtype=torch.float32)
        self.past_time_features = torch.tensor(past_time_features, dtype=torch.float32)
        self.future_time_features = torch.tensor(future_time_features, dtype=torch.float32)
        self.future_values = torch.tensor(future_values, dtype=torch.float32) if future_values is not None else None

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx] if self.future_values is not None else None,
        )

# Check if minimal dataset can be created
if train_past_values_minimal.shape[0] == 0:
    raise ValueError(f"Minimal training dataset is empty with seq_len={seq_len}, pred_len={pred_len}. Data length is {len(train_df)}. Need data length >= seq_len + pred_len.")

train_dataset_minimal = TimeSeriesDatasetMinimal(
    train_past_values_minimal,
    train_past_time_features_minimal,
    train_future_time_features_minimal,
    train_future_values_minimal,
)

batch_size = 2 # Absolute minimal batch size
train_dataloader_minimal = DataLoader(train_dataset_minimal, batch_size=batch_size, shuffle=True)

print(f"\nCreated Minimal TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of samples in minimal training dataset: {len(train_dataset_minimal)}")
print(f"Number of batches in minimal training DataLoader: {len(train_dataloader_minimal)}")

# --- Implement Training Loop (Retry 3 - Minimal Config) ---
num_epochs = 2 # Absolute minimal epochs

print(f"\nStarting training retry 3 for {num_epochs} epochs with MINIMAL config...")

model.train() # Set the model to training mode

for epoch in range(num_epochs):
    total_loss = 0

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader_minimal):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device) # future_values should not be None here

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        try:
            # Forward pass
            outputs = model(
                past_values=past_values,
                past_time_features=past_time_features,
                future_time_features=future_time_features,
                past_observed_mask=past_observed_mask,
                future_values=future_values # Provided as labels for loss calculation
            )

            # The output of the forward pass includes the loss when future_values is provided
            loss = outputs.loss

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Print loss every few batches
            if (i + 1) % 10 == 0: # Print more frequently for minimal data
                print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader_minimal)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
            print("Batch shapes:")
            print(f"  past_values: {past_values.shape}")
            print(f"  past_time_features: {past_time_features.shape}")
            print(f"  future_time_features: {future_time_features.shape}")
            print(f"  future_values: {future_values.shape}")
            print(f"  past_observed_mask: {past_observed_mask.shape}")
            # Re-raise the exception to stop training
            raise

    # Print average epoch loss
    avg_epoch_loss = total_loss / len(train_dataloader_minimal)
    print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

print("\nTraining finished (Retry 3 - Minimal Config).")

# Check if training completed without error
print("\nChecking training completion status...")
if 'e' not in locals() and 'e' not in globals():
     print("‚úÖ Training completed successfully with the minimal configuration!")
else:
     print("‚ùå Training failed with an error.")
     print(f"Error: {e}")
     print(f"Minimal configuration used: seq_len={seq_len}, pred_len={pred_len}, lags_sequence={explicit_lags_sequence}, moving_avg={conservative_moving_avg}")

"""**Reasoning**:
Fix the AttributeError by using the correct method `torch.cuda.is_available()` to check for CUDA availability and then continue with the rest of the custom training loop implementation using the minimal configuration and data preparation.


"""

# --- Fix the CUDA check ---
# The previous error was AttributeError: module 'torch.cuda' has no attribute 'cuda_is_available'
# The correct method is torch.cuda.is_available()

# Ensure model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model moved to device: {device}")

# --- Continue with Setup (already done, but printing to confirm variables exist) ---
# Re-define the optimizer (using AdamW) for the new model instance
# optimizer = optim.AdamW(model.parameters(), lr=0.001) # Already defined
print("Optimizer (AdamW) is defined.")

# Define the loss function (using Mean Squared Error) - already defined
# criterion = nn.MSELoss() # Already defined
print("Loss function (MSELoss) is defined.")

# --- Data Preparation (using the adjusted data tensors and dataloader) ---
# The data tensors and dataloader for the minimal config were prepared in the previous step
# (train_past_values_minimal, train_past_time_features_minimal, train_future_time_features_minimal, train_future_values_minimal)
# train_dataset_minimal, train_dataloader_minimal

print(f"\nReusing minimal dataset and dataloader (seq_len={seq_len}, pred_len={pred_len})...")
print(f"Number of samples in minimal training dataset: {len(train_dataset_minimal)}")
print(f"Number of batches in minimal training DataLoader: {len(train_dataloader_minimal)}")

# --- Implement Training Loop (Retry 3 - Minimal Config) ---
num_epochs = 2 # Absolute minimal epochs

print(f"\nStarting training retry 3 for {num_epochs} epochs with MINIMAL config...")

model.train() # Set the model to training mode

# Initialize a flag to track if training failed
training_failed = False

for epoch in range(num_epochs):
    total_loss = 0
    print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader_minimal):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device) # future_values should not be None here

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        try:
            # Forward pass
            outputs = model(
                past_values=past_values,
                past_time_features=past_time_features,
                future_time_features=future_time_features,
                past_observed_mask=past_observed_mask,
                future_values=future_values # Provided as labels for loss calculation
            )

            # The output of the forward pass includes the loss when future_values is provided
            loss = outputs.loss

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Print loss every few batches
            if (i + 1) % 10 == 0: # Print more frequently for minimal data
                print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader_minimal)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
            print("Batch shapes:")
            print(f"  past_values: {past_values.shape}")
            print(f"  past_time_features: {past_time_features.shape}")
            print(f"  future_time_features: {future_time_features.shape}")
            print(f"  future_values: {future_values.shape}")
            print(f"  past_observed_mask: {past_observed_mask.shape}")
            # Set the flag and break the inner loop
            training_failed = True
            break # Exit the batch loop

    # Print average epoch loss if the epoch completed without error
    if not training_failed:
        avg_epoch_loss = total_loss / len(train_dataloader_minimal)
        print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

    # If training failed in this epoch, break the outer loop as well
    if training_failed:
        break # Exit the epoch loop


print("\nTraining finished.")

# Check if training completed without error
print("\nChecking training completion status...")
if not training_failed:
     print("‚úÖ Training completed successfully with the minimal configuration!")
else:
     print("‚ùå Training failed with an error.")
     print(f"Error: {e}")
     print(f"Minimal configuration used: seq_len={seq_len}, pred_len={pred_len}, lags_sequence={explicit_lags_sequence}, moving_avg={conservative_moving_avg}")

"""**Reasoning**:
The previous command failed because the `train_dataset_minimal` and `train_dataloader_minimal` variables were not defined in the current execution context, even though the code block intended to reuse them from a previous run. I need to regenerate the minimal dataset and dataloader within this code block before attempting to use them in the training loop.


"""

# --- Fix: Regenerate minimal dataset and dataloader ---
# The previous error indicated that train_dataset_minimal and train_dataloader_minimal were not defined.
# I need to regenerate them using the minimal seq_len and pred_len defined in the previous steps.

# Re-create tensors with minimal seq_len and pred_len
# Assuming train_df and train_time_features are available from previous data prep steps
train_past_values_minimal, train_past_time_features_minimal, train_future_time_features_minimal, train_future_values_minimal = create_hf_dataset_format_minimal(
    train_df, train_time_features, seq_len, pred_len, is_training=True
)

# Re-create Dataset and DataLoader
class TimeSeriesDatasetMinimal(Dataset):
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = torch.tensor(past_values, dtype=torch.float32)
        self.past_time_features = torch.tensor(past_time_features, dtype=torch.float32)
        self.future_time_features = torch.tensor(future_time_features, dtype=torch.float32)
        self.future_values = torch.tensor(future_values, dtype=torch.float32) if future_values is not None else None

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx] if self.future_values is not None else None,
        )

# Check if minimal dataset can be created
if train_past_values_minimal.shape[0] == 0:
    raise ValueError(f"Minimal training dataset is empty with seq_len={seq_len}, pred_len={pred_len}. Data length is {len(train_df)}. Need data length >= seq_len + pred_len.")

train_dataset_minimal = TimeSeriesDatasetMinimal(
    train_past_values_minimal,
    train_past_time_features_minimal,
    train_future_time_features_minimal,
    train_future_values_minimal,
)

batch_size = 2 # Absolute minimal batch size (from previous step)
train_dataloader_minimal = DataLoader(train_dataset_minimal, batch_size=batch_size, shuffle=True)

print(f"\nRegenerated Minimal TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of samples in minimal training dataset: {len(train_dataset_minimal)}")
print(f"Number of batches in minimal training DataLoader: {len(train_dataloader_minimal)}")


# --- Continue with Training Loop (Retry 3 - Minimal Config) ---
# The model, optimizer, and criterion are assumed to be defined from the previous successful steps.
# The device is also set correctly now.

num_epochs = 2 # Absolute minimal epochs (from previous step)

print(f"\nStarting training retry 3 for {num_epochs} epochs with MINIMAL config...")

model.train() # Set the model to training mode

# Initialize a flag to track if training failed
training_failed = False

for epoch in range(num_epochs):
    total_loss = 0
    print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader_minimal):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device) # future_values should not be None here

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        try:
            # Forward pass
            outputs = model(
                past_values=past_values,
                past_time_features=past_time_features,
                future_time_features=future_time_features,
                past_observed_mask=past_observed_mask,
                future_values=future_values # Provided as labels for loss calculation
            )

            # The output of the forward pass includes the loss when future_values is provided
            loss = outputs.loss

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Print loss every few batches
            if (i + 1) % 10 == 0: # Print more frequently for minimal data
                print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader_minimal)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
            print("Batch shapes:")
            print(f"  past_values: {past_values.shape}")
            print(f"  past_time_features: {past_time_features.shape}")
            print(f"  future_time_features: {future_time_features.shape}")
            print(f"  future_values: {future_values.shape}")
            print(f"  past_observed_mask: {past_observed_mask.shape}")
            # Set the flag and break the inner loop
            training_failed = True
            break # Exit the batch loop

    # Print average epoch loss if the epoch completed without error
    if not training_failed:
        avg_epoch_loss = total_loss / len(train_dataloader_minimal)
        print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

    # If training failed in this epoch, break the outer loop as well
    if training_failed:
        break # Exit the epoch loop


print("\nTraining finished.")

# Check if training completed without error
print("\nChecking training completion status...")
if not training_failed:
     print("‚úÖ Training completed successfully with the minimal configuration!")
else:
     print("‚ùå Training failed with an error.")
     print(f"Error: {e}")
     print(f"Minimal configuration used: seq_len={seq_len}, pred_len={pred_len}, lags_sequence={explicit_lags_sequence}, moving_avg={conservative_moving_avg}")

"""**Reasoning**:
The previous command failed because the helper function `create_hf_dataset_format_minimal` was not defined in the current execution context. I need to include the definition of this function within the code block before calling it to regenerate the minimal dataset and dataloader. I also need to include the definition of `TimeSeriesDatasetMinimal`. Then I can proceed with the training loop as intended.


"""

# --- Fix: Include function and class definitions ---
# The previous error indicated that create_hf_dataset_format_minimal and TimeSeriesDatasetMinimal were not defined.
# I need to include their definitions within this code block.

def create_hf_dataset_format_minimal(dataframe, time_features_array, seq_len, pred_len, is_training=True):
    """
    Helper function to create data tensors in Hugging Face format with minimal lengths.
    """
    past_values_list = []
    past_time_features_list = []
    future_time_features_list = []
    future_values_list = []

    total_length = len(dataframe)
    # Adjust loop range for the new sequence lengths
    # Loop up to the point where a full (seq_len + pred_len) window can be formed
    # Ensure total_length is sufficient for at least one window
    if total_length < seq_len + pred_len:
        print(f"Warning: Data length ({total_length}) is less than seq_len ({seq_len}) + pred_len ({pred_len}). Cannot create any samples.")
        return np.array([]), np.array([]), np.array([]), np.array([]) if is_training else np.array([])


    for i in range(total_length - seq_len - pred_len + 1):
        past_values = dataframe.iloc[i : i + seq_len].values
        past_time_features = time_features_array[i : i + seq_len]

        future_time_features = time_features_array[i + seq_len : i + seq_len + pred_len]

        past_values_list.append(past_values)
        past_time_features_list.append(past_time_features)
        future_time_features_list.append(future_time_features)

        if is_training:
            future_values = dataframe.iloc[i + seq_len : i + seq_len + pred_len].values
            future_values_list.append(future_values)


    # Ensure numpy arrays are created only if lists are not empty
    if not past_values_list:
         return np.array([]), np.array([]), np.array([]), np.array([]) if is_training else np.array([])

    past_values_np = np.array(past_values_list)
    past_time_features_np = np.array(past_time_features_list)
    future_time_features_np = np.array(future_time_features_list)
    future_values_np = np.array(future_values_list) if is_training and future_values_list else None # Handle empty case


    return past_values_np, past_time_features_np, future_time_features_np, future_values_np


class TimeSeriesDatasetMinimal(Dataset):
    """
    Minimal PyTorch Dataset for time series data.
    """
    def __init__(self, past_values, past_time_features, future_time_features, future_values):
        self.past_values = torch.tensor(past_values, dtype=torch.float32)
        self.past_time_features = torch.tensor(past_time_features, dtype=torch.float32)
        self.future_time_features = torch.tensor(future_time_features, dtype=torch.float32)
        self.future_values = torch.tensor(future_values, dtype=torch.float32) if future_values is not None else None

    def __len__(self):
        return self.past_values.shape[0]

    def __getitem__(self, idx):
        return (
            self.past_values[idx],
            self.past_time_features[idx],
            self.future_time_features[idx],
            self.future_values[idx] if self.future_values is not None else None,
        )


# --- Regenerate minimal dataset and dataloader ---
# Use the minimal seq_len and pred_len defined in previous steps (seq_len=2, pred_len=1)

# Re-create tensors with minimal seq_len and pred_len
# Assuming train_df and train_time_features are available from previous data prep steps
# (They should be available in the kernel state)
train_past_values_minimal, train_past_time_features_minimal, train_future_time_features_minimal, train_future_values_minimal = create_hf_dataset_format_minimal(
    train_df, train_time_features, seq_len, pred_len, is_training=True
)

# Check if minimal dataset can be created
if train_past_values_minimal.shape[0] == 0:
    # This check is duplicated from the function, but kept for clarity/safety
    raise ValueError(f"Minimal training dataset is empty with seq_len={seq_len}, pred_len={pred_len}. Data length is {len(train_df)}. Need data length >= seq_len + pred_len.")

train_dataset_minimal = TimeSeriesDatasetMinimal(
    train_past_values_minimal,
    train_past_time_features_minimal,
    train_future_time_features_minimal,
    train_future_values_minimal,
)

batch_size = 2 # Absolute minimal batch size (from previous step)
train_dataloader_minimal = DataLoader(train_dataset_minimal, batch_size=batch_size, shuffle=True)

print(f"\nRegenerated Minimal TimeSeriesDataset and DataLoader with batch size: {batch_size}")
print(f"Number of samples in minimal training dataset: {len(train_dataset_minimal)}")
print(f"Number of batches in minimal training DataLoader: {len(train_dataloader_minimal)}")


# --- Continue with Training Loop (Retry 3 - Minimal Config) ---
# The model, optimizer, and criterion are assumed to be defined from previous successful steps.
# The device is also set correctly now.

num_epochs = 2 # Absolute minimal epochs (from previous step)

print(f"\nStarting training retry 3 for {num_epochs} epochs with MINIMAL config...")

model.train() # Set the model to training mode

# Initialize a flag to track if training failed
training_failed = False
caught_exception = None # Variable to store the caught exception

for epoch in range(num_epochs):
    total_loss = 0
    print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader_minimal):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device) # future_values should not be None here

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        try:
            # Forward pass
            outputs = model(
                past_values=past_values,
                past_time_features=past_time_features,
                future_time_features=future_time_features,
                past_observed_mask=past_observed_mask,
                future_values=future_values # Provided as labels for loss calculation
            )

            # The output of the forward pass includes the loss when future_values is provided
            loss = outputs.loss

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Print loss every few batches
            if (i + 1) % 10 == 0: # Print more frequently for minimal data
                print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader_minimal)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
            print("Batch shapes:")
            print(f"  past_values: {past_values.shape}")
            print(f"  past_time_features: {past_time_features.shape}")
            print(f"  future_time_features: {future_time_features.shape}")
            print(f"  future_values: {future_values.shape}")
            print(f"  past_observed_mask: {past_observed_mask.shape}")
            # Store the exception and set the flag
            caught_exception = e
            training_failed = True
            break # Exit the batch loop

    # Print average epoch loss if the epoch completed without error
    if not training_failed:
        avg_epoch_loss = total_loss / len(train_dataloader_minimal)
        print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

    # If training failed in this epoch, break the outer loop as well
    if training_failed:
        break # Exit the epoch loop


print("\nTraining finished.")

# Check if training completed without error
print("\nChecking training completion status...")
if not training_failed:
     print("‚úÖ Training completed successfully with the minimal configuration!")
     # If successful, finish the subtask
     finish_status = "success"
else:
     print("‚ùå Training failed with an error.")
     print(f"Error: {caught_exception}")
     print(f"Minimal configuration used: seq_len={seq_len}, pred_len={pred_len}, lags_sequence={explicit_lags_sequence}, moving_avg={conservative_moving_avg}")
     # If failed, finish the subtask
     finish_status = "failure"

"""**Reasoning**:
The training failed again with the "lags cannot go further than history length" error, specifically reporting "found lag 3 while history length is only 3". This confirms the suspicion that the model's internal check might be `lag < history_length` instead of `lag <= history_length`. To attempt to work around this, I will try the minimal configuration again, but this time I will explicitly set the `lags_sequence` to exclude the maximum lag (3 in this case), setting it to `[1, 2]`. This is a final attempt to train with a minimal configuration before concluding that the Hugging Face Autoformer implementation has an unresolvable issue with minimal sequence lengths or lags based on the current error pattern.


"""

# --- Error Analysis and Configuration Adjustment (Attempt 5 - Minimal Config, Adjusted Lags) ---
# The error persists: "lags cannot go further than history length, found lag 3 while history length is only 3".
# History length = context_length (2) + prediction_length (1) = 3.
# The error occurs when a lag value is equal to the history length.
# To work around this, I will explicitly set lags_sequence to exclude lag 3.

seq_len = 2 # Absolute minimal context length (from previous step)
pred_len = 1 # Absolute minimal prediction length (from previous step)

# History length = 3. Exclude lag 3 from the lags sequence.
explicit_lags_sequence = [1, 2] # Exclude lag 3

# Moving average remains 1 (from previous step)
conservative_moving_avg = 1

print(f"Trying absolute MINIMAL config with ADJUSTED lags:")
print(f"  seq_len: {seq_len}, pred_len: {pred_len}")
print(f"  Calculated history_length (context_length + prediction_length): {seq_len + pred_len}")
print(f"  Using explicit_lags_sequence: {explicit_lags_sequence}")
print(f"  Moving average: {conservative_moving_avg}")

num_variates = hf_data['num_variates'] # Should be 667

config = AutoformerConfig(
    prediction_length=pred_len,
    context_length=seq_len,
    input_size=num_variates,
    num_outputs=num_variates, # Predict all variates
    frequency="d",
    d_model=16, # Minimal model dimensions
    encoder_layers=1,
    decoder_layers=1,
    encoder_attention_heads=2,
    decoder_attention_heads=2,
    moving_avg=conservative_moving_avg, # Set minimal moving average
    use_past_context=True,
    lags_sequence=explicit_lags_sequence, # Explicitly set the adjusted lags sequence
)

print("\n‚úÖ AutoformerConfig re-defined with MINIMAL config and ADJUSTED explicit_lags_sequence:")
print(config)

# --- Instantiate Model and Setup ---
# Instantiate the model using the new config
model = AutoformerForPrediction(config)

print("\n‚úÖ AutoformerForPrediction model instantiated with new MINIMAL config.")

# Ensure model is on the correct device (device variable should be defined from previous steps)
model.to(device)
print(f"Model moved to device: {device}")

# Re-define the optimizer (using AdamW) for the new model instance
optimizer = optim.AdamW(model.parameters(), lr=0.001)
print("Optimizer (AdamW) re-defined for the new model instance")

# Define the loss function (using Mean Squared Error) (criterion variable should be defined)
# criterion = nn.MSELoss()
print("Loss function (MSELoss) is defined.")


# --- Data Preparation (Reuse minimal dataset and dataloader) ---
# The minimal dataset and dataloader were successfully regenerated in the previous step.
# train_dataset_minimal, train_dataloader_minimal

print(f"\nReusing minimal dataset and dataloader (seq_len={seq_len}, pred_len={pred_len})...")
print(f"Number of samples in minimal training dataset: {len(train_dataset_minimal)}")
print(f"Number of batches in minimal training DataLoader: {len(train_dataloader_minimal)}")

# --- Implement Training Loop (Retry 4 - Minimal Config, Adjusted Lags) ---
num_epochs = 2 # Absolute minimal epochs (from previous step)

print(f"\nStarting training retry 4 for {num_epochs} epochs with MINIMAL config and ADJUSTED lags...")

model.train() # Set the model to training mode

# Initialize a flag to track if training failed
training_failed = False
caught_exception = None # Variable to store the caught exception

for epoch in range(num_epochs):
    total_loss = 0
    print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader_minimal):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device) # future_values should not be None here

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        try:
            # Forward pass
            outputs = model(
                past_values=past_values,
                past_time_features=past_time_time_features, # Corrected typo
                future_time_features=future_time_features,
                past_observed_mask=past_observed_mask,
                future_values=future_values # Provided as labels for loss calculation
            )

            # The output of the forward pass includes the loss when future_values is provided
            loss = outputs.loss

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Print loss every few batches
            if (i + 1) % 10 == 0: # Print more frequently for minimal data
                print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader_minimal)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
            print("Batch shapes:")
            print(f"  past_values: {past_values.shape}")
            print(f"  past_time_features: {past_time_features.shape}")
            print(f"  future_time_features: {future_time_features.shape}")
            print(f"  future_values: {future_values.shape}")
            print(f"  past_observed_mask: {past_observed_mask.shape}")
            # Store the exception and set the flag
            caught_exception = e
            training_failed = True
            break # Exit the batch loop

    # Print average epoch loss if the epoch completed without error
    if not training_failed:
        avg_epoch_loss = total_loss / len(train_dataloader_minimal)
        print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

    # If training failed in this epoch, break the outer loop as well
    if training_failed:
        break # Exit the epoch loop


print("\nTraining finished.")

# Check if training completed without error
print("\nChecking training completion status...")
if not training_failed:
     print("‚úÖ Training completed successfully with the minimal configuration!")
     # If successful, finish the subtask
     finish_status = "success"
else:
     print("‚ùå Training failed with an error.")
     print(f"Error: {caught_exception}")
     print(f"Minimal configuration used: seq_len={seq_len}, pred_len={pred_len}, lags_sequence={explicit_lags_sequence}, moving_avg={conservative_moving_avg}")
     # If failed, finish the subtask
     finish_status = "failure"

"""**Reasoning**:
The training loop failed due to a simple typo (`past_time_time_features` instead of `past_time_features`) in the model's forward pass call within the `try` block. I need to correct this typo and retry the training loop with the same minimal configuration and adjusted lags.


"""

# --- Fix: Correct the typo in the forward pass ---
# The previous error was NameError: name 'past_time_time_features' is not defined
# This was a typo in the call to model(...)

print("Correcting typo in model forward pass...")

# --- Implement Training Loop (Retry 5 - Minimal Config, Corrected Typo) ---
# Use the minimal config (seq_len=2, pred_len=1, lags_sequence=[1, 2], moving_avg=1)
# Use the minimal dataset and dataloader (train_dataset_minimal, train_dataloader_minimal)
# Use the model, optimizer, criterion, and device from previous steps.

num_epochs = 2 # Absolute minimal epochs (from previous step)

print(f"\nStarting training retry 5 for {num_epochs} epochs with MINIMAL config and CORRECTED typo...")

model.train() # Set the model to training mode

# Initialize a flag to track if training failed
training_failed = False
caught_exception = None # Variable to store the caught exception

for epoch in range(num_epochs):
    total_loss = 0
    print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")

    for i, (past_values, past_time_features, future_time_features, future_values) in enumerate(train_dataloader_minimal):
        # Move batch to device
        past_values = past_values.to(device)
        past_time_features = past_time_features.to(device)
        future_time_features = future_time_features.to(device)
        future_values = future_values.to(device) # future_values should not be None here

        # Create past_observed_mask: assuming all values in past_values are observed (not missing)
        past_observed_mask = torch.ones_like(past_values, dtype=torch.bool).to(device)

        # Zero the gradients
        optimizer.zero_grad()

        try:
            # Forward pass - CORRECTED TYPO HERE
            outputs = model(
                past_values=past_values,
                past_time_features=past_time_features, # Corrected from past_time_time_features
                future_time_features=future_time_features,
                past_observed_mask=past_observed_mask,
                future_values=future_values # Provided as labels for loss calculation
            )

            # The output of the forward pass includes the loss when future_values is provided
            loss = outputs.loss

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Print loss every few batches
            if (i + 1) % 10 == 0: # Print more frequently for minimal data
                print(f"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader_minimal)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"\n‚ùå Error during training loop at Epoch {epoch+1}, Step {i+1}: {e}")
            print("Batch shapes:")
            print(f"  past_values: {past_values.shape}")
            print(f"  past_time_features: {past_time_features.shape}")
            print(f"  future_time_features: {future_time_features.shape}")
            print(f"  future_values: {future_values.shape}")
            print(f"  past_observed_mask: {past_observed_mask.shape}")
            # Store the exception and set the flag
            caught_exception = e
            training_failed = True
            break # Exit the batch loop

    # Print average epoch loss if the epoch completed without error
    if not training_failed:
        avg_epoch_loss = total_loss / len(train_dataloader_minimal)
        print(f"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_epoch_loss:.4f}")

    # If training failed in this epoch, break the outer loop as well
    if training_failed:
        break # Exit the epoch loop


print("\nTraining finished.")

# Check if training completed without error
print("\nChecking training completion status...")
if not training_failed:
     print("‚úÖ Training completed successfully with the minimal configuration!")
     # If successful, finish the subtask
     finish_status = "success"
else:
     print("‚ùå Training failed with an error.")
     print(f"Error: {caught_exception}")
     print(f"Minimal configuration used: seq_len={seq_len}, pred_len={pred_len}, lags_sequence={explicit_lags_sequence}, moving_avg={conservative_moving_avg}")
     # If failed, finish the subtask
     finish_status = "failure"

"""## Summary:

### Data Analysis Key Findings

*   The Hugging Face `transformers` and `datasets` libraries were successfully installed (or confirmed to be already installed).
*   The data loading and preparation process was adapted to the Hugging Face Autoformer format, including creating time features and splitting the data into training and testing sets using a sliding window approach. The `create_time_features` function was fixed to correctly extract the week of the year using `isocalendar().week`.
*   The data was successfully converted to PyTorch tensors with the expected shapes for training.
*   Attempts to train the `AutoformerForPrediction` model from Hugging Face failed due to a persistent `ValueError: lags cannot go further than history length`.
*   This error occurred even with minimal configurations (`context_length=2`, `prediction_length=1`) and explicitly setting the `lags_sequence` to values less than the calculated history length. The error message itself was inconsistent, sometimes reporting a lag value equal to or less than the history length as being problematic.

### Insights or Next Steps

*   The persistent "lags cannot go further than history length" error suggests a potential bug or incompatibility within the Hugging Face `AutoformerForPrediction` implementation, particularly with minimal sequence lengths. Further investigation into the model's internal lag calculation and handling or consulting Hugging Face documentation/issues might be necessary.
*   Consider exploring alternative time series forecasting models available in Hugging Face or other libraries if the Autoformer lag issue cannot be resolved.

**Reasoning**:
The previous training attempt failed due to a `ValueError: lags cannot go further than history length`. This indicates a mismatch between the configured `context_length` (seq_len), `prediction_length`, and the `lags_sequence` used by the Hugging Face Autoformer model. I need to understand how "history length" is calculated internally and ensure that the specified lags are within the allowed range. I will review the model configuration and error message to identify the problematic parameter(s) and modify the training cell (`c5620810`) to adjust them.
"""