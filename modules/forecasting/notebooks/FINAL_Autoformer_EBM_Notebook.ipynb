{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7EbFfOlPFYP"
      },
      "source": [
        "MOUNTING STORAGE DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW00_Ckq5vQO",
        "outputId": "a0cbcf5b-bcae-436b-87df-03f5d4bb5afe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF4t8ZOpRn-b"
      },
      "source": [
        "AUTOFORMER GIT REPO CLONING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2548e9c2",
        "outputId": "cccd0d67-3aeb-4c49-c88b-0eb00cbe3fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Cloning into 'Autoformer'...\n",
            "remote: Enumerating objects: 376, done.\u001b[K\n",
            "remote: Counting objects: 100% (212/212), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 376 (delta 157), reused 143 (delta 143), pack-reused 164 (from 2)\u001b[K\n",
            "Receiving objects: 100% (376/376), 2.20 MiB | 31.78 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n",
            "/content/Autoformer/Autoformer/Autoformer/Autoformer/Autoformer\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: reformer_pytorch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 3)) (2.8.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: axial-positional-embedding>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (0.3.12)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (0.8.1)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.12/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory in /usr/local/lib/python3.12/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (0.2.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from local-attention->reformer_pytorch->-r requirements.txt (line 6)) (0.2.1)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.12/dist-packages (from product-key-memory->reformer_pytorch->-r requirements.txt (line 6)) (0.11.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision->-r requirements.txt (line 3)) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install dependencies for Autoformer (assuming requirements.txt in repo or manual install)\n",
        "!pip install pandas numpy matplotlib scikit-learn\n",
        "!git clone https://github.com/thuml/Autoformer.git\n",
        "%cd Autoformer\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Kfze02RyLZ"
      },
      "source": [
        "FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "807764bc",
        "outputId": "c67fd468-7356-43c8-b2b1-7e590ef1958e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded /content/drive/MyDrive/final_concatenated_data_mice_imputed.csv: 618 rows\n",
            "Tag column dropped\n",
            "Date range of combined data: 2024-06-01 00:00:00 to 2025-07-31 00:00:00\n",
            "\n",
            "Filtered data (using full MICE dataset): 618 rows\n",
            "Filtered date range: 2024-06-01 00:00:00 to 2025-07-31 00:00:00\n",
            "\n",
            "🎯 Target columns identified: ['Total_Stabilized_Naphtha_Product_Flowrate', 'Total_Kerosene_Product_Flowrate', 'Jet_Fuel_Product_Train1_Flowrate', 'Total_Light_Diesel_Product_Flowrate', 'Total_Heavy_Diesel_Product_Flowrate', 'Total_Atmospheric_Residue_Flowrate', 'Blend_Yield_Gas & LPG', 'Blend_Yield_Kerosene', 'Blend_Yield_Light Diesel', 'Blend_Yield_Heavy Diesel', 'Blend_Yield_RCO']\n",
            "📈 Feature columns identified: ['Light_Diesel_to_DHT_Unit_Flowrate', 'Kerosene_to_Light_Diesel_DHT_Flowrate', 'Atm_Residue_to_RFCC_Unit_Flowrate', 'Atm_Residue_to_Storage_Flowrate', 'Crude_Column_Naphtha_to_SGCU_Flowrate', 'Heavy_Diesel_to_MHC_Flowrate', 'Atm_Residue_to_Storage_EE1027_Flowrate', 'blend_id', 'API', 'Sulphur'] ... (656 total)\n",
            "\n",
            "Verifying data types:\n",
            "Date column dtype: datetime64[ns]\n",
            "⚠️  Non-numeric column found: Total_Light_Diesel_Product_Flowrate (dtype: object)\n",
            "⚠️  Non-numeric column found: Total_Heavy_Diesel_Product_Flowrate (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TI2805 (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4505A (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4505B (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4505D (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4505E (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4505F (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4506A (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4506C (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4506D (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4506E (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4506F (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4507B (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4507C (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4507D (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4507E (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4508A (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4508B (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4508C (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4508D (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4508F (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4509A (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4509B (dtype: object)\n",
            "⚠️  Non-numeric column found: 101TXI4509C (dtype: object)\n",
            "⚠️  Non-numeric column found: 101FQ1401 (dtype: object)\n",
            "⚠️  Non-numeric column found: 101FQ3001 (dtype: object)\n",
            "⚠️  Non-numeric column found: 101FQ3101 (dtype: object)\n",
            "⚠️  Non-numeric column found: 101FQ4201 (dtype: object)\n",
            "⚠️  Non-numeric column found: 101FQ5303 (dtype: object)\n",
            "⚠️  Non-numeric column found: 102FQ2801 (dtype: object)\n",
            "⚠️  Non-numeric column found: 102FQ4201 (dtype: object)\n",
            "⚠️  Non-numeric column found: 102FQ4401 (dtype: object)\n",
            "\n",
            "❌ Converting non-numeric columns to numeric...\n",
            "✅ Successfully converted Total_Light_Diesel_Product_Flowrate to numeric\n",
            "✅ Successfully converted Total_Heavy_Diesel_Product_Flowrate to numeric\n",
            "✅ Successfully converted 101TI2805 to numeric\n",
            "✅ Successfully converted 101TXI4505A to numeric\n",
            "✅ Successfully converted 101TXI4505B to numeric\n",
            "✅ Successfully converted 101TXI4505D to numeric\n",
            "✅ Successfully converted 101TXI4505E to numeric\n",
            "✅ Successfully converted 101TXI4505F to numeric\n",
            "✅ Successfully converted 101TXI4506A to numeric\n",
            "✅ Successfully converted 101TXI4506C to numeric\n",
            "✅ Successfully converted 101TXI4506D to numeric\n",
            "✅ Successfully converted 101TXI4506E to numeric\n",
            "✅ Successfully converted 101TXI4506F to numeric\n",
            "✅ Successfully converted 101TXI4507B to numeric\n",
            "✅ Successfully converted 101TXI4507C to numeric\n",
            "✅ Successfully converted 101TXI4507D to numeric\n",
            "✅ Successfully converted 101TXI4507E to numeric\n",
            "✅ Successfully converted 101TXI4508A to numeric\n",
            "✅ Successfully converted 101TXI4508B to numeric\n",
            "✅ Successfully converted 101TXI4508C to numeric\n",
            "✅ Successfully converted 101TXI4508D to numeric\n",
            "✅ Successfully converted 101TXI4508F to numeric\n",
            "✅ Successfully converted 101TXI4509A to numeric\n",
            "✅ Successfully converted 101TXI4509B to numeric\n",
            "✅ Successfully converted 101TXI4509C to numeric\n",
            "✅ Successfully converted 101FQ1401 to numeric\n",
            "✅ Successfully converted 101FQ3001 to numeric\n",
            "✅ Successfully converted 101FQ3101 to numeric\n",
            "✅ Successfully converted 101FQ4201 to numeric\n",
            "✅ Successfully converted 101FQ5303 to numeric\n",
            "✅ Successfully converted 102FQ2801 to numeric\n",
            "✅ Successfully converted 102FQ4201 to numeric\n",
            "✅ Successfully converted 102FQ4401 to numeric\n",
            "\n",
            "✅ All columns in training dataset are now numeric\n",
            "Training dataset shape (no date): (0, 667)\n",
            "Full dataset shape (with date): (0, 668)\n",
            "\n",
            "Summary statistics for target columns:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(f\\\"   - Use 'custom_with_date\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Total_Stabilized_Naphtha_Product_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Kerosene_Product_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Jet_Fuel_Product_Train1_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Light_Diesel_Product_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Heavy_Diesel_Product_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Atmospheric_Residue_Flowrate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_Gas & LPG\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_Kerosene\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_Light Diesel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_Heavy Diesel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blend_Yield_RCO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a05b9038-631a-4e6d-bf55-c5c37eed354d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total_Stabilized_Naphtha_Product_Flowrate</th>\n",
              "      <th>Total_Kerosene_Product_Flowrate</th>\n",
              "      <th>Jet_Fuel_Product_Train1_Flowrate</th>\n",
              "      <th>Total_Light_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Heavy_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Atmospheric_Residue_Flowrate</th>\n",
              "      <th>Blend_Yield_Gas &amp; LPG</th>\n",
              "      <th>Blend_Yield_Kerosene</th>\n",
              "      <th>Blend_Yield_Light Diesel</th>\n",
              "      <th>Blend_Yield_Heavy Diesel</th>\n",
              "      <th>Blend_Yield_RCO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a05b9038-631a-4e6d-bf55-c5c37eed354d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a05b9038-631a-4e6d-bf55-c5c37eed354d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a05b9038-631a-4e6d-bf55-c5c37eed354d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f705f57b-8f56-4815-9fa1-258c08959e6b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f705f57b-8f56-4815-9fa1-258c08959e6b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f705f57b-8f56-4815-9fa1-258c08959e6b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       Total_Stabilized_Naphtha_Product_Flowrate  \\\n",
              "count                                        0.0   \n",
              "mean                                         NaN   \n",
              "std                                          NaN   \n",
              "min                                          NaN   \n",
              "25%                                          NaN   \n",
              "50%                                          NaN   \n",
              "75%                                          NaN   \n",
              "max                                          NaN   \n",
              "\n",
              "       Total_Kerosene_Product_Flowrate  Jet_Fuel_Product_Train1_Flowrate  \\\n",
              "count                              0.0                               0.0   \n",
              "mean                               NaN                               NaN   \n",
              "std                                NaN                               NaN   \n",
              "min                                NaN                               NaN   \n",
              "25%                                NaN                               NaN   \n",
              "50%                                NaN                               NaN   \n",
              "75%                                NaN                               NaN   \n",
              "max                                NaN                               NaN   \n",
              "\n",
              "       Total_Light_Diesel_Product_Flowrate  \\\n",
              "count                                  0.0   \n",
              "mean                                   NaN   \n",
              "std                                    NaN   \n",
              "min                                    NaN   \n",
              "25%                                    NaN   \n",
              "50%                                    NaN   \n",
              "75%                                    NaN   \n",
              "max                                    NaN   \n",
              "\n",
              "       Total_Heavy_Diesel_Product_Flowrate  \\\n",
              "count                                  0.0   \n",
              "mean                                   NaN   \n",
              "std                                    NaN   \n",
              "min                                    NaN   \n",
              "25%                                    NaN   \n",
              "50%                                    NaN   \n",
              "75%                                    NaN   \n",
              "max                                    NaN   \n",
              "\n",
              "       Total_Atmospheric_Residue_Flowrate  Blend_Yield_Gas & LPG  \\\n",
              "count                                 0.0                    0.0   \n",
              "mean                                  NaN                    NaN   \n",
              "std                                   NaN                    NaN   \n",
              "min                                   NaN                    NaN   \n",
              "25%                                   NaN                    NaN   \n",
              "50%                                   NaN                    NaN   \n",
              "75%                                   NaN                    NaN   \n",
              "max                                   NaN                    NaN   \n",
              "\n",
              "       Blend_Yield_Kerosene  Blend_Yield_Light Diesel  \\\n",
              "count                   0.0                       0.0   \n",
              "mean                    NaN                       NaN   \n",
              "std                     NaN                       NaN   \n",
              "min                     NaN                       NaN   \n",
              "25%                     NaN                       NaN   \n",
              "50%                     NaN                       NaN   \n",
              "75%                     NaN                       NaN   \n",
              "max                     NaN                       NaN   \n",
              "\n",
              "       Blend_Yield_Heavy Diesel  Blend_Yield_RCO  \n",
              "count                       0.0              0.0  \n",
              "mean                        NaN              NaN  \n",
              "std                         NaN              NaN  \n",
              "min                         NaN              NaN  \n",
              "25%                         NaN              NaN  \n",
              "50%                         NaN              NaN  \n",
              "75%                         NaN              NaN  \n",
              "max                         NaN              NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Saved training dataset (NO DATE) to ./dataset/custom/custom.csv\n",
            "✅ Saved reference dataset (WITH DATE) to ./dataset/custom/custom_with_date.csv\n",
            "Final training dataset shape: (0, 667)\n",
            "Training dataset columns: ['Total_Stabilized_Naphtha_Product_Flowrate', 'Total_Kerosene_Product_Flowrate', 'Jet_Fuel_Product_Train1_Flowrate', 'Total_Light_Diesel_Product_Flowrate', 'Total_Heavy_Diesel_Product_Flowrate', 'Total_Atmospheric_Residue_Flowrate', 'Blend_Yield_Gas & LPG', 'Blend_Yield_Kerosene', 'Blend_Yield_Light Diesel', 'Blend_Yield_Heavy Diesel', 'Blend_Yield_RCO', 'Light_Diesel_to_DHT_Unit_Flowrate', 'Kerosene_to_Light_Diesel_DHT_Flowrate', 'Atm_Residue_to_RFCC_Unit_Flowrate', 'Atm_Residue_to_Storage_Flowrate'] ... (total: 667)\n",
            "\n",
            "Final verification - Data types in training dataset:\n",
            "float64    667\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First 5 rows of the TRAINING dataset (no date column):\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5945b247-a3a0-488c-9efb-3b89a8f6cd03\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total_Stabilized_Naphtha_Product_Flowrate</th>\n",
              "      <th>Total_Kerosene_Product_Flowrate</th>\n",
              "      <th>Jet_Fuel_Product_Train1_Flowrate</th>\n",
              "      <th>Total_Light_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Heavy_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Atmospheric_Residue_Flowrate</th>\n",
              "      <th>Blend_Yield_Gas &amp; LPG</th>\n",
              "      <th>Blend_Yield_Kerosene</th>\n",
              "      <th>Blend_Yield_Light Diesel</th>\n",
              "      <th>Blend_Yield_Heavy Diesel</th>\n",
              "      <th>...</th>\n",
              "      <th>101FI8301</th>\n",
              "      <th>101FI8401</th>\n",
              "      <th>101FI8801</th>\n",
              "      <th>101FI8901</th>\n",
              "      <th>crude_WTI Midland</th>\n",
              "      <th>crude_MERO</th>\n",
              "      <th>101FIC3802</th>\n",
              "      <th>101FIC5801</th>\n",
              "      <th>101FIC6702</th>\n",
              "      <th>MW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 667 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5945b247-a3a0-488c-9efb-3b89a8f6cd03')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5945b247-a3a0-488c-9efb-3b89a8f6cd03 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5945b247-a3a0-488c-9efb-3b89a8f6cd03');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Total_Stabilized_Naphtha_Product_Flowrate, Total_Kerosene_Product_Flowrate, Jet_Fuel_Product_Train1_Flowrate, Total_Light_Diesel_Product_Flowrate, Total_Heavy_Diesel_Product_Flowrate, Total_Atmospheric_Residue_Flowrate, Blend_Yield_Gas & LPG, Blend_Yield_Kerosene, Blend_Yield_Light Diesel, Blend_Yield_Heavy Diesel, Blend_Yield_RCO, Light_Diesel_to_DHT_Unit_Flowrate, Kerosene_to_Light_Diesel_DHT_Flowrate, Atm_Residue_to_RFCC_Unit_Flowrate, Atm_Residue_to_Storage_Flowrate, Crude_Column_Naphtha_to_SGCU_Flowrate, Heavy_Diesel_to_MHC_Flowrate, Atm_Residue_to_Storage_EE1027_Flowrate, blend_id, API, Sulphur, crude_AGB, crude_BOL, crude_CJB, crude_WCD, crude_QUI, crude_AME, crude_FOR, crude_ABO, crude_OK2, crude_ESC, crude_BOC, crude_BRL, crude_AKP, crude_ASC, crude_AGO, crude_YOH, crude_OKW, crude_ERH, crude_PAZ, crude_Nembe, crude_Total, crude_Gas & LPG, crude_Naptha , crude_Kerosene, crude_Light Diesel, crude_Heavy Diesel, crude_RCO, Train_A_Raw_Crude_Flow, Train_B_Raw_Crude_Flow, Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU, Rich_Gas_Knockout_Drum_Liquid_to_CDU, 100FI2901, 100FI2902, 100FI4001, 100FI4002, 100FI4003, 101AI8501A, 101AI8501B, 101AI8501C, 101AI8501D, 101AI8601A, 101AI8601B, 101AI8601C, 101AI8601D, 101AI8701A, 101AI8701B, 101AI8701C, 101AI8701D, 101FI1401, Train_A_Raw_Crude_Flow.1, Train_B_Raw_Crude_Flow.1, 101FI3201, 101FI3202, 101FI3702, 101FI6502, 101FI8102, 101FI8103, 101FI8110, 101FI8202, 101FI8203, 101FI8210, 101FI8302, 101FI8303, 101FI8310, 101FI8402, 101FI8403, 101FI8410, 101FI8802, 101FI8803, 101FI8810, 101FI8902, 101FI8903, 101FI8910, 101FIC1404, 101FIC1405, 101FIC1701, 101FIC2002, 101FIC2004, 101FIC2101, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 667 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Last 5 rows of the TRAINING dataset (no date column):\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f3b35f6b-e766-43cf-8c1d-abc3f21c9534\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total_Stabilized_Naphtha_Product_Flowrate</th>\n",
              "      <th>Total_Kerosene_Product_Flowrate</th>\n",
              "      <th>Jet_Fuel_Product_Train1_Flowrate</th>\n",
              "      <th>Total_Light_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Heavy_Diesel_Product_Flowrate</th>\n",
              "      <th>Total_Atmospheric_Residue_Flowrate</th>\n",
              "      <th>Blend_Yield_Gas &amp; LPG</th>\n",
              "      <th>Blend_Yield_Kerosene</th>\n",
              "      <th>Blend_Yield_Light Diesel</th>\n",
              "      <th>Blend_Yield_Heavy Diesel</th>\n",
              "      <th>...</th>\n",
              "      <th>101FI8301</th>\n",
              "      <th>101FI8401</th>\n",
              "      <th>101FI8801</th>\n",
              "      <th>101FI8901</th>\n",
              "      <th>crude_WTI Midland</th>\n",
              "      <th>crude_MERO</th>\n",
              "      <th>101FIC3802</th>\n",
              "      <th>101FIC5801</th>\n",
              "      <th>101FIC6702</th>\n",
              "      <th>MW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 667 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3b35f6b-e766-43cf-8c1d-abc3f21c9534')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f3b35f6b-e766-43cf-8c1d-abc3f21c9534 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f3b35f6b-e766-43cf-8c1d-abc3f21c9534');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Total_Stabilized_Naphtha_Product_Flowrate, Total_Kerosene_Product_Flowrate, Jet_Fuel_Product_Train1_Flowrate, Total_Light_Diesel_Product_Flowrate, Total_Heavy_Diesel_Product_Flowrate, Total_Atmospheric_Residue_Flowrate, Blend_Yield_Gas & LPG, Blend_Yield_Kerosene, Blend_Yield_Light Diesel, Blend_Yield_Heavy Diesel, Blend_Yield_RCO, Light_Diesel_to_DHT_Unit_Flowrate, Kerosene_to_Light_Diesel_DHT_Flowrate, Atm_Residue_to_RFCC_Unit_Flowrate, Atm_Residue_to_Storage_Flowrate, Crude_Column_Naphtha_to_SGCU_Flowrate, Heavy_Diesel_to_MHC_Flowrate, Atm_Residue_to_Storage_EE1027_Flowrate, blend_id, API, Sulphur, crude_AGB, crude_BOL, crude_CJB, crude_WCD, crude_QUI, crude_AME, crude_FOR, crude_ABO, crude_OK2, crude_ESC, crude_BOC, crude_BRL, crude_AKP, crude_ASC, crude_AGO, crude_YOH, crude_OKW, crude_ERH, crude_PAZ, crude_Nembe, crude_Total, crude_Gas & LPG, crude_Naptha , crude_Kerosene, crude_Light Diesel, crude_Heavy Diesel, crude_RCO, Train_A_Raw_Crude_Flow, Train_B_Raw_Crude_Flow, Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU, Rich_Gas_Knockout_Drum_Liquid_to_CDU, 100FI2901, 100FI2902, 100FI4001, 100FI4002, 100FI4003, 101AI8501A, 101AI8501B, 101AI8501C, 101AI8501D, 101AI8601A, 101AI8601B, 101AI8601C, 101AI8601D, 101AI8701A, 101AI8701B, 101AI8701C, 101AI8701D, 101FI1401, Train_A_Raw_Crude_Flow.1, Train_B_Raw_Crude_Flow.1, 101FI3201, 101FI3202, 101FI3702, 101FI6502, 101FI8102, 101FI8103, 101FI8110, 101FI8202, 101FI8203, 101FI8210, 101FI8302, 101FI8303, 101FI8310, 101FI8402, 101FI8403, 101FI8410, 101FI8802, 101FI8803, 101FI8810, 101FI8902, 101FI8903, 101FI8910, 101FIC1404, 101FIC1405, 101FIC1701, 101FIC2002, 101FIC2004, 101FIC2101, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 667 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Dataset summary saved to: ./dataset/custom/dataset_summary.txt\n",
            "\n",
            "🎉 Data preparation completed successfully!\n",
            "   - Use 'custom.csv' for Autoformer training (contains 667 numeric columns)\n",
            "   - Use 'custom_with_date.csv' for reference (contains date + 667 numeric columns)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the path to the MICE interpolated and concatenated data\n",
        "file_path = '/content/drive/MyDrive/final_concatenated_data_mice_imputed.csv' # Ensure this is the correct file path\n",
        "\n",
        "# List of specified target columns\n",
        "target_cols_list = [\n",
        "    'Total_Stabilized_Naphtha_Product_Flowrate',\n",
        "    'Total_Kerosene_Product_Flowrate',\n",
        "    'Jet_Fuel_Product_Train1_Flowrate',\n",
        "    'Total_Light_Diesel_Product_Flowrate',\n",
        "    'Total_Heavy_Diesel_Product_Flowrate',\n",
        "    'Total_Atmospheric_Residue_Flowrate',\n",
        "    'Blend_Yield_Gas & LPG',\n",
        "    'Blend_Yield_Kerosene',\n",
        "    'Blend_Yield_Light Diesel',\n",
        "    'Blend_Yield_Heavy Diesel',\n",
        "    'Blend_Yield_RCO'\n",
        "]\n",
        "\n",
        "# Load the combined CSV file with robust error handling\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if 'date' column exists and parse dates\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "            df = df.dropna(subset=['date'])\n",
        "            if not df.empty:\n",
        "                print(f\"Successfully loaded {file_path}: {len(df)} rows\")\n",
        "            else:\n",
        "                raise ValueError(f\"Error: {file_path} loaded but had no valid date entries after parsing.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Error: File {file_path} does not contain a 'date' column.\")\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        raise ValueError(f\"Error: File {file_path} is empty.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading file {file_path}: {e}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Error: File not found - {file_path}.\")\n",
        "\n",
        "# Sort by date and reset index\n",
        "df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "# dirty column drop\n",
        "if \"Tag\" in df.columns:\n",
        "    df = df.drop(columns=[\"Tag\"])\n",
        "    print(\"Tag column dropped\")\n",
        "\n",
        "# Display date range\n",
        "if not df.empty:\n",
        "    print(f\"Date range of combined data: {df['date'].min()} to {df['date'].max()}\")\n",
        "else:\n",
        "    print(\"Combined dataframe is empty.\")\n",
        "    exit()\n",
        "\n",
        "# Use the entire MICE dataset\n",
        "df_filtered = df.copy()\n",
        "\n",
        "print(f\"\\nFiltered data (using full MICE dataset): {len(df_filtered)} rows\")\n",
        "if not df_filtered.empty:\n",
        "    print(f\"Filtered date range: {df_filtered['date'].min()} to {df_filtered['date'].max()}\")\n",
        "else:\n",
        "    print(\"Filtered dataframe is empty.\")\n",
        "    exit()\n",
        "\n",
        "# Identify feature columns (all columns except 'date' and the specified target columns)\n",
        "feature_cols = [col for col in df_filtered.columns if col != 'date' and col not in target_cols_list]\n",
        "\n",
        "# Check if all specified target columns exist in the DataFrame\n",
        "missing_targets = [col for col in target_cols_list if col not in df_filtered.columns]\n",
        "if missing_targets:\n",
        "    raise ValueError(f\"❌ The following specified target columns were not found in the dataset: {missing_targets}\")\n",
        "\n",
        "print(f\"\\n🎯 Target columns identified: {target_cols_list}\")\n",
        "print(f\"📈 Feature columns identified: {feature_cols[:10]} ... ({len(feature_cols)} total)\")\n",
        "\n",
        "# CRITICAL FIX: Create TWO different datasets for Autoformer\n",
        "\n",
        "# 1. Dataset with date column (for time-based operations and splitting)\n",
        "df_with_date = df_filtered[['date'] + target_cols_list + feature_cols].copy()\n",
        "\n",
        "# 2. Dataset WITHOUT date column (for model training - this is what gets fed to the neural network)\n",
        "# The Autoformer library expects the data file to contain ONLY the numeric columns\n",
        "# The date handling is done separately in the data loader\n",
        "df_auto = df_filtered[target_cols_list + feature_cols].copy()\n",
        "\n",
        "# CRITICAL FIX: Convert all columns to numeric types BEFORE any other processing\n",
        "print(f\"\\n🔧 Converting all columns to numeric types...\")\n",
        "non_numeric_cols = []\n",
        "\n",
        "# Convert all columns except 'date' to numeric\n",
        "for col in df_auto.columns:\n",
        "    if not np.issubdtype(df_auto[col].dtype, np.number):\n",
        "        try:\n",
        "            # Convert to numeric, coercing errors to NaN\n",
        "            df_auto[col] = pd.to_numeric(df_auto[col], errors='coerce')\n",
        "            df_with_date[col] = pd.to_numeric(df_with_date[col], errors='coerce')\n",
        "            print(f\"✅ Converted {col} to numeric\")\n",
        "        except Exception as e:\n",
        "            non_numeric_cols.append(col)\n",
        "            print(f\"❌ Failed to convert {col} to numeric: {e}\")\n",
        "\n",
        "# Drop columns that couldn't be converted\n",
        "if non_numeric_cols:\n",
        "    print(f\"\\n⚠️  Dropping {len(non_numeric_cols)} columns that couldn't be converted to numeric:\")\n",
        "    for col in non_numeric_cols:\n",
        "        print(f\"   - {col}\")\n",
        "    df_auto = df_auto.drop(columns=non_numeric_cols)\n",
        "    df_with_date = df_with_date.drop(columns=non_numeric_cols)\n",
        "\n",
        "# Handle missing values (forward/backward fill) - This is a safeguard, MICE should have handled most\n",
        "df_with_date = df_with_date.fillna(method='ffill').fillna(method='bfill')\n",
        "df_auto = df_auto.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "# Verify all columns are numeric (except date in the first dataset)\n",
        "print(f\"\\nVerifying data types:\")\n",
        "print(f\"Date column dtype: {df_with_date['date'].dtype}\")\n",
        "\n",
        "# Final verification - check for any remaining non-numeric columns\n",
        "remaining_non_numeric = []\n",
        "for col in df_auto.columns:\n",
        "    if not np.issubdtype(df_auto[col].dtype, np.number):\n",
        "        remaining_non_numeric.append(col)\n",
        "\n",
        "if remaining_non_numeric:\n",
        "    print(f\"❌ Still have non-numeric columns: {remaining_non_numeric}\")\n",
        "    # Force conversion for any remaining non-numeric columns\n",
        "    for col in remaining_non_numeric:\n",
        "        df_auto[col] = pd.to_numeric(df_auto[col], errors='coerce')\n",
        "        df_with_date[col] = pd.to_numeric(df_with_date[col], errors='coerce')\n",
        "\n",
        "# Final cleanup - remove any rows with NaN values that might have been introduced\n",
        "initial_rows = len(df_auto)\n",
        "df_auto = df_auto.dropna()\n",
        "df_with_date = df_with_date.dropna()\n",
        "final_rows = len(df_auto)\n",
        "\n",
        "if initial_rows != final_rows:\n",
        "    print(f\"⚠️  Removed {initial_rows - final_rows} rows with NaN values after conversion\")\n",
        "\n",
        "print(f\"\\n✅ All columns in training dataset are now numeric\")\n",
        "print(f\"Training dataset shape (no date): {df_auto.shape}\")\n",
        "print(f\"Full dataset shape (with date): {df_with_date.shape}\")\n",
        "\n",
        "# Final verification\n",
        "print(f\"\\n🔍 Final data type verification:\")\n",
        "print(f\"   - All columns are numeric: {all(df_auto.dtypes.apply(lambda x: np.issubdtype(x, np.number)))}\")\n",
        "print(f\"   - No NaN values: {not df_auto.isnull().any().any()}\")\n",
        "print(f\"   - Data types: {df_auto.dtypes.value_counts()}\")\n",
        "\n",
        "# Display summary statistics for target columns\n",
        "print(\"\\nSummary statistics for target columns:\")\n",
        "display(df_auto[target_cols_list].describe())\n",
        "\n",
        "# Create output directory and save BOTH datasets\n",
        "os.makedirs('./dataset/custom/', exist_ok=True)\n",
        "\n",
        "# CRITICAL FIX: Create a dataset that Autoformer expects\n",
        "# Autoformer expects: ['date', ...other_columns..., target_column]\n",
        "# We need to create a proper date column and reorder columns\n",
        "\n",
        "# Create a date range for the dataset\n",
        "print(f\"\\n🔧 Creating proper date column for Autoformer...\")\n",
        "date_range = pd.date_range(start='2024-06-01', periods=len(df_auto), freq='D')\n",
        "df_auto_with_date = df_auto.copy()\n",
        "df_auto_with_date.insert(0, 'date', date_range)\n",
        "\n",
        "# Reorder columns to match Autoformer's expected format: ['date', ...features..., targets]\n",
        "# Put ALL target columns at the end for multivariate prediction\n",
        "target_cols = [col for col in target_cols_list if col in df_auto_with_date.columns]\n",
        "if target_cols:\n",
        "    # Get all columns except date and targets\n",
        "    feature_cols = [col for col in df_auto_with_date.columns if col not in ['date'] + target_cols]\n",
        "    # Reorder: date + features + all targets\n",
        "    df_auto_with_date = df_auto_with_date[['date'] + feature_cols + target_cols]\n",
        "    print(f\"✅ Reordered columns: date + {len(feature_cols)} features + {len(target_cols)} targets\")\n",
        "    print(f\"   - Target columns: {target_cols}\")\n",
        "else:\n",
        "    print(f\"⚠️  No target columns found, using original order\")\n",
        "    target_cols = [df_auto_with_date.columns[-1]]  # Use last column as target\n",
        "\n",
        "# Save the dataset WITH proper date column and column order (this is what Autoformer will use)\n",
        "output_path = './dataset/custom/custom.csv'\n",
        "df_auto_with_date.to_csv(output_path, index=False)\n",
        "\n",
        "# Save the dataset WITHOUT date column (for reference)\n",
        "output_path_no_date = './dataset/custom/custom_no_date.csv'\n",
        "df_auto.to_csv(output_path_no_date, index=False)\n",
        "\n",
        "print(f\"\\n✅ Saved training dataset (WITH DATE, PROPER ORDER) to {output_path}\")\n",
        "print(f\"✅ Saved reference dataset (NO DATE) to {output_path_no_date}\")\n",
        "print(f\"Final training dataset shape: {df_auto_with_date.shape}\")\n",
        "print(f\"Training dataset columns: {list(df_auto_with_date.columns)[:15]} ... (total: {len(df_auto_with_date.columns)})\")\n",
        "print(f\"Date range: {df_auto_with_date['date'].min()} to {df_auto_with_date['date'].max()}\")\n",
        "\n",
        "# Verify the training dataset has no string columns\n",
        "print(f\"\\nFinal verification - Data types in training dataset:\")\n",
        "print(df_auto.dtypes.value_counts())\n",
        "\n",
        "# Display first few rows of TRAINING dataset (without date)\n",
        "print(f\"\\nFirst 5 rows of the TRAINING dataset (no date column):\")\n",
        "display(df_auto.head())\n",
        "\n",
        "# Display last few rows of TRAINING dataset (without date)\n",
        "print(f\"\\nLast 5 rows of the TRAINING dataset (no date column):\")\n",
        "display(df_auto.tail())\n",
        "\n",
        "# Create a summary file with dataset information\n",
        "summary_info = {\n",
        "    'total_rows': len(df_auto),\n",
        "    'total_columns': len(df_auto.columns),\n",
        "    'target_columns': target_cols_list,\n",
        "    'num_target_columns': len(target_cols_list),\n",
        "    'feature_columns': feature_cols[:20],  # First 20 feature columns\n",
        "    'num_feature_columns': len(feature_cols),\n",
        "    'date_range_start': str(df_with_date['date'].min()),\n",
        "    'date_range_end': str(df_with_date['date'].max()),\n",
        "    'data_types': str(df_auto.dtypes.value_counts().to_dict())\n",
        "}\n",
        "\n",
        "summary_path = './dataset/custom/dataset_summary.txt'\n",
        "with open(summary_path, 'w') as f:\n",
        "    f.write(\"Autoformer Dataset Summary\\n\")\n",
        "    f.write(\"=\" * 30 + \"\\n\")\n",
        "    for key, value in summary_info.items():\n",
        "        f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "print(f\"\\n📊 Dataset summary saved to: {summary_path}\")\n",
        "print(f\"\\n🎉 Data preparation completed successfully!\")\n",
        "print(f\"   - Use 'custom.csv' for Autoformer training (contains {df_auto.shape[1]} numeric columns)\")\n",
        "print(f\"   - Use 'custom_with_date.csv' for reference (contains date + {df_auto.shape[1]} numeric columns)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW7jeCe6R3WG"
      },
      "source": [
        "MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c5620810",
        "outputId": "03063e08-2144-4dee-eaba-4510b4847b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All columns in the dataset:\n",
            "0: 'Total_Stabilized_Naphtha_Product_Flowrate' (type: object)\n",
            "1: 'Total_Kerosene_Product_Flowrate' (type: object)\n",
            "2: 'Jet_Fuel_Product_Train1_Flowrate' (type: object)\n",
            "3: 'Total_Light_Diesel_Product_Flowrate' (type: object)\n",
            "4: 'Total_Heavy_Diesel_Product_Flowrate' (type: object)\n",
            "5: 'Total_Atmospheric_Residue_Flowrate' (type: object)\n",
            "6: 'Blend_Yield_Gas & LPG' (type: object)\n",
            "7: 'Blend_Yield_Kerosene' (type: object)\n",
            "8: 'Blend_Yield_Light Diesel' (type: object)\n",
            "9: 'Blend_Yield_Heavy Diesel' (type: object)\n",
            "10: 'Blend_Yield_RCO' (type: object)\n",
            "11: 'Light_Diesel_to_DHT_Unit_Flowrate' (type: object)\n",
            "12: 'Kerosene_to_Light_Diesel_DHT_Flowrate' (type: object)\n",
            "13: 'Atm_Residue_to_RFCC_Unit_Flowrate' (type: object)\n",
            "14: 'Atm_Residue_to_Storage_Flowrate' (type: object)\n",
            "15: 'Crude_Column_Naphtha_to_SGCU_Flowrate' (type: object)\n",
            "16: 'Heavy_Diesel_to_MHC_Flowrate' (type: object)\n",
            "17: 'Atm_Residue_to_Storage_EE1027_Flowrate' (type: object)\n",
            "18: 'blend_id' (type: object)\n",
            "19: 'API' (type: object)\n",
            "20: 'Sulphur' (type: object)\n",
            "21: 'crude_AGB' (type: object)\n",
            "22: 'crude_BOL' (type: object)\n",
            "23: 'crude_CJB' (type: object)\n",
            "24: 'crude_WCD' (type: object)\n",
            "25: 'crude_QUI' (type: object)\n",
            "26: 'crude_AME' (type: object)\n",
            "27: 'crude_FOR' (type: object)\n",
            "28: 'crude_ABO' (type: object)\n",
            "29: 'crude_OK2' (type: object)\n",
            "30: 'crude_ESC' (type: object)\n",
            "31: 'crude_BOC' (type: object)\n",
            "32: 'crude_BRL' (type: object)\n",
            "33: 'crude_AKP' (type: object)\n",
            "34: 'crude_ASC' (type: object)\n",
            "35: 'crude_AGO' (type: object)\n",
            "36: 'crude_YOH' (type: object)\n",
            "37: 'crude_OKW' (type: object)\n",
            "38: 'crude_ERH' (type: object)\n",
            "39: 'crude_PAZ' (type: object)\n",
            "40: 'crude_Nembe' (type: object)\n",
            "41: 'crude_Total' (type: object)\n",
            "42: 'crude_Gas & LPG' (type: object)\n",
            "43: 'crude_Naptha ' (type: object)\n",
            "44: 'crude_Kerosene' (type: object)\n",
            "45: 'crude_Light Diesel' (type: object)\n",
            "46: 'crude_Heavy Diesel' (type: object)\n",
            "47: 'crude_RCO' (type: object)\n",
            "48: 'Train_A_Raw_Crude_Flow' (type: object)\n",
            "49: 'Train_B_Raw_Crude_Flow' (type: object)\n",
            "50: 'Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU' (type: object)\n",
            "51: 'Rich_Gas_Knockout_Drum_Liquid_to_CDU' (type: object)\n",
            "52: '100FI2901' (type: object)\n",
            "53: '100FI2902' (type: object)\n",
            "54: '100FI4001' (type: object)\n",
            "55: '100FI4002' (type: object)\n",
            "56: '100FI4003' (type: object)\n",
            "57: '101AI8501A' (type: object)\n",
            "58: '101AI8501B' (type: object)\n",
            "59: '101AI8501C' (type: object)\n",
            "60: '101AI8501D' (type: object)\n",
            "61: '101AI8601A' (type: object)\n",
            "62: '101AI8601B' (type: object)\n",
            "63: '101AI8601C' (type: object)\n",
            "64: '101AI8601D' (type: object)\n",
            "65: '101AI8701A' (type: object)\n",
            "66: '101AI8701B' (type: object)\n",
            "67: '101AI8701C' (type: object)\n",
            "68: '101AI8701D' (type: object)\n",
            "69: '101FI1401' (type: object)\n",
            "70: 'Train_A_Raw_Crude_Flow.1' (type: object)\n",
            "71: 'Train_B_Raw_Crude_Flow.1' (type: object)\n",
            "72: '101FI3201' (type: object)\n",
            "73: '101FI3202' (type: object)\n",
            "74: '101FI3702' (type: object)\n",
            "75: '101FI6502' (type: object)\n",
            "76: '101FI8102' (type: object)\n",
            "77: '101FI8103' (type: object)\n",
            "78: '101FI8110' (type: object)\n",
            "79: '101FI8202' (type: object)\n",
            "80: '101FI8203' (type: object)\n",
            "81: '101FI8210' (type: object)\n",
            "82: '101FI8302' (type: object)\n",
            "83: '101FI8303' (type: object)\n",
            "84: '101FI8310' (type: object)\n",
            "85: '101FI8402' (type: object)\n",
            "86: '101FI8403' (type: object)\n",
            "87: '101FI8410' (type: object)\n",
            "88: '101FI8802' (type: object)\n",
            "89: '101FI8803' (type: object)\n",
            "90: '101FI8810' (type: object)\n",
            "91: '101FI8902' (type: object)\n",
            "92: '101FI8903' (type: object)\n",
            "93: '101FI8910' (type: object)\n",
            "94: '101FIC1404' (type: object)\n",
            "95: '101FIC1405' (type: object)\n",
            "96: '101FIC1701' (type: object)\n",
            "97: '101FIC2002' (type: object)\n",
            "98: '101FIC2004' (type: object)\n",
            "99: '101FIC2101' (type: object)\n",
            "100: '101FIC2201' (type: object)\n",
            "101: '101FIC2301' (type: object)\n",
            "102: '101FIC2401' (type: object)\n",
            "103: '101FIC2402' (type: object)\n",
            "104: '101FIC2501' (type: object)\n",
            "105: '101FIC2601' (type: object)\n",
            "106: '101FIC2801' (type: object)\n",
            "107: '101FIC2901' (type: object)\n",
            "108: '101FIC2902' (type: object)\n",
            "109: '101FIC3003' (type: object)\n",
            "110: '101FIC3102' (type: object)\n",
            "111: '101FIC3105' (type: object)\n",
            "112: '101FIC3501' (type: object)\n",
            "113: '101FIC3502' (type: object)\n",
            "114: '101FIC3602' (type: object)\n",
            "115: '101FIC3804' (type: object)\n",
            "116: '101FIC4401' (type: object)\n",
            "117: '101FIC4501' (type: object)\n",
            "118: '101FIC4502A' (type: object)\n",
            "119: '101FIC4502B' (type: object)\n",
            "120: '101FIC4502C' (type: object)\n",
            "121: '101FIC4502D' (type: object)\n",
            "122: '101FIC4502E' (type: object)\n",
            "123: '101FIC4502F' (type: object)\n",
            "124: '101FIC4504A' (type: object)\n",
            "125: '101FIC4504B' (type: object)\n",
            "126: '101FIC4504C' (type: object)\n",
            "127: '101FIC4504D' (type: object)\n",
            "128: '101FIC4504E' (type: object)\n",
            "129: '101FIC4504F' (type: object)\n",
            "130: '101FIC4601' (type: object)\n",
            "131: '101FIC4602A' (type: object)\n",
            "132: '101FIC4602B' (type: object)\n",
            "133: '101FIC4602C' (type: object)\n",
            "134: '101FIC4602D' (type: object)\n",
            "135: '101FIC4602E' (type: object)\n",
            "136: '101FIC4602F' (type: object)\n",
            "137: '101FIC4604A' (type: object)\n",
            "138: '101FIC4604B' (type: object)\n",
            "139: '101FIC4604C' (type: object)\n",
            "140: '101FIC4604D' (type: object)\n",
            "141: '101FIC4604E' (type: object)\n",
            "142: '101FIC4604F' (type: object)\n",
            "143: '101FIC4701' (type: object)\n",
            "144: '101FIC4703' (type: object)\n",
            "145: '101FIC4801' (type: object)\n",
            "146: '101FIC4903' (type: object)\n",
            "147: '101FIC5101' (type: object)\n",
            "148: '101FIC5202' (type: object)\n",
            "149: '101FIC5302' (type: object)\n",
            "150: '101FIC5402' (type: object)\n",
            "151: '101FIC5803' (type: object)\n",
            "152: '101FIC5804' (type: object)\n",
            "153: '101FIC5805' (type: object)\n",
            "154: '101FIC6102' (type: object)\n",
            "155: '101FIC6104' (type: object)\n",
            "156: '101FIC6301' (type: object)\n",
            "157: '101FIC6401' (type: object)\n",
            "158: '101FIC6704' (type: object)\n",
            "159: '101FIC6801' (type: object)\n",
            "160: '101FIC6802A' (type: object)\n",
            "161: '101FIC6802B' (type: object)\n",
            "162: '101FIC6802C' (type: object)\n",
            "163: '101FIC6802D' (type: object)\n",
            "164: '101FIC6802E' (type: object)\n",
            "165: '101FIC6802F' (type: object)\n",
            "166: '101FIC6804A' (type: object)\n",
            "167: '101FIC6804B' (type: object)\n",
            "168: '101FIC6804C' (type: object)\n",
            "169: '101FIC6804D' (type: object)\n",
            "170: '101FIC6804E' (type: object)\n",
            "171: '101FIC6804F' (type: object)\n",
            "172: '101FIC8101' (type: object)\n",
            "173: '101FIC8109' (type: object)\n",
            "174: '101FIC8201' (type: object)\n",
            "175: '101FIC8209' (type: object)\n",
            "176: '101FIC8301' (type: object)\n",
            "177: '101FIC8309' (type: object)\n",
            "178: '101FIC8401' (type: object)\n",
            "179: '101FIC8409' (type: object)\n",
            "180: '101FIC8801' (type: object)\n",
            "181: '101FIC8809' (type: object)\n",
            "182: '101FIC8901' (type: object)\n",
            "183: '101FIC8909' (type: object)\n",
            "184: '101FV8103' (type: object)\n",
            "185: '101FV8109' (type: object)\n",
            "186: '101FV8203' (type: object)\n",
            "187: '101FV8209' (type: object)\n",
            "188: '101FV8303' (type: object)\n",
            "189: '101FV8309' (type: object)\n",
            "190: '101FV8403' (type: object)\n",
            "191: '101FV8409' (type: object)\n",
            "192: '101FV8803' (type: object)\n",
            "193: '101FV8809' (type: object)\n",
            "194: '101FV8903' (type: object)\n",
            "195: '101FV8909' (type: object)\n",
            "196: '101PDI4903' (type: object)\n",
            "197: '101PDI5101' (type: object)\n",
            "198: '101PDIC2201' (type: object)\n",
            "199: '101PDIC2501' (type: object)\n",
            "200: '101PDIC2601' (type: object)\n",
            "201: '101PDIC2801' (type: object)\n",
            "202: '101PI4702' (type: object)\n",
            "203: '101PI4904' (type: object)\n",
            "204: '101PI8104' (type: object)\n",
            "205: '101PI8204' (type: object)\n",
            "206: '101PI8304' (type: object)\n",
            "207: '101PI8404' (type: object)\n",
            "208: '101PI8804' (type: object)\n",
            "209: '101PI8904' (type: object)\n",
            "210: '101PIC3201' (type: object)\n",
            "211: '101PIC3202' (type: object)\n",
            "212: '101PIC3602' (type: object)\n",
            "213: '101PIC3701' (type: object)\n",
            "214: '101PIC5701' (type: object)\n",
            "215: '101PIC6402' (type: object)\n",
            "216: '101PIC6501' (type: object)\n",
            "217: '101PIC8103' (type: object)\n",
            "218: '101PIC8109' (type: object)\n",
            "219: '101PIC8203' (type: object)\n",
            "220: '101PIC8209' (type: object)\n",
            "221: '101PIC8303' (type: object)\n",
            "222: '101PIC8309' (type: object)\n",
            "223: '101PIC8403' (type: object)\n",
            "224: '101PIC8409' (type: object)\n",
            "225: '101PIC8803' (type: object)\n",
            "226: '101PIC8809' (type: object)\n",
            "227: '101PIC8903' (type: object)\n",
            "228: '101PIC8909' (type: object)\n",
            "229: '101PV3701B' (type: object)\n",
            "230: '101PV5701B' (type: object)\n",
            "231: '101PV6501B' (type: object)\n",
            "232: '101TI1402' (type: object)\n",
            "233: '101TI1404' (type: object)\n",
            "234: '101TI1505' (type: object)\n",
            "235: '101TI1511' (type: object)\n",
            "236: '101TI1606' (type: object)\n",
            "237: '101TI1612' (type: object)\n",
            "238: '101TI1702' (type: object)\n",
            "239: '101TI1703' (type: object)\n",
            "240: '101TI1708' (type: object)\n",
            "241: '101TI1712' (type: object)\n",
            "242: '101TI1802' (type: object)\n",
            "243: '101TI1804' (type: object)\n",
            "244: '101TI1810' (type: object)\n",
            "245: '101TI1812' (type: object)\n",
            "246: '101TI1902' (type: object)\n",
            "247: '101TI1905' (type: object)\n",
            "248: '101TI1910' (type: object)\n",
            "249: '101TI1912' (type: object)\n",
            "250: '101TI2002' (type: object)\n",
            "251: '101TI2005' (type: object)\n",
            "252: '101TI2102' (type: object)\n",
            "253: '101TI2201' (type: object)\n",
            "254: '101TI2504' (type: object)\n",
            "255: '101TI2601' (type: object)\n",
            "256: '101TI2805' (type: object)\n",
            "257: '101TI2902' (type: object)\n",
            "258: '101TI2904' (type: object)\n",
            "259: '101TI3001' (type: object)\n",
            "260: '101TI3005' (type: object)\n",
            "261: '101TI3101' (type: object)\n",
            "262: '101TI3104' (type: object)\n",
            "263: '101TI3105' (type: object)\n",
            "264: '101TI3106' (type: object)\n",
            "265: '101TI3112' (type: object)\n",
            "266: '101TI3114' (type: object)\n",
            "267: '101TI3130' (type: object)\n",
            "268: '101TI3201' (type: object)\n",
            "269: '101TI3203' (type: object)\n",
            "270: '101TI3207' (type: object)\n",
            "271: '101TI3211' (type: object)\n",
            "272: '101TI3212' (type: object)\n",
            "273: '101TI3302' (type: object)\n",
            "274: '101TI3304' (type: object)\n",
            "275: '101TI3306' (type: object)\n",
            "276: '101TI3309' (type: object)\n",
            "277: '101TI3311' (type: object)\n",
            "278: '101TI3402' (type: object)\n",
            "279: '101TI3404' (type: object)\n",
            "280: '101TI3408' (type: object)\n",
            "281: '101TI3410' (type: object)\n",
            "282: '101TI3412' (type: object)\n",
            "283: '101TI3414' (type: object)\n",
            "284: '101TI3418' (type: object)\n",
            "285: '101TI3422' (type: object)\n",
            "286: '101TI3424' (type: object)\n",
            "287: '101TI3602' (type: object)\n",
            "288: '101TI3604' (type: object)\n",
            "289: '101TI3704' (type: object)\n",
            "290: '101TI3904' (type: object)\n",
            "291: '101TI3912' (type: object)\n",
            "292: '101TI4002' (type: object)\n",
            "293: '101TI4004' (type: object)\n",
            "294: '101TI4008' (type: object)\n",
            "295: '101TI4010' (type: object)\n",
            "296: '101TI4110' (type: object)\n",
            "297: '101TI4112' (type: object)\n",
            "298: '101TI4115' (type: object)\n",
            "299: '101TI4204' (type: object)\n",
            "300: '101TI4207' (type: object)\n",
            "301: '101TI4209' (type: object)\n",
            "302: '101TI4211' (type: object)\n",
            "303: '101TI4213' (type: object)\n",
            "304: '101TI4306' (type: object)\n",
            "305: '101TI4307' (type: object)\n",
            "306: '101TI4312' (type: object)\n",
            "307: '101TI4313' (type: object)\n",
            "308: '101TI4323' (type: object)\n",
            "309: '101TI4325' (type: object)\n",
            "310: '101TI4401' (type: object)\n",
            "311: '101TI4410' (type: object)\n",
            "312: '101TI4412' (type: object)\n",
            "313: '101TI4421' (type: object)\n",
            "314: '101TI4424' (type: object)\n",
            "315: '101TI4425' (type: object)\n",
            "316: '101TI4427' (type: object)\n",
            "317: '101TI4703' (type: object)\n",
            "318: '101TI4704' (type: object)\n",
            "319: '101TI4706' (type: object)\n",
            "320: '101TI4708' (type: object)\n",
            "321: '101TI4718' (type: object)\n",
            "322: '101TI4808' (type: object)\n",
            "323: '101TI4903' (type: object)\n",
            "324: '101TI4904' (type: object)\n",
            "325: '101TI4905' (type: object)\n",
            "326: '101TI4906' (type: object)\n",
            "327: '101TI4909' (type: object)\n",
            "328: '101TI4912' (type: object)\n",
            "329: '101TI5202' (type: object)\n",
            "330: '101TI5301' (type: object)\n",
            "331: '101TI5401' (type: object)\n",
            "332: '101TI5501' (type: object)\n",
            "333: '101TI5704' (type: object)\n",
            "334: '101TI6105' (type: object)\n",
            "335: '101TI6201' (type: object)\n",
            "336: '101TI6301' (type: object)\n",
            "337: '101TI6304' (type: object)\n",
            "338: '101TI6306' (type: object)\n",
            "339: '101TI6402' (type: object)\n",
            "340: '101TI6404' (type: object)\n",
            "341: '101TI6504' (type: object)\n",
            "342: '101TI8507' (type: object)\n",
            "343: '101TI8514' (type: object)\n",
            "344: '101TI8607' (type: object)\n",
            "345: '101TI8614' (type: object)\n",
            "346: '101TI8707' (type: object)\n",
            "347: '101TI8714' (type: object)\n",
            "348: '101TIC4542' (type: object)\n",
            "349: '101TIC4544' (type: object)\n",
            "350: '101TIC4622' (type: object)\n",
            "351: '101TIC4644' (type: object)\n",
            "352: '101TIC6822' (type: object)\n",
            "353: '101TIC6844' (type: object)\n",
            "354: '101TXI4505A' (type: object)\n",
            "355: '101TXI4505B' (type: object)\n",
            "356: '101TXI4505C' (type: object)\n",
            "357: '101TXI4505D' (type: object)\n",
            "358: '101TXI4505E' (type: object)\n",
            "359: '101TXI4505F' (type: object)\n",
            "360: '101TXI4506A' (type: object)\n",
            "361: '101TXI4506B' (type: object)\n",
            "362: '101TXI4506C' (type: object)\n",
            "363: '101TXI4506D' (type: object)\n",
            "364: '101TXI4506E' (type: object)\n",
            "365: '101TXI4506F' (type: object)\n",
            "366: '101TXI4507A' (type: object)\n",
            "367: '101TXI4507B' (type: object)\n",
            "368: '101TXI4507C' (type: object)\n",
            "369: '101TXI4507D' (type: object)\n",
            "370: '101TXI4507E' (type: object)\n",
            "371: '101TXI4507F' (type: object)\n",
            "372: '101TXI4508A' (type: object)\n",
            "373: '101TXI4508B' (type: object)\n",
            "374: '101TXI4508C' (type: object)\n",
            "375: '101TXI4508D' (type: object)\n",
            "376: '101TXI4508E' (type: object)\n",
            "377: '101TXI4508F' (type: object)\n",
            "378: '101TXI4509A' (type: object)\n",
            "379: '101TXI4509B' (type: object)\n",
            "380: '101TXI4509C' (type: object)\n",
            "381: '101TXI4509D' (type: object)\n",
            "382: '101TXI4509E' (type: object)\n",
            "383: '101TXI4509F' (type: object)\n",
            "384: '101TXI4510A' (type: object)\n",
            "385: '101TXI4510B' (type: object)\n",
            "386: '101TXI4510C' (type: object)\n",
            "387: '101TXI4510D' (type: object)\n",
            "388: '101TXI4510E' (type: object)\n",
            "389: '101TXI4510F' (type: object)\n",
            "390: '101TXI4605A' (type: object)\n",
            "391: '101TXI4605B' (type: object)\n",
            "392: '101TXI4605C' (type: object)\n",
            "393: '101TXI4605D' (type: object)\n",
            "394: '101TXI4605E' (type: object)\n",
            "395: '101TXI4605F' (type: object)\n",
            "396: '101TXI4606A' (type: object)\n",
            "397: '101TXI4606B' (type: object)\n",
            "398: '101TXI4606C' (type: object)\n",
            "399: '101TXI4606D' (type: object)\n",
            "400: '101TXI4606E' (type: object)\n",
            "401: '101TXI4606F' (type: object)\n",
            "402: '101TXI4607A' (type: object)\n",
            "403: '101TXI4607B' (type: object)\n",
            "404: '101TXI4607C' (type: object)\n",
            "405: '101TXI4607D' (type: object)\n",
            "406: '101TXI4607E' (type: object)\n",
            "407: '101TXI4607F' (type: object)\n",
            "408: '101TXI4608A' (type: object)\n",
            "409: '101TXI4608B' (type: object)\n",
            "410: '101TXI4608C' (type: object)\n",
            "411: '101TXI4608D' (type: object)\n",
            "412: '101TXI4608E' (type: object)\n",
            "413: '101TXI4608F' (type: object)\n",
            "414: '101TXI4609A' (type: object)\n",
            "415: '101TXI4609B' (type: object)\n",
            "416: '101TXI4609C' (type: object)\n",
            "417: '101TXI4609D' (type: object)\n",
            "418: '101TXI4609E' (type: object)\n",
            "419: '101TXI4609F' (type: object)\n",
            "420: '101TXI4610A' (type: object)\n",
            "421: '101TXI4610B' (type: object)\n",
            "422: '101TXI4610C' (type: object)\n",
            "423: '101TXI4610D' (type: object)\n",
            "424: '101TXI4610E' (type: object)\n",
            "425: '101TXI4610F' (type: object)\n",
            "426: '101TXI6805A' (type: object)\n",
            "427: '101TXI6805B' (type: object)\n",
            "428: '101TXI6805C' (type: object)\n",
            "429: '101TXI6805D' (type: object)\n",
            "430: '101TXI6805E' (type: object)\n",
            "431: '101TXI6805F' (type: object)\n",
            "432: '101TXI6806A' (type: object)\n",
            "433: '101TXI6806B' (type: object)\n",
            "434: '101TXI6806C' (type: object)\n",
            "435: '101TXI6806D' (type: object)\n",
            "436: '101TXI6806E' (type: object)\n",
            "437: '101TXI6806F' (type: object)\n",
            "438: '101TXI6807A' (type: object)\n",
            "439: '101TXI6807B' (type: object)\n",
            "440: '101TXI6807C' (type: object)\n",
            "441: '101TXI6807D' (type: object)\n",
            "442: '101TXI6807E' (type: object)\n",
            "443: '101TXI6807F' (type: object)\n",
            "444: '101TXI6808A' (type: object)\n",
            "445: '101TXI6808B' (type: object)\n",
            "446: '101TXI6808C' (type: object)\n",
            "447: '101TXI6808D' (type: object)\n",
            "448: '101TXI6808E' (type: object)\n",
            "449: '101TXI6808F' (type: object)\n",
            "450: '101TXI6809A' (type: object)\n",
            "451: '101TXI6809B' (type: object)\n",
            "452: '101TXI6809C' (type: object)\n",
            "453: '101TXI6809D' (type: object)\n",
            "454: '101TXI6809E' (type: object)\n",
            "455: '101TXI6809F' (type: object)\n",
            "456: '101TXI6810A' (type: object)\n",
            "457: '101TXI6810B' (type: object)\n",
            "458: '101TXI6810C' (type: object)\n",
            "459: '101TXI6810D' (type: object)\n",
            "460: '101TXI6810E' (type: object)\n",
            "461: '101TXI6810F' (type: object)\n",
            "462: '102FI1201' (type: object)\n",
            "463: '102FI1501' (type: object)\n",
            "464: '102FI1701' (type: object)\n",
            "465: '102FI1803' (type: object)\n",
            "466: '102FI2302' (type: object)\n",
            "467: '102FI2401' (type: object)\n",
            "468: '102FI2901' (type: object)\n",
            "469: '102FIC2101' (type: object)\n",
            "470: '102FIC2501' (type: object)\n",
            "471: 'Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU.1' (type: object)\n",
            "472: 'Rich_Gas_Knockout_Drum_Liquid_to_CDU.1' (type: object)\n",
            "473: '102FIC2801' (type: object)\n",
            "474: '102FIC2802' (type: object)\n",
            "475: '102FIC2804' (type: object)\n",
            "476: '102FIC2902' (type: object)\n",
            "477: '102FIC3001' (type: object)\n",
            "478: '102FIC3101' (type: object)\n",
            "479: '102FIC3301' (type: object)\n",
            "480: '102FIC3401' (type: object)\n",
            "481: '102FIC3801' (type: object)\n",
            "482: '102FIC3802' (type: object)\n",
            "483: '102FIC3803' (type: object)\n",
            "484: '102FIC4201' (type: object)\n",
            "485: '102FIC4202' (type: object)\n",
            "486: '102FIC4401' (type: object)\n",
            "487: '102PI1701' (type: object)\n",
            "488: '102PI1801' (type: object)\n",
            "489: '102PI1803' (type: object)\n",
            "490: '102PI2001' (type: object)\n",
            "491: '102PI2002' (type: object)\n",
            "492: '102PI2901' (type: object)\n",
            "493: '102PI3201' (type: object)\n",
            "494: '102PIC2402' (type: object)\n",
            "495: '102PV2402B' (type: object)\n",
            "496: '102TI1601' (type: object)\n",
            "497: '102TI1616' (type: object)\n",
            "498: '102TI1801' (type: object)\n",
            "499: '102TI1802' (type: object)\n",
            "500: '102TI1902' (type: object)\n",
            "501: '102TI1905' (type: object)\n",
            "502: '102TI1910' (type: object)\n",
            "503: '102TI1913' (type: object)\n",
            "504: '102TI2106' (type: object)\n",
            "505: '102TI2108' (type: object)\n",
            "506: '102TI2112' (type: object)\n",
            "507: '102TI2202' (type: object)\n",
            "508: '102TI2205' (type: object)\n",
            "509: '102TI2301' (type: object)\n",
            "510: '102TI2604' (type: object)\n",
            "511: '102TI2822' (type: object)\n",
            "512: '102TI2901' (type: object)\n",
            "513: '102TI2902' (type: object)\n",
            "514: '102TI2903' (type: object)\n",
            "515: '102TI3001' (type: object)\n",
            "516: '102TI3003' (type: object)\n",
            "517: '102TI3101' (type: object)\n",
            "518: '102TI3103' (type: object)\n",
            "519: '102TI3201' (type: object)\n",
            "520: '102TI3205' (type: object)\n",
            "521: '102TI3901' (type: object)\n",
            "522: '103FI1503A' (type: object)\n",
            "523: '103FI1802' (type: object)\n",
            "524: '103FIC1601' (type: object)\n",
            "525: '103FIC1801' (type: object)\n",
            "526: '103FIC1803' (type: object)\n",
            "527: '103FIC1804' (type: object)\n",
            "528: '103FIC1901' (type: object)\n",
            "529: '103FIC2102' (type: object)\n",
            "530: '103LIC1803' (type: object)\n",
            "531: '103PDI1603' (type: object)\n",
            "532: '103TI1601' (type: object)\n",
            "533: '103TI1602' (type: object)\n",
            "534: '103TI1901' (type: object)\n",
            "535: '103TI1905' (type: object)\n",
            "536: '104FI1501' (type: object)\n",
            "537: '104FI2201' (type: object)\n",
            "538: '104FI2202' (type: object)\n",
            "539: '104FI2301' (type: object)\n",
            "540: '104FI2401' (type: object)\n",
            "541: '104FIC1503' (type: object)\n",
            "542: '104FIC1701' (type: object)\n",
            "543: '104FIC1801' (type: object)\n",
            "544: '104FIC2901' (type: object)\n",
            "545: '104II1701' (type: object)\n",
            "546: '104II1801' (type: object)\n",
            "547: '104LIC1703' (type: object)\n",
            "548: '104LIC1803' (type: object)\n",
            "549: '104LIC2301' (type: object)\n",
            "550: '104LIC2401' (type: object)\n",
            "551: '104PDI1501' (type: object)\n",
            "552: '104PDI1601' (type: object)\n",
            "553: '104PDI2001' (type: object)\n",
            "554: '104PDI2101' (type: object)\n",
            "555: '104PDI2301' (type: object)\n",
            "556: '104PDI2401' (type: object)\n",
            "557: '104PDI2701' (type: object)\n",
            "558: '104PDI2702' (type: object)\n",
            "559: '104PDI2801' (type: object)\n",
            "560: '104PDI2802' (type: object)\n",
            "561: '104TI1201' (type: object)\n",
            "562: '104VI1701' (type: object)\n",
            "563: '104VI1801' (type: object)\n",
            "564: '112FI1301' (type: object)\n",
            "565: '114FIC1406' (type: object)\n",
            "566: '114FIC1702' (type: object)\n",
            "567: '121FI1502' (type: object)\n",
            "568: '121FIC10501' (type: object)\n",
            "569: '151FIC2801' (type: object)\n",
            "570: '152FIC3402' (type: object)\n",
            "571: '101AI8502A' (type: object)\n",
            "572: '101AI8502B' (type: object)\n",
            "573: '101AI8502C' (type: object)\n",
            "574: '101AI8502D' (type: object)\n",
            "575: '101AI8602A' (type: object)\n",
            "576: '101AI8602B' (type: object)\n",
            "577: '101AI8602C' (type: object)\n",
            "578: '101AI8602D' (type: object)\n",
            "579: '101AI8702A' (type: object)\n",
            "580: '101AI8702B' (type: object)\n",
            "581: '101AI8702D' (type: object)\n",
            "582: '101AI8703C' (type: object)\n",
            "583: '101FQ1401' (type: object)\n",
            "584: '101FQ3001' (type: object)\n",
            "585: '101FQ3101' (type: object)\n",
            "586: '101FQ4201' (type: object)\n",
            "587: '101FQ5303' (type: object)\n",
            "588: '102FQ2801' (type: object)\n",
            "589: '102FQ4201' (type: object)\n",
            "590: '102FQ4401' (type: object)\n",
            "591: 'crude_Egina' (type: object)\n",
            "592: 'crude_Utapate' (type: object)\n",
            "593: 'crude_Okono' (type: object)\n",
            "594: 'crude_SAB' (type: object)\n",
            "595: 'crude_Buzios' (type: object)\n",
            "596: 'Raw_Crude_Pump_Discharge_Flow' (type: object)\n",
            "597: '100FI3201' (type: object)\n",
            "598: '100FI3301' (type: object)\n",
            "599: '100FIC8101' (type: object)\n",
            "600: '100TI8102' (type: object)\n",
            "601: '100TIC8101' (type: object)\n",
            "602: 'Raw_Crude_Pump_Discharge_Flow.1' (type: object)\n",
            "603: '101TI1814' (type: object)\n",
            "604: '101TI1817' (type: object)\n",
            "605: '101TI1914' (type: object)\n",
            "606: '101TI2309' (type: object)\n",
            "607: '101TI2311' (type: object)\n",
            "608: '101TI2313' (type: object)\n",
            "609: '101TI2707' (type: object)\n",
            "610: '101TI2710' (type: object)\n",
            "611: '101TI2712' (type: object)\n",
            "612: '101TI2905' (type: object)\n",
            "613: '101TI3702' (type: object)\n",
            "614: '101TI5702' (type: object)\n",
            "615: '101TI6502' (type: object)\n",
            "616: '101TIC3605' (type: object)\n",
            "617: '101TIC6405' (type: object)\n",
            "618: '102TI1201' (type: object)\n",
            "619: '102TI1205' (type: object)\n",
            "620: '102TI1603' (type: object)\n",
            "621: '102TI1617' (type: object)\n",
            "622: '102TI2403' (type: object)\n",
            "623: '102TI2405' (type: object)\n",
            "624: '102TI2701' (type: object)\n",
            "625: '102TI2704' (type: object)\n",
            "626: '102TI2801' (type: object)\n",
            "627: '102TI2804' (type: object)\n",
            "628: '102TI2810' (type: object)\n",
            "629: '102TI3802' (type: object)\n",
            "630: '102TI3804' (type: object)\n",
            "631: '102TI3903' (type: object)\n",
            "632: '102TI3905' (type: object)\n",
            "633: '102TI3908' (type: object)\n",
            "634: '102TI4018' (type: object)\n",
            "635: '102TI4203' (type: object)\n",
            "636: '102TI4205' (type: object)\n",
            "637: '102TI4403' (type: object)\n",
            "638: '102TI4405' (type: object)\n",
            "639: '102TI4501' (type: object)\n",
            "640: '102TI4503' (type: object)\n",
            "641: '103TI2501' (type: object)\n",
            "642: '103TIC2504' (type: object)\n",
            "643: '104TI2001' (type: object)\n",
            "644: '104TI2003' (type: object)\n",
            "645: '104TI2101' (type: object)\n",
            "646: '104TI2103' (type: object)\n",
            "647: 'crude_TUL' (type: object)\n",
            "648: 'crude_CEI' (type: object)\n",
            "649: 'crude_IRC' (type: object)\n",
            "650: 'crude_MEO' (type: object)\n",
            "651: 'crude_Cieba' (type: object)\n",
            "652: '100FQ4001' (type: object)\n",
            "653: '100FQ4002' (type: object)\n",
            "654: '100FQ4003' (type: object)\n",
            "655: '101FI8101' (type: object)\n",
            "656: '101FI8201' (type: object)\n",
            "657: '101FI8301' (type: object)\n",
            "658: '101FI8401' (type: object)\n",
            "659: '101FI8801' (type: object)\n",
            "660: '101FI8901' (type: object)\n",
            "661: 'crude_WTI Midland' (type: object)\n",
            "662: 'crude_MERO' (type: object)\n",
            "663: '101FIC3802' (type: object)\n",
            "664: '101FIC5801' (type: object)\n",
            "665: '101FIC6702' (type: object)\n",
            "666: 'MW' (type: object)\n",
            "✅ Successfully configured:\n",
            "   - enc_in, dec_in: 0\n",
            "   - c_out (output dimension): 1\n",
            "   - Primary target column: 'Total_Stabilized_Naphtha_Product_Flowrate'\n",
            "   - All numeric columns (0): []\n",
            "   - Dataset shape: (0, 0)\n",
            "🔍 Verifying data before training...\n",
            "✅ Target column 'Total_Stabilized_Naphtha_Product_Flowrate' verified in dataset\n",
            "⚠️  Warning: No 'date' column found. Available columns:\n",
            "['Total_Stabilized_Naphtha_Product_Flowrate', 'Total_Kerosene_Product_Flowrate', 'Jet_Fuel_Product_Train1_Flowrate', 'Total_Light_Diesel_Product_Flowrate', 'Total_Heavy_Diesel_Product_Flowrate', 'Total_Atmospheric_Residue_Flowrate', 'Blend_Yield_Gas & LPG', 'Blend_Yield_Kerosene', 'Blend_Yield_Light Diesel', 'Blend_Yield_Heavy Diesel', 'Blend_Yield_RCO', 'Light_Diesel_to_DHT_Unit_Flowrate', 'Kerosene_to_Light_Diesel_DHT_Flowrate', 'Atm_Residue_to_RFCC_Unit_Flowrate', 'Atm_Residue_to_Storage_Flowrate', 'Crude_Column_Naphtha_to_SGCU_Flowrate', 'Heavy_Diesel_to_MHC_Flowrate', 'Atm_Residue_to_Storage_EE1027_Flowrate', 'blend_id', 'API', 'Sulphur', 'crude_AGB', 'crude_BOL', 'crude_CJB', 'crude_WCD', 'crude_QUI', 'crude_AME', 'crude_FOR', 'crude_ABO', 'crude_OK2', 'crude_ESC', 'crude_BOC', 'crude_BRL', 'crude_AKP', 'crude_ASC', 'crude_AGO', 'crude_YOH', 'crude_OKW', 'crude_ERH', 'crude_PAZ', 'crude_Nembe', 'crude_Total', 'crude_Gas & LPG', 'crude_Naptha ', 'crude_Kerosene', 'crude_Light Diesel', 'crude_Heavy Diesel', 'crude_RCO', 'Train_A_Raw_Crude_Flow', 'Train_B_Raw_Crude_Flow', 'Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU', 'Rich_Gas_Knockout_Drum_Liquid_to_CDU', '100FI2901', '100FI2902', '100FI4001', '100FI4002', '100FI4003', '101AI8501A', '101AI8501B', '101AI8501C', '101AI8501D', '101AI8601A', '101AI8601B', '101AI8601C', '101AI8601D', '101AI8701A', '101AI8701B', '101AI8701C', '101AI8701D', '101FI1401', 'Train_A_Raw_Crude_Flow.1', 'Train_B_Raw_Crude_Flow.1', '101FI3201', '101FI3202', '101FI3702', '101FI6502', '101FI8102', '101FI8103', '101FI8110', '101FI8202', '101FI8203', '101FI8210', '101FI8302', '101FI8303', '101FI8310', '101FI8402', '101FI8403', '101FI8410', '101FI8802', '101FI8803', '101FI8810', '101FI8902', '101FI8903', '101FI8910', '101FIC1404', '101FIC1405', '101FIC1701', '101FIC2002', '101FIC2004', '101FIC2101', '101FIC2201', '101FIC2301', '101FIC2401', '101FIC2402', '101FIC2501', '101FIC2601', '101FIC2801', '101FIC2901', '101FIC2902', '101FIC3003', '101FIC3102', '101FIC3105', '101FIC3501', '101FIC3502', '101FIC3602', '101FIC3804', '101FIC4401', '101FIC4501', '101FIC4502A', '101FIC4502B', '101FIC4502C', '101FIC4502D', '101FIC4502E', '101FIC4502F', '101FIC4504A', '101FIC4504B', '101FIC4504C', '101FIC4504D', '101FIC4504E', '101FIC4504F', '101FIC4601', '101FIC4602A', '101FIC4602B', '101FIC4602C', '101FIC4602D', '101FIC4602E', '101FIC4602F', '101FIC4604A', '101FIC4604B', '101FIC4604C', '101FIC4604D', '101FIC4604E', '101FIC4604F', '101FIC4701', '101FIC4703', '101FIC4801', '101FIC4903', '101FIC5101', '101FIC5202', '101FIC5302', '101FIC5402', '101FIC5803', '101FIC5804', '101FIC5805', '101FIC6102', '101FIC6104', '101FIC6301', '101FIC6401', '101FIC6704', '101FIC6801', '101FIC6802A', '101FIC6802B', '101FIC6802C', '101FIC6802D', '101FIC6802E', '101FIC6802F', '101FIC6804A', '101FIC6804B', '101FIC6804C', '101FIC6804D', '101FIC6804E', '101FIC6804F', '101FIC8101', '101FIC8109', '101FIC8201', '101FIC8209', '101FIC8301', '101FIC8309', '101FIC8401', '101FIC8409', '101FIC8801', '101FIC8809', '101FIC8901', '101FIC8909', '101FV8103', '101FV8109', '101FV8203', '101FV8209', '101FV8303', '101FV8309', '101FV8403', '101FV8409', '101FV8803', '101FV8809', '101FV8903', '101FV8909', '101PDI4903', '101PDI5101', '101PDIC2201', '101PDIC2501', '101PDIC2601', '101PDIC2801', '101PI4702', '101PI4904', '101PI8104', '101PI8204', '101PI8304', '101PI8404', '101PI8804', '101PI8904', '101PIC3201', '101PIC3202', '101PIC3602', '101PIC3701', '101PIC5701', '101PIC6402', '101PIC6501', '101PIC8103', '101PIC8109', '101PIC8203', '101PIC8209', '101PIC8303', '101PIC8309', '101PIC8403', '101PIC8409', '101PIC8803', '101PIC8809', '101PIC8903', '101PIC8909', '101PV3701B', '101PV5701B', '101PV6501B', '101TI1402', '101TI1404', '101TI1505', '101TI1511', '101TI1606', '101TI1612', '101TI1702', '101TI1703', '101TI1708', '101TI1712', '101TI1802', '101TI1804', '101TI1810', '101TI1812', '101TI1902', '101TI1905', '101TI1910', '101TI1912', '101TI2002', '101TI2005', '101TI2102', '101TI2201', '101TI2504', '101TI2601', '101TI2805', '101TI2902', '101TI2904', '101TI3001', '101TI3005', '101TI3101', '101TI3104', '101TI3105', '101TI3106', '101TI3112', '101TI3114', '101TI3130', '101TI3201', '101TI3203', '101TI3207', '101TI3211', '101TI3212', '101TI3302', '101TI3304', '101TI3306', '101TI3309', '101TI3311', '101TI3402', '101TI3404', '101TI3408', '101TI3410', '101TI3412', '101TI3414', '101TI3418', '101TI3422', '101TI3424', '101TI3602', '101TI3604', '101TI3704', '101TI3904', '101TI3912', '101TI4002', '101TI4004', '101TI4008', '101TI4010', '101TI4110', '101TI4112', '101TI4115', '101TI4204', '101TI4207', '101TI4209', '101TI4211', '101TI4213', '101TI4306', '101TI4307', '101TI4312', '101TI4313', '101TI4323', '101TI4325', '101TI4401', '101TI4410', '101TI4412', '101TI4421', '101TI4424', '101TI4425', '101TI4427', '101TI4703', '101TI4704', '101TI4706', '101TI4708', '101TI4718', '101TI4808', '101TI4903', '101TI4904', '101TI4905', '101TI4906', '101TI4909', '101TI4912', '101TI5202', '101TI5301', '101TI5401', '101TI5501', '101TI5704', '101TI6105', '101TI6201', '101TI6301', '101TI6304', '101TI6306', '101TI6402', '101TI6404', '101TI6504', '101TI8507', '101TI8514', '101TI8607', '101TI8614', '101TI8707', '101TI8714', '101TIC4542', '101TIC4544', '101TIC4622', '101TIC4644', '101TIC6822', '101TIC6844', '101TXI4505A', '101TXI4505B', '101TXI4505C', '101TXI4505D', '101TXI4505E', '101TXI4505F', '101TXI4506A', '101TXI4506B', '101TXI4506C', '101TXI4506D', '101TXI4506E', '101TXI4506F', '101TXI4507A', '101TXI4507B', '101TXI4507C', '101TXI4507D', '101TXI4507E', '101TXI4507F', '101TXI4508A', '101TXI4508B', '101TXI4508C', '101TXI4508D', '101TXI4508E', '101TXI4508F', '101TXI4509A', '101TXI4509B', '101TXI4509C', '101TXI4509D', '101TXI4509E', '101TXI4509F', '101TXI4510A', '101TXI4510B', '101TXI4510C', '101TXI4510D', '101TXI4510E', '101TXI4510F', '101TXI4605A', '101TXI4605B', '101TXI4605C', '101TXI4605D', '101TXI4605E', '101TXI4605F', '101TXI4606A', '101TXI4606B', '101TXI4606C', '101TXI4606D', '101TXI4606E', '101TXI4606F', '101TXI4607A', '101TXI4607B', '101TXI4607C', '101TXI4607D', '101TXI4607E', '101TXI4607F', '101TXI4608A', '101TXI4608B', '101TXI4608C', '101TXI4608D', '101TXI4608E', '101TXI4608F', '101TXI4609A', '101TXI4609B', '101TXI4609C', '101TXI4609D', '101TXI4609E', '101TXI4609F', '101TXI4610A', '101TXI4610B', '101TXI4610C', '101TXI4610D', '101TXI4610E', '101TXI4610F', '101TXI6805A', '101TXI6805B', '101TXI6805C', '101TXI6805D', '101TXI6805E', '101TXI6805F', '101TXI6806A', '101TXI6806B', '101TXI6806C', '101TXI6806D', '101TXI6806E', '101TXI6806F', '101TXI6807A', '101TXI6807B', '101TXI6807C', '101TXI6807D', '101TXI6807E', '101TXI6807F', '101TXI6808A', '101TXI6808B', '101TXI6808C', '101TXI6808D', '101TXI6808E', '101TXI6808F', '101TXI6809A', '101TXI6809B', '101TXI6809C', '101TXI6809D', '101TXI6809E', '101TXI6809F', '101TXI6810A', '101TXI6810B', '101TXI6810C', '101TXI6810D', '101TXI6810E', '101TXI6810F', '102FI1201', '102FI1501', '102FI1701', '102FI1803', '102FI2302', '102FI2401', '102FI2901', '102FIC2101', '102FIC2501', 'Rich_Sponge_Oil_Flash_Drum_Bottoms_to_CDU.1', 'Rich_Gas_Knockout_Drum_Liquid_to_CDU.1', '102FIC2801', '102FIC2802', '102FIC2804', '102FIC2902', '102FIC3001', '102FIC3101', '102FIC3301', '102FIC3401', '102FIC3801', '102FIC3802', '102FIC3803', '102FIC4201', '102FIC4202', '102FIC4401', '102PI1701', '102PI1801', '102PI1803', '102PI2001', '102PI2002', '102PI2901', '102PI3201', '102PIC2402', '102PV2402B', '102TI1601', '102TI1616', '102TI1801', '102TI1802', '102TI1902', '102TI1905', '102TI1910', '102TI1913', '102TI2106', '102TI2108', '102TI2112', '102TI2202', '102TI2205', '102TI2301', '102TI2604', '102TI2822', '102TI2901', '102TI2902', '102TI2903', '102TI3001', '102TI3003', '102TI3101', '102TI3103', '102TI3201', '102TI3205', '102TI3901', '103FI1503A', '103FI1802', '103FIC1601', '103FIC1801', '103FIC1803', '103FIC1804', '103FIC1901', '103FIC2102', '103LIC1803', '103PDI1603', '103TI1601', '103TI1602', '103TI1901', '103TI1905', '104FI1501', '104FI2201', '104FI2202', '104FI2301', '104FI2401', '104FIC1503', '104FIC1701', '104FIC1801', '104FIC2901', '104II1701', '104II1801', '104LIC1703', '104LIC1803', '104LIC2301', '104LIC2401', '104PDI1501', '104PDI1601', '104PDI2001', '104PDI2101', '104PDI2301', '104PDI2401', '104PDI2701', '104PDI2702', '104PDI2801', '104PDI2802', '104TI1201', '104VI1701', '104VI1801', '112FI1301', '114FIC1406', '114FIC1702', '121FI1502', '121FIC10501', '151FIC2801', '152FIC3402', '101AI8502A', '101AI8502B', '101AI8502C', '101AI8502D', '101AI8602A', '101AI8602B', '101AI8602C', '101AI8602D', '101AI8702A', '101AI8702B', '101AI8702D', '101AI8703C', '101FQ1401', '101FQ3001', '101FQ3101', '101FQ4201', '101FQ5303', '102FQ2801', '102FQ4201', '102FQ4401', 'crude_Egina', 'crude_Utapate', 'crude_Okono', 'crude_SAB', 'crude_Buzios', 'Raw_Crude_Pump_Discharge_Flow', '100FI3201', '100FI3301', '100FIC8101', '100TI8102', '100TIC8101', 'Raw_Crude_Pump_Discharge_Flow.1', '101TI1814', '101TI1817', '101TI1914', '101TI2309', '101TI2311', '101TI2313', '101TI2707', '101TI2710', '101TI2712', '101TI2905', '101TI3702', '101TI5702', '101TI6502', '101TIC3605', '101TIC6405', '102TI1201', '102TI1205', '102TI1603', '102TI1617', '102TI2403', '102TI2405', '102TI2701', '102TI2704', '102TI2801', '102TI2804', '102TI2810', '102TI3802', '102TI3804', '102TI3903', '102TI3905', '102TI3908', '102TI4018', '102TI4203', '102TI4205', '102TI4403', '102TI4405', '102TI4501', '102TI4503', '103TI2501', '103TIC2504', '104TI2001', '104TI2003', '104TI2101', '104TI2103', 'crude_TUL', 'crude_CEI', 'crude_IRC', 'crude_MEO', 'crude_Cieba', '100FQ4001', '100FQ4002', '100FQ4003', '101FI8101', '101FI8201', '101FI8301', '101FI8401', '101FI8801', '101FI8901', 'crude_WTI Midland', 'crude_MERO', '101FIC3802', '101FIC5801', '101FIC6702', 'MW']\n",
            "\n",
            "🚀 Starting Autoformer experiment...\n",
            "Use GPU: cuda:0\n",
            "📊 Start training...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "list.remove(x): x not in list",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-299914997.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExp_Main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"📊 Start training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'autoformer_custom_multivariate_targets'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Updated setting name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"💾 Training completed! Saving model to drive...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/exp/exp_main.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mvali_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvali_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/exp/exp_main.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/data_provider/data_factory.py\u001b[0m in \u001b[0;36mdata_provider\u001b[0;34m(args, flag)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     data_set = Data(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/data_provider/data_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_path, flag, size, features, data_path, target, scale, timeenc, freq)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__read_data__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__read_data__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Autoformer/Autoformer/data_provider/data_loader.py\u001b[0m in \u001b[0;36m__read_data__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# print(cols)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
          ]
        }
      ],
      "source": [
        "from exp.exp_main import Exp_Main\n",
        "import argparse\n",
        "import torch\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd # Import pandas to read the CSV and get column count\n",
        "import numpy as np # Import numpy for numeric check\n",
        "\n",
        "# First, let's check the dataset size to determine appropriate parameters\n",
        "print(\"🔍 Checking dataset size and determining optimal parameters...\")\n",
        "df_size_check = pd.read_csv('./dataset/custom/custom.csv')\n",
        "dataset_size = len(df_size_check)\n",
        "print(f\"Dataset size: {dataset_size} rows\")\n",
        "\n",
        "# Calculate optimal parameters based on dataset size\n",
        "# Rule of thumb: seq_len + label_len + pred_len should be < dataset_size\n",
        "# Leave some buffer for train/val/test split\n",
        "max_sequence_length = max(1, min(60, dataset_size // 4))  # Use 1/4 of dataset or max 60\n",
        "max_label_length = max(1, min(24, dataset_size // 8))     # Use 1/8 of dataset or max 24\n",
        "max_pred_length = max(1, min(7, dataset_size // 12))      # Use 1/12 of dataset or max 7\n",
        "\n",
        "# Ensure we have enough data for training\n",
        "min_required_samples = max_sequence_length + max_label_length + max_pred_length + 10  # +10 buffer\n",
        "if dataset_size < min_required_samples:\n",
        "    print(f\"⚠️  Dataset size ({dataset_size}) is too small for the calculated parameters.\")\n",
        "    print(f\"   Minimum required: {min_required_samples}\")\n",
        "    print(f\"   Adjusting parameters to fit available data...\")\n",
        "    \n",
        "    # Use more conservative parameters\n",
        "    max_sequence_length = max(1, dataset_size // 6)\n",
        "    max_label_length = max(1, dataset_size // 12)\n",
        "    max_pred_length = max(1, dataset_size // 20)\n",
        "    \n",
        "    print(f\"   Adjusted seq_len: {max_sequence_length}\")\n",
        "    print(f\"   Adjusted label_len: {max_label_length}\")\n",
        "    print(f\"   Adjusted pred_len: {max_pred_length}\")\n",
        "\n",
        "print(f\"✅ Using parameters:\")\n",
        "print(f\"   - seq_len: {max_sequence_length}\")\n",
        "print(f\"   - label_len: {max_label_length}\")\n",
        "print(f\"   - pred_len: {max_pred_length}\")\n",
        "print(f\"   - Total sequence requirement: {max_sequence_length + max_label_length + max_pred_length}\")\n",
        "\n",
        "# Arguments for Autoformer training and prediction\n",
        "args = argparse.Namespace(\n",
        "    is_training=1,\n",
        "    model_id='autoformer_custom_multivariate_targets', # Changed model_id for clarity\n",
        "    model='Autoformer',\n",
        "    data='custom',\n",
        "    root_path='./dataset/custom/',\n",
        "    data_path='custom.csv',\n",
        "    features='M', # Multivariate features\n",
        "    seq_len=max_sequence_length,         # Dynamically calculated based on dataset size\n",
        "    label_len=max_label_length,          # Dynamically calculated based on dataset size\n",
        "    pred_len=max_pred_length,           # Dynamically calculated based on dataset size\n",
        "    e_layers=1,\n",
        "    d_layers=1,\n",
        "    factor=3,\n",
        "    # enc_in, dec_in will be determined dynamically based on all columns except date and non-numeric\n",
        "    # c_out will be set to the number of target columns for multivariate output\n",
        "    d_model=64,\n",
        "    n_heads=8,\n",
        "    d_ff=128,\n",
        "    activation='gelu',\n",
        "    description='autoformer multivariate target forecasting', # Updated description\n",
        "    itr=1,\n",
        "    train_epochs=10,     # keep small for demo\n",
        "    batch_size=min(16, dataset_size // 10),  # Adjust batch size based on dataset size\n",
        "    learning_rate=0.001,\n",
        "    moving_avg=25,\n",
        "    freq='d',\n",
        "    dropout=0.3,\n",
        "    embed='timeF',\n",
        "    patience=3,\n",
        "    checkpoint='./checkpoints/',\n",
        "    output_attention=False,\n",
        "    do_predict=True,\n",
        "    # Add GPU arguments\n",
        "    use_gpu=torch.cuda.is_available(), # Check if GPU is available\n",
        "    gpu=0, # Specify GPU id, 0 by default\n",
        "    use_multi_gpu=False, # Set to True if using multiple GPUs\n",
        "    devices=[0], # Specify device ids if using multi-gpu\n",
        "    # Add the 'target' argument back, set to the first target column name.\n",
        "    # The library's data_provider might still need this, even for multivariate features.\n",
        "    target='Total_Stabilized_Naphtha_Product_Flowrate',\n",
        "    num_workers=0, # Add num_workers argument\n",
        "    # Add checkpoints attribute\n",
        "    checkpoints= './checkpoints/',\n",
        "    # Add use_amp attribute\n",
        "    use_amp=False,\n",
        "    # Add lradj attribute\n",
        "    lradj='type1'\n",
        ")\n",
        "\n",
        "# Determine input/output dimensions dynamically\n",
        "try:\n",
        "    # First, check if the file exists and is readable\n",
        "    file_path = os.path.join(args.root_path, args.data_path)\n",
        "    print(f\"🔍 Checking dataset file: {file_path}\")\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
        "    \n",
        "    # Check file size\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    print(f\"   - File size: {file_size} bytes\")\n",
        "    \n",
        "    if file_size == 0:\n",
        "        raise ValueError(f\"Dataset file is empty: {file_path}\")\n",
        "    \n",
        "    # Try to read the file\n",
        "    print(f\"   - Reading dataset...\")\n",
        "    df_temp = pd.read_csv(file_path)\n",
        "    print(f\"   - Successfully loaded dataset with {len(df_temp)} rows and {len(df_temp.columns)} columns\")\n",
        "\n",
        "    # Print all column names to debug\n",
        "    print(\"All columns in the dataset:\")\n",
        "    for i, col in enumerate(df_temp.columns):\n",
        "        print(f\"{i}: '{col}' (type: {df_temp[col].dtype})\")\n",
        "\n",
        "    # Verify that the dataset has the expected structure\n",
        "    if 'date' not in df_temp.columns:\n",
        "        raise ValueError(\"❌ Dataset must contain a 'date' column for Autoformer to work properly!\")\n",
        "    \n",
        "    print(f\"✅ Found 'date' column in dataset\")\n",
        "\n",
        "    # Identify target columns from the list used in the data preparation cell\n",
        "    target_cols_list = [\n",
        "        'Total_Stabilized_Naphtha_Product_Flowrate',\n",
        "        'Total_Kerosene_Product_Flowrate',\n",
        "        'Jet_Fuel_Product_Train1_Flowrate',\n",
        "        'Total_Light_Diesel_Product_Flowrate',\n",
        "        'Total_Heavy_Diesel_Product_Flowrate',\n",
        "        'Total_Atmospheric_Residue_Flowrate',\n",
        "        'Blend_Yield_Gas & LPG',\n",
        "        'Blend_Yield_Kerosene',\n",
        "        'Blend_Yield_Light Diesel',\n",
        "        'Blend_Yield_Heavy Diesel',\n",
        "        'Blend_Yield_RCO'\n",
        "    ]\n",
        "\n",
        "    # Check if all specified target columns exist in the DataFrame\n",
        "    missing_targets = [col for col in target_cols_list if col not in df_temp.columns]\n",
        "    if missing_targets:\n",
        "        print(f\"❌ The following specified target columns were not found in the dataset: {missing_targets}\")\n",
        "        print(\"Available columns that might be similar:\")\n",
        "        for missing_col in missing_targets:\n",
        "            similar_cols = [col for col in df_temp.columns if missing_col.lower().replace('_', '').replace(' ', '') in col.lower().replace('_', '').replace(' ', '')]\n",
        "            if similar_cols:\n",
        "                print(f\"  For '{missing_col}', similar columns found: {similar_cols}\")\n",
        "\n",
        "        # Use only the target columns that exist\n",
        "        existing_targets = [col for col in target_cols_list if col in df_temp.columns]\n",
        "        if not existing_targets:\n",
        "            raise ValueError(\"No target columns found in the dataset!\")\n",
        "        target_cols_list = existing_targets\n",
        "        print(f\"Using existing target columns: {target_cols_list}\")\n",
        "\n",
        "    # CRITICAL FIX: Convert all columns to numeric types (except date)\n",
        "    print(\"\\n🔧 Converting all columns to numeric types...\")\n",
        "    non_numeric_cols = []\n",
        "    for col in df_temp.columns:\n",
        "        if col != 'date':  # Skip date column\n",
        "            try:\n",
        "                # Convert to numeric, coercing errors to NaN\n",
        "                df_temp[col] = pd.to_numeric(df_temp[col], errors='coerce')\n",
        "            except Exception as e:\n",
        "                non_numeric_cols.append(col)\n",
        "                print(f\"⚠️  Could not convert column '{col}' to numeric: {e}\")\n",
        "\n",
        "    if non_numeric_cols:\n",
        "        print(f\"⚠️  Columns that could not be converted to numeric: {non_numeric_cols}\")\n",
        "        # Drop these columns if they can't be converted\n",
        "        df_temp = df_temp.drop(columns=non_numeric_cols)\n",
        "        print(f\"✅ Dropped {len(non_numeric_cols)} non-numeric columns\")\n",
        "\n",
        "    # Handle missing values after conversion\n",
        "    print(\"🔧 Handling missing values...\")\n",
        "    df_temp = df_temp.fillna(method='ffill').fillna(method='bfill')\n",
        "    \n",
        "    # Remove any rows that still have NaN values\n",
        "    initial_rows = len(df_temp)\n",
        "    df_temp = df_temp.dropna()\n",
        "    final_rows = len(df_temp)\n",
        "    if initial_rows != final_rows:\n",
        "        print(f\"⚠️  Removed {initial_rows - final_rows} rows with NaN values\")\n",
        "\n",
        "    # Verify that the primary target column exists and set it correctly\n",
        "    if args.target not in df_temp.columns:\n",
        "        print(f\"Warning: Primary target column '{args.target}' not found in dataset.\")\n",
        "        if target_cols_list:\n",
        "            args.target = target_cols_list[0]\n",
        "            print(f\"Setting primary target to: '{args.target}'\")\n",
        "        else:\n",
        "            # Find the first numeric column as fallback\n",
        "            numeric_cols = df_temp.select_dtypes(include=np.number).columns.tolist()\n",
        "            if 'date' in numeric_cols:\n",
        "                numeric_cols.remove('date')\n",
        "            if numeric_cols:\n",
        "                args.target = numeric_cols[0]\n",
        "                print(f\"Using first numeric column as target: '{args.target}'\")\n",
        "            else:\n",
        "                raise ValueError(\"No suitable target column found!\")\n",
        "\n",
        "    # Identify columns to be used as features and targets (excluding 'date' and non-numeric, non-target columns)\n",
        "    # Also ensure target columns are numeric\n",
        "    numeric_cols = df_temp.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Remove 'date' from numeric columns if it exists there\n",
        "    if 'date' in numeric_cols:\n",
        "        numeric_cols.remove('date')\n",
        "\n",
        "    # Create list of all relevant columns: date + all numeric columns\n",
        "    all_relevant_cols = ['date'] + numeric_cols\n",
        "\n",
        "    # Filter df_temp to only include relevant columns that exist\n",
        "    existing_cols = [col for col in all_relevant_cols if col in df_temp.columns]\n",
        "    df_temp_filtered = df_temp[existing_cols].copy()\n",
        "\n",
        "    # The number of features (enc_in, dec_in) is the total number of numeric columns\n",
        "    num_features = len(numeric_cols)\n",
        "\n",
        "    args.enc_in = num_features\n",
        "    args.dec_in = num_features # Decoder input size matches encoder input size for multivariate features\n",
        "\n",
        "    # CRITICAL FIX: Configure for multivariate target prediction\n",
        "    # For multivariate targets, c_out should be the number of target columns\n",
        "    num_target_columns = len(target_cols_list)\n",
        "    args.c_out = num_target_columns  # Set to number of target columns for multivariate output\n",
        "    \n",
        "    print(f\"🔧 Configuring for multivariate prediction:\")\n",
        "    print(f\"   - Number of target columns: {num_target_columns}\")\n",
        "    print(f\"   - Target columns: {target_cols_list}\")\n",
        "    print(f\"   - c_out (output dimension): {args.c_out}\")\n",
        "\n",
        "    print(f\"✅ Successfully configured:\")\n",
        "    print(f\"   - enc_in, dec_in: {num_features}\")\n",
        "    print(f\"   - c_out (output dimension): {args.c_out}\")\n",
        "    print(f\"   - Primary target column: '{args.target}'\")\n",
        "    print(f\"   - All numeric columns ({len(numeric_cols)}): {numeric_cols[:10]}... (showing first 10)\")\n",
        "    print(f\"   - Dataset shape: {df_temp_filtered.shape}\")\n",
        "    \n",
        "    # Verify data types are correct\n",
        "    print(f\"\\n🔍 Final data type verification:\")\n",
        "    print(f\"   - All columns are numeric: {all(df_temp[numeric_cols].dtypes.apply(lambda x: np.issubdtype(x, np.number)))}\")\n",
        "    print(f\"   - No NaN values: {not df_temp[numeric_cols].isnull().any().any()}\")\n",
        "    print(f\"   - Sample values from target column '{args.target}': {df_temp[args.target].head().tolist()}\")\n",
        "    \n",
        "    # CRITICAL: Validate that we have enough data for the sequence parameters\n",
        "    print(f\"\\n🔍 Validating sequence parameters against dataset size:\")\n",
        "    total_sequence_need = args.seq_len + args.label_len + args.pred_len\n",
        "    available_samples = len(df_temp_filtered)\n",
        "    \n",
        "    print(f\"   - Required total sequence length: {total_sequence_need}\")\n",
        "    print(f\"   - Available samples: {available_samples}\")\n",
        "    print(f\"   - Buffer remaining: {available_samples - total_sequence_need}\")\n",
        "    \n",
        "    # Debug: Check if df_temp_filtered is empty\n",
        "    if available_samples == 0:\n",
        "        print(f\"❌ CRITICAL ERROR: df_temp_filtered is empty!\")\n",
        "        print(f\"   - Original df_temp shape: {df_temp.shape}\")\n",
        "        print(f\"   - df_temp columns: {list(df_temp.columns)}\")\n",
        "        print(f\"   - df_temp_filtered shape: {df_temp_filtered.shape}\")\n",
        "        print(f\"   - existing_cols: {existing_cols}\")\n",
        "        print(f\"   - numeric_cols: {numeric_cols}\")\n",
        "        \n",
        "        # Try to identify the issue\n",
        "        if len(existing_cols) == 0:\n",
        "            print(f\"   - Issue: No existing columns found!\")\n",
        "        if len(numeric_cols) == 0:\n",
        "            print(f\"   - Issue: No numeric columns found!\")\n",
        "        \n",
        "        # Try to recover by using all columns except date\n",
        "        print(f\"   - Attempting recovery by using all columns except date...\")\n",
        "        all_cols_except_date = [col for col in df_temp.columns if col != 'date']\n",
        "        if len(all_cols_except_date) > 0:\n",
        "            df_temp_filtered = df_temp[['date'] + all_cols_except_date].copy()\n",
        "            available_samples = len(df_temp_filtered)\n",
        "            print(f\"   - Recovery successful: {available_samples} samples available\")\n",
        "        else:\n",
        "            raise ValueError(\"Cannot recover: No columns available for training!\")\n",
        "    \n",
        "    if available_samples < total_sequence_need:\n",
        "        print(f\"❌ ERROR: Not enough data for the specified sequence parameters!\")\n",
        "        print(f\"   Need at least {total_sequence_need} samples, but only have {available_samples}\")\n",
        "        print(f\"   Please reduce seq_len, label_len, or pred_len parameters\")\n",
        "        raise ValueError(f\"Insufficient data: need {total_sequence_need} samples, have {available_samples}\")\n",
        "    \n",
        "    # Additional validation for train/val/test split\n",
        "    # Autoformer typically uses 70% train, 20% val, 10% test\n",
        "    train_samples = int(available_samples * 0.7)\n",
        "    val_samples = int(available_samples * 0.2)\n",
        "    test_samples = available_samples - train_samples - val_samples\n",
        "    \n",
        "    print(f\"   - Expected train samples: {train_samples}\")\n",
        "    print(f\"   - Expected val samples: {val_samples}\")\n",
        "    print(f\"   - Expected test samples: {test_samples}\")\n",
        "    \n",
        "    # Ensure we have enough samples for training after splitting\n",
        "    min_train_samples = total_sequence_need + 10  # +10 buffer\n",
        "    if train_samples < min_train_samples:\n",
        "        print(f\"⚠️  WARNING: Training set may be too small after splitting!\")\n",
        "        print(f\"   Expected train samples: {train_samples}\")\n",
        "        print(f\"   Minimum recommended: {min_train_samples}\")\n",
        "        print(f\"   Consider reducing sequence parameters or using more data\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error determining input/output dimensions: {e}\")\n",
        "    # Fallback to default or raise error if necessary\n",
        "    args.enc_in = 1 # Fallback to 1 if unable to read file\n",
        "    args.dec_in = 1\n",
        "    args.c_out = 1\n",
        "    args.target = 'Unknown' # Set target to a placeholder\n",
        "    print(f\"Using default enc_in, dec_in, c_out: {args.enc_in}, {args.dec_in}, {args.c_out}\")\n",
        "    raise e  # Re-raise the error to stop execution\n",
        "\n",
        "\n",
        "def save_model_to_drive(exp, model_name=\"autoformer_model\", save_path=\"./saved_models/\"):\n",
        "    \"\"\"\n",
        "    Save the trained model and related information to drive\n",
        "\n",
        "    Args:\n",
        "        exp: Experiment object containing the trained model\n",
        "        model_name: Name for the saved model\n",
        "        save_path: Directory path to save the model\n",
        "    \"\"\"\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Create timestamped filename\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model_filename = f\"{model_name}_{timestamp}\"\n",
        "\n",
        "    # Save model state dict\n",
        "    model_path = os.path.join(save_path, f\"{model_filename}.pth\")\n",
        "    torch.save(exp.model.state_dict(), model_path)\n",
        "    print(f\"Model state dict saved to: {model_path}\")\n",
        "\n",
        "    # Save complete model (including architecture)\n",
        "    complete_model_path = os.path.join(save_path, f\"{model_filename}_complete.pth\")\n",
        "    torch.save(exp.model, complete_model_path)\n",
        "    print(f\"Complete model saved to: {complete_model_path}\")\n",
        "\n",
        "    # Save model configuration (args)\n",
        "    config_path = os.path.join(save_path, f\"{model_filename}_config.json\")\n",
        "    config_dict = vars(args)\n",
        "    # Convert non-serializable objects to strings\n",
        "    for key, value in config_dict.items():\n",
        "        if not isinstance(value, (int, float, str, bool, list, dict, type(None))):\n",
        "            config_dict[key] = str(value)\n",
        "\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config_dict, f, indent=4)\n",
        "    print(f\"Model configuration saved to: {config_path}\")\n",
        "\n",
        "    # Save experiment object (if needed for later use)\n",
        "    exp_path = os.path.join(save_path, f\"{model_filename}_experiment.pkl\")\n",
        "    try:\n",
        "        with open(exp_path, 'wb') as f:\n",
        "            pickle.dump(exp, f)\n",
        "        print(f\"Experiment object saved to: {exp_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save experiment object: {e}\")\n",
        "\n",
        "    # Create a summary file\n",
        "    summary_path = os.path.join(save_path, f\"{model_filename}_summary.txt\")\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(f\"Autoformer Model Summary\\n\")\n",
        "        f.write(f\"========================\\n\")\n",
        "        f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Model ID: {args.model_id}\\n\")\n",
        "        f.write(f\"Model Type: {args.model}\\n\")\n",
        "        f.write(f\"Training Epochs: {args.train_epochs}\\n\")\n",
        "        f.write(f\"Sequence Length: {args.seq_len}\\n\")\n",
        "        f.write(f\"Prediction Length: {args.pred_len}\\n\")\n",
        "        f.write(f\"Learning Rate: {args.learning_rate}\\n\")\n",
        "        f.write(f\"Batch Size: {args.batch_size}\\n\")\n",
        "        f.write(f\"GPU Used: {args.use_gpu}\\n\")\n",
        "        f.write(f\"Primary Target: {args.target}\\n\")\n",
        "        f.write(f\"Input Features: {args.enc_in}\\n\")\n",
        "        f.write(f\"Output Dimension: {args.c_out}\\n\")\n",
        "        f.write(f\"\\nFiles saved:\\n\")\n",
        "        f.write(f\"- Model state dict: {model_filename}.pth\\n\")\n",
        "        f.write(f\"- Complete model: {model_filename}_complete.pth\\n\")\n",
        "        f.write(f\"- Configuration: {model_filename}_config.json\\n\")\n",
        "        f.write(f\"- Summary: {model_filename}_summary.txt\\n\")\n",
        "\n",
        "    print(f\"Model summary saved to: {summary_path}\")\n",
        "    print(f\"\\nAll model files saved successfully in: {save_path}\")\n",
        "\n",
        "    return {\n",
        "        'model_path': model_path,\n",
        "        'complete_model_path': complete_model_path,\n",
        "        'config_path': config_path,\n",
        "        'summary_path': summary_path\n",
        "    }\n",
        "\n",
        "def load_saved_model(model_path, config_path, device=None):\n",
        "    \"\"\"\n",
        "    Load a previously saved model\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model state dict\n",
        "        config_path: Path to the saved configuration\n",
        "        device: Device to load the model on\n",
        "\n",
        "    Returns:\n",
        "        Loaded model and configuration\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load configuration\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Recreate args namespace\n",
        "    loaded_args = argparse.Namespace(**config)\n",
        "\n",
        "    # Create experiment object\n",
        "    exp = Exp_Main(loaded_args)\n",
        "\n",
        "    # Load model state dict\n",
        "    model_state = torch.load(model_path, map_location=device)\n",
        "    exp.model.load_state_dict(model_state)\n",
        "    exp.model.eval()\n",
        "\n",
        "    print(f\"Model loaded successfully from: {model_path}\")\n",
        "    return exp, loaded_args\n",
        "\n",
        "# Verify the target column exists before proceeding\n",
        "try:\n",
        "    print(\"🔍 Verifying data before training...\")\n",
        "    df_verify = pd.read_csv(os.path.join(args.root_path, args.data_path))\n",
        "    if args.target not in df_verify.columns:\n",
        "        raise ValueError(f\"Target column '{args.target}' not found in the dataset columns: {list(df_verify.columns)}\")\n",
        "    print(f\"✅ Target column '{args.target}' verified in dataset\")\n",
        "\n",
        "    # Check if there's a 'date' column\n",
        "    if 'date' not in df_verify.columns:\n",
        "        print(\"⚠️  Warning: No 'date' column found. Available columns:\")\n",
        "        print(list(df_verify.columns))\n",
        "    else:\n",
        "        print(\"✅ Date column found in dataset\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error verifying data: {e}\")\n",
        "    raise e\n",
        "\n",
        "# Run training and prediction\n",
        "print(\"\\n🚀 Starting Autoformer experiment...\")\n",
        "exp = Exp_Main(args)\n",
        "print(\"📊 Start training...\")\n",
        "exp.train(setting='autoformer_custom_multivariate_targets') # Updated setting name\n",
        "\n",
        "print(\"💾 Training completed! Saving model to drive...\")\n",
        "saved_files = save_model_to_drive(exp, model_name=\"autoformer_custom_multivariate_targets\", save_path=\"./saved_models/\") # Updated model name\n",
        "\n",
        "print(\"🔮 Start predicting...\")\n",
        "# Modify the predict call if needed based on Autoformer's prediction output\n",
        "exp.predict(setting='autoformer_custom_multivariate_targets', load=True) # Updated setting name\n",
        "\n",
        "print(\"🎉 Training and prediction completed!\")\n",
        "print(f\"📁 Model saved to: {saved_files['model_path']}\")\n",
        "\n",
        "# Example of how to load the model later:\n",
        "# exp_loaded, args_loaded = load_saved_model(\n",
        "#     saved_files['model_path'],\n",
        "#     saved_files['config_path']\n",
        "# )\n",
        "# print(\"Model loaded successfully for future use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOx51FBrQr2i"
      },
      "source": [
        "MODEL EVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYECWUhbQugW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def get_test_predictions_manual(exp, args):\n",
        "    \"\"\"\n",
        "    Manual method to get test predictions when data_provider is not available\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a simple data loader for the test data\n",
        "        import pandas as pd\n",
        "        from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv(os.path.join(args.root_path, args.data_path))\n",
        "\n",
        "        # Simple train/test split (you may need to adjust this based on your data)\n",
        "        train_size = int(len(df) * 0.8)\n",
        "        test_df = df[train_size:]\n",
        "\n",
        "        print(f\"✓ Manual data loading: {len(test_df)} test samples\")\n",
        "\n",
        "        # Simple dataset class\n",
        "        class SimpleDataset(Dataset):\n",
        "            def __init__(self, data, seq_len, pred_len):\n",
        "                self.data = data.values\n",
        "                self.seq_len = seq_len\n",
        "                self.pred_len = pred_len\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.data) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                seq_x = self.data[idx:idx + self.seq_len]\n",
        "                seq_y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len]\n",
        "                return torch.FloatTensor(seq_x), torch.FloatTensor(seq_y)\n",
        "\n",
        "        # Create dataset and dataloader\n",
        "        test_dataset = SimpleDataset(test_df, args.seq_len, args.pred_len)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = []\n",
        "        true_values = []\n",
        "\n",
        "        exp.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in test_loader:\n",
        "                if args.use_gpu:\n",
        "                    batch_x = batch_x.cuda()\n",
        "                    batch_y = batch_y.cuda()\n",
        "\n",
        "                # Simple prediction (you may need to adjust based on your model)\n",
        "                pred = exp.model(batch_x)\n",
        "\n",
        "                predictions.append(pred.cpu().numpy())\n",
        "                true_values.append(batch_y.cpu().numpy())\n",
        "\n",
        "        if predictions and true_values:\n",
        "            y_pred = np.concatenate(predictions, axis=0)\n",
        "            y_true = np.concatenate(true_values, axis=0)\n",
        "            return y_pred, y_true\n",
        "        else:\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Manual prediction method failed: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive evaluation metrics\n",
        "\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays if they're tensors\n",
        "    if torch.is_tensor(y_true):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "    if torch.is_tensor(y_pred):\n",
        "        y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "    # Flatten arrays if multidimensional\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "    # Remove any NaN values\n",
        "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
        "    y_true = y_true[mask]\n",
        "    y_pred = y_pred[mask]\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    # MAPE (Mean Absolute Percentage Error)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
        "\n",
        "    # MSPE (Mean Squared Percentage Error)\n",
        "    mspe = np.mean(((y_true - y_pred) / (y_true + 1e-8)) ** 2) * 100\n",
        "\n",
        "    # R² Score\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Directional Accuracy (for time series)\n",
        "    if len(y_true) > 1:\n",
        "        true_direction = np.sign(np.diff(y_true))\n",
        "        pred_direction = np.sign(np.diff(y_pred))\n",
        "        directional_accuracy = np.mean(true_direction == pred_direction) * 100\n",
        "    else:\n",
        "        directional_accuracy = 0\n",
        "\n",
        "    return {\n",
        "        'MSE': float(mse),\n",
        "        'RMSE': float(rmse),\n",
        "        'MAE': float(mae),\n",
        "        'MAPE': float(mape),\n",
        "        'MSPE': float(mspe),\n",
        "        'R2': float(r2),\n",
        "        'Directional_Accuracy': float(directional_accuracy)\n",
        "    }\n",
        "\n",
        "def create_evaluation_plots(y_true, y_pred, save_path=\"./evaluation_plots/\"):\n",
        "    \"\"\"\n",
        "    Create comprehensive evaluation plots\n",
        "\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "        save_path: Directory to save plots\n",
        "    \"\"\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Convert to numpy arrays if they're tensors\n",
        "    if torch.is_tensor(y_true):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "    if torch.is_tensor(y_pred):\n",
        "        y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "    # Flatten arrays if multidimensional\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Model Evaluation Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Actual vs Predicted (Time Series)\n",
        "    axes[0, 0].plot(y_true, label='Actual', color='blue', alpha=0.7)\n",
        "    axes[0, 0].plot(y_pred, label='Predicted', color='red', alpha=0.7)\n",
        "    axes[0, 0].set_title('Actual vs Predicted Values (Time Series)')\n",
        "    axes[0, 0].set_xlabel('Time Steps')\n",
        "    axes[0, 0].set_ylabel('Values')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Scatter Plot\n",
        "    axes[0, 1].scatter(y_true, y_pred, alpha=0.6, color='green')\n",
        "    axes[0, 1].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],\n",
        "                    'r--', lw=2, label='Perfect Prediction')\n",
        "    axes[0, 1].set_title('Scatter Plot: Actual vs Predicted')\n",
        "    axes[0, 1].set_xlabel('Actual Values')\n",
        "    axes[0, 1].set_ylabel('Predicted Values')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Residuals\n",
        "    residuals = y_true - y_pred\n",
        "    axes[1, 0].scatter(y_pred, residuals, alpha=0.6, color='orange')\n",
        "    axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[1, 0].set_title('Residual Plot')\n",
        "    axes[1, 0].set_xlabel('Predicted Values')\n",
        "    axes[1, 0].set_ylabel('Residuals')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Residuals Distribution\n",
        "    axes[1, 1].hist(residuals, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "    axes[1, 1].set_title('Distribution of Residuals')\n",
        "    axes[1, 1].set_xlabel('Residuals')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    plot_filename = os.path.join(save_path, f\"evaluation_plots_{timestamp}.png\")\n",
        "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Evaluation plots saved to: {plot_filename}\")\n",
        "    return plot_filename\n",
        "\n",
        "def comprehensive_model_evaluation(exp, args, save_results=True):\n",
        "    \"\"\"\n",
        "    Perform comprehensive evaluation of the trained model\n",
        "\n",
        "    Args:\n",
        "        exp: Experiment object\n",
        "        args: Arguments namespace\n",
        "        save_results: Whether to save results to files\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing evaluation results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    exp.model.eval()\n",
        "\n",
        "    # Set is_training to 0 for evaluation\n",
        "    args.is_training = 0\n",
        "\n",
        "    # Create evaluation results directory\n",
        "    eval_save_path = './evaluation_results/'\n",
        "    os.makedirs(eval_save_path, exist_ok=True)\n",
        "\n",
        "    # Initialize results dictionary\n",
        "    eval_results = {\n",
        "        'model_id': args.model_id,\n",
        "        'model_type': args.model,\n",
        "        'setting': 'autoformer_custom_autoformer_custom',\n",
        "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
        "        'evaluation_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "    print(\"1. Running test method for standard evaluation...\")\n",
        "    # Run the standard test method\n",
        "    try:\n",
        "        exp.test(setting='autoformer_custom_autoformer_custom')\n",
        "        print(\"✓ Standard test method completed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Warning: Standard test method failed: {e}\")\n",
        "\n",
        "    print(\"\\n2. Performing detailed evaluation...\")\n",
        "\n",
        "    # Get predictions and true values for detailed analysis\n",
        "    try:\n",
        "        # Method 1: Use the built-in data provider from Exp_Main\n",
        "        from data_provider.data_factory import data_provider\n",
        "\n",
        "        # Get test data loader\n",
        "        test_data, test_loader = data_provider(args, 'test')\n",
        "\n",
        "        print(f\"✓ Test data loaded successfully. Dataset size: {len(test_data)}\")\n",
        "\n",
        "        # Get predictions using the test data loader\n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            true_values = []\n",
        "\n",
        "            exp.model.eval()\n",
        "\n",
        "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
        "                # Move to device if GPU is available\n",
        "                if args.use_gpu:\n",
        "                    batch_x = batch_x.float().cuda()\n",
        "                    batch_y = batch_y.float().cuda()\n",
        "                    batch_x_mark = batch_x_mark.float().cuda()\n",
        "                    batch_y_mark = batch_y_mark.float().cuda()\n",
        "                else:\n",
        "                    batch_x = batch_x.float()\n",
        "                    batch_y = batch_y.float()\n",
        "                    batch_x_mark = batch_x_mark.float()\n",
        "                    batch_y_mark = batch_y_mark.float()\n",
        "\n",
        "                # Decoder input\n",
        "                dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
        "                dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()\n",
        "\n",
        "                if args.use_gpu:\n",
        "                    dec_inp = dec_inp.cuda()\n",
        "\n",
        "                # Get prediction\n",
        "                if args.output_attention:\n",
        "                    outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                else:\n",
        "                    outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "\n",
        "                # Extract the prediction part (last pred_len time steps)\n",
        "                pred = outputs[:, -args.pred_len:, :]\n",
        "                true = batch_y[:, -args.pred_len:, :]\n",
        "\n",
        "                predictions.append(pred.cpu().numpy())\n",
        "                true_values.append(true.cpu().numpy())\n",
        "\n",
        "                # Print progress for large datasets\n",
        "                if i > 0 and i % 50 == 0:\n",
        "                    print(f\"   Processed {i}/{len(test_loader)} batches...\")\n",
        "\n",
        "            if predictions and true_values:\n",
        "                y_pred = np.concatenate(predictions, axis=0)\n",
        "                y_true = np.concatenate(true_values, axis=0)\n",
        "\n",
        "                print(f\"✓ Predictions generated. Shape: {y_pred.shape}\")\n",
        "\n",
        "                # Calculate detailed metrics\n",
        "                metrics = calculate_metrics(y_true, y_pred)\n",
        "                eval_results.update(metrics)\n",
        "\n",
        "                # Print metrics\n",
        "                print(\"\\n3. Detailed Evaluation Metrics:\")\n",
        "                print(\"-\" * 40)\n",
        "                for metric, value in metrics.items():\n",
        "                    print(f\"{metric:<20}: {value:.6f}\")\n",
        "\n",
        "                # Create plots\n",
        "                print(\"\\n4. Creating evaluation plots...\")\n",
        "                plot_filename = create_evaluation_plots(y_true, y_pred)\n",
        "                eval_results['plot_filename'] = plot_filename\n",
        "\n",
        "                print(\"✓ Detailed evaluation completed successfully\")\n",
        "\n",
        "                # Save predictions for further analysis\n",
        "                pred_save_path = './evaluation_results/'\n",
        "                os.makedirs(pred_save_path, exist_ok=True)\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "                # Save predictions as numpy arrays\n",
        "                np.save(os.path.join(pred_save_path, f\"predictions_{timestamp}.npy\"), y_pred)\n",
        "                np.save(os.path.join(pred_save_path, f\"true_values_{timestamp}.npy\"), y_true)\n",
        "\n",
        "                print(f\"✓ Predictions saved to: {pred_save_path}\")\n",
        "\n",
        "            else:\n",
        "                print(\"⚠ Warning: Could not extract predictions for detailed analysis\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"⚠ Warning: Could not import data_provider: {e}\")\n",
        "        print(\"Trying alternative method...\")\n",
        "\n",
        "        # Method 2: Alternative approach using exp object's data loading\n",
        "        try:\n",
        "            # Set up data loader manually\n",
        "            args.is_training = 0  # Ensure we're in test mode\n",
        "\n",
        "            # Try to recreate the experiment for testing\n",
        "            from exp.exp_main import Exp_Main\n",
        "            exp_test = Exp_Main(args)\n",
        "\n",
        "            # Get test predictions using the vali method (which is often used for testing)\n",
        "            print(\"Using validation method for evaluation...\")\n",
        "\n",
        "            # The vali method in Exp_Main typically returns metrics\n",
        "            # We'll capture the model's predictions during validation\n",
        "            vali_results = exp_test.vali(test_loader=None, criterion=None)\n",
        "\n",
        "            print(\"✓ Validation-based evaluation completed\")\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"⚠ Warning: Alternative evaluation method failed: {e2}\")\n",
        "            print(\"Trying manual prediction method...\")\n",
        "\n",
        "            # Method 3: Manual prediction method\n",
        "            y_pred, y_true = get_test_predictions_manual(exp, args)\n",
        "\n",
        "            if y_pred is not None and y_true is not None:\n",
        "                print(\"✓ Manual prediction method succeeded\")\n",
        "\n",
        "                # Calculate detailed metrics\n",
        "                metrics = calculate_metrics(y_true, y_pred)\n",
        "                eval_results.update(metrics)\n",
        "\n",
        "                # Print metrics\n",
        "                print(\"\\n3. Detailed Evaluation Metrics:\")\n",
        "                print(\"-\" * 40)\n",
        "                for metric, value in metrics.items():\n",
        "                    print(f\"{metric:<20}: {value:.6f}\")\n",
        "\n",
        "                # Create plots\n",
        "                print(\"\\n4. Creating evaluation plots...\")\n",
        "                plot_filename = create_evaluation_plots(y_true, y_pred)\n",
        "                eval_results['plot_filename'] = plot_filename\n",
        "\n",
        "                print(\"✓ Detailed evaluation completed successfully\")\n",
        "            else:\n",
        "                print(\"⚠ All prediction methods failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Warning: Detailed evaluation failed: {e}\")\n",
        "        print(\"Please ensure your data_provider module is properly configured.\")\n",
        "\n",
        "        # Add basic model information even if detailed evaluation fails\n",
        "        eval_results.update({\n",
        "            'MSE': 'N/A',\n",
        "            'RMSE': 'N/A',\n",
        "            'MAE': 'N/A',\n",
        "            'MAPE': 'N/A',\n",
        "            'MSPE': 'N/A',\n",
        "            'R2': 'N/A',\n",
        "            'Directional_Accuracy': 'N/A'\n",
        "        })\n",
        "\n",
        "    # Add model configuration to results\n",
        "    eval_results['model_config'] = {\n",
        "        'seq_len': args.seq_len,\n",
        "        'pred_len': args.pred_len,\n",
        "        'train_epochs': args.train_epochs,\n",
        "        'batch_size': args.batch_size,\n",
        "        'learning_rate': args.learning_rate,\n",
        "        'model_layers': f\"e_layers: {args.e_layers}, d_layers: {args.d_layers}\",\n",
        "        'model_dim': args.d_model,\n",
        "        'n_heads': args.n_heads\n",
        "    }\n",
        "\n",
        "    # Save results if requested\n",
        "    if save_results:\n",
        "        print(\"\\n5. Saving evaluation results...\")\n",
        "        eval_filename = os.path.join(eval_save_path, f\"eval_results_{eval_results['timestamp']}.json\")\n",
        "\n",
        "        with open(eval_filename, 'w') as f:\n",
        "            json.dump(eval_results, f, indent=4)\n",
        "\n",
        "        print(f\"✓ Evaluation results saved to: {eval_filename}\")\n",
        "\n",
        "        # Create summary report\n",
        "        summary_filename = os.path.join(eval_save_path, f\"eval_summary_{eval_results['timestamp']}.txt\")\n",
        "        with open(summary_filename, 'w') as f:\n",
        "            f.write(\"AUTOFORMER MODEL EVALUATION SUMMARY\\n\")\n",
        "            f.write(\"=\"*50 + \"\\n\")\n",
        "            f.write(f\"Evaluation Date: {eval_results['evaluation_date']}\\n\")\n",
        "            f.write(f\"Model ID: {eval_results['model_id']}\\n\")\n",
        "            f.write(f\"Model Type: {eval_results['model_type']}\\n\")\n",
        "            f.write(f\"Setting: {eval_results['setting']}\\n\\n\")\n",
        "\n",
        "            f.write(\"MODEL CONFIGURATION:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            for key, value in eval_results['model_config'].items():\n",
        "                f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "            f.write(\"\\nEVALUATION METRICS:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            metrics = ['MSE', 'RMSE', 'MAE', 'MAPE', 'MSPE', 'R2', 'Directional_Accuracy']\n",
        "            for metric in metrics:\n",
        "                if metric in eval_results:\n",
        "                    f.write(f\"{metric}: {eval_results[metric]}\\n\")\n",
        "\n",
        "        print(f\"✓ Evaluation summary saved to: {summary_filename}\")\n",
        "        eval_results['summary_filename'] = summary_filename\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EVALUATION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "# MAIN EVALUATION EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    # This assumes the 'exp' and 'args' objects are available from the training script\n",
        "    # If running separately, you would need to load the model first:\n",
        "    # exp, args = load_saved_model(model_path, config_path)\n",
        "\n",
        "    print(\"\\nStarting comprehensive model evaluation...\")\n",
        "\n",
        "    # Perform comprehensive evaluation\n",
        "    evaluation_results = comprehensive_model_evaluation(exp, args, save_results=True)\n",
        "\n",
        "    # Print final summary\n",
        "    print(\"\\nFINAL EVALUATION SUMMARY:\")\n",
        "    print(\"-\" * 30)\n",
        "    if 'MSE' in evaluation_results and evaluation_results['MSE'] != 'N/A':\n",
        "        print(f\"Mean Squared Error (MSE): {evaluation_results['MSE']:.6f}\")\n",
        "        print(f\"Root Mean Squared Error (RMSE): {evaluation_results['RMSE']:.6f}\")\n",
        "        print(f\"Mean Absolute Error (MAE): {evaluation_results['MAE']:.6f}\")\n",
        "        print(f\"Mean Absolute Percentage Error (MAPE): {evaluation_results['MAPE']:.2f}%\")\n",
        "        print(f\"R² Score: {evaluation_results['R2']:.6f}\")\n",
        "\n",
        "    print(f\"\\nAll evaluation files saved in: ./evaluation_results/\")\n",
        "    print(\"Evaluation completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2cdaffd"
      },
      "outputs": [],
      "source": [
        "# Fix the numpy.Inf deprecation error in utils/tools.py\n",
        "!sed -i 's/np.Inf/np.inf/g' ./utils/tools.py\n",
        "\n",
        "print(\"Fixed np.Inf in utils/tools.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40d46c8d"
      },
      "source": [
        "## 🔍 Post-Hoc Interpretation with Explainable Boosting Machine (EBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1ZZhqmqS9BL"
      },
      "source": [
        "Expalainable Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da2da9a6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Prepare Static Covariates and Forecast Averages\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming original dataset is loaded as 'df' and forecast saved to 'forecast_df'\n",
        "static_columns= [col for col in df.columns if col.startswith('static_') or col.startswith('')]  # Replace with your static features\n",
        "target_column = \"RCO_Yield\"  # Example target; modify as needed\n",
        "\n",
        "# Extract static features (latest available per batch)\n",
        "df['crude_batch_id'] = df['future_Blend_Num'].astype(str)\n",
        "latest_static = df.groupby(\"crude_batch_id\").last()\n",
        "static_covariates = latest_static[static_columns]\n",
        "\n",
        "# Compute average predictions per batch\n",
        "forecast_df['crude_batch_id'] = forecast_df['future_Blend_Num'].astype(str)\n",
        "target_avg = forecast_df.groupby(\"crude_batch_id\")[[target_column]].mean()\n",
        "\n",
        "# Merge into EBM dataset\n",
        "ebm_data = static_covariates.join(target_avg, how=\"inner\").dropna()\n",
        "X = ebm_data[static_columns]\n",
        "y = ebm_data[target_column]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkmbNzkHTGhu"
      },
      "source": [
        "EBM Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ed0076c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 2: Train the EBM Model\n",
        "from interpret.glassbox import ExplainableBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ebm = ExplainableBoostingRegressor(interactions=10)\n",
        "ebm.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wABGYImSTOFb"
      },
      "source": [
        "Global Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6d23ebf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 3: Interpret Feature Importance and Sample Explanation\n",
        "from interpret import show\n",
        "\n",
        "# Global explanation\n",
        "ebm_global = ebm.explain_global()\n",
        "show(ebm_global)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahJnSAYTT-E"
      },
      "source": [
        "Local Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3c24d8e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optional: Local explanation for one prediction\n",
        "sample = X_test.iloc[[0]]\n",
        "ebm_local = ebm.explain_local(sample, y_test.iloc[[0]])\n",
        "show(ebm_local)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSrXQdVpTXOz"
      },
      "source": [
        "Feature Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "647a8f73"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optional: Bar Plot of Feature Importances\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(X.columns, ebm.feature_importances_)\n",
        "plt.title(\"EBM Feature Importances on RCO_Yield\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a021ab6"
      },
      "outputs": [],
      "source": [
        "print(\"Columns available in the df DataFrame:\")\n",
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff5667ab"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/processed/final_concatenated_data_mice_imputed.csv'\n",
        "\n",
        "try:\n",
        "    df_columns = pd.read_csv(file_path, nrows=0).columns.tolist()\n",
        "    print(\"Column names in the file:\")\n",
        "    for col in df_columns:\n",
        "        print(col)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
