{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hugging Face Autoformer with EBM Explanation for Multivariate Forecasting\n",
        "\n",
        "This notebook implements:\n",
        "1. Data loading and preprocessing for concatenated interpolated data\n",
        "2. Hugging Face Autoformer for multivariate time series forecasting\n",
        "3. Model training and evaluation\n",
        "4. EBM (Explainable Boosting Machine) for blend effects explanation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets evaluate accelerate\n",
        "!pip install interpret pandas numpy scikit-learn matplotlib seaborn\n",
        "!pip install torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoformerConfig, AutoformerForPrediction, Trainer, TrainingArguments\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from interpret.glassbox import ExplainableBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All packages imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the concatenated interpolated data\n",
        "file_path = 'final_concatenated_data_mice_imputed.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Define target columns (multivariate)\n",
        "target_cols = [\n",
        "    'Total_Stabilized_Naphtha_Product_Flowrate',\n",
        "    'Total_Kerosene_Product_Flowrate', \n",
        "    'Jet_Fuel_Product_Train1_Flowrate',\n",
        "    'Total_Light_Diesel_Product_Flowrate',\n",
        "    'Total_Heavy_Diesel_Product_Flowrate',\n",
        "    'Total_Atmospheric_Residue_Flowrate',\n",
        "    'Blend_Yield_Gas & LPG',\n",
        "    'Blend_Yield_Kerosene',\n",
        "    'Blend_Yield_Light Diesel',\n",
        "    'Blend_Yield_Heavy Diesel',\n",
        "    'Blend_Yield_RCO'\n",
        "]\n",
        "\n",
        "# Define static features (blend characteristics)\n",
        "static_cols = [col for col in df.columns if col.startswith('crude_') or col in ['API', 'Sulphur', 'blend_id']]\n",
        "\n",
        "# Define time series features (excluding targets and statics)\n",
        "ts_cols = [col for col in df.columns if col not in ['date'] + target_cols + static_cols]\n",
        "\n",
        "print(f\"Target columns: {len(target_cols)}\")\n",
        "print(f\"Static columns: {len(static_cols)}\")\n",
        "print(f\"Time series columns: {len(ts_cols)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_data(df, target_cols, ts_cols, static_cols):\n",
        "    # Convert date column\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Convert all columns to numeric\n",
        "    for col in target_cols + ts_cols + static_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    \n",
        "    # Handle missing values\n",
        "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "    \n",
        "    # Remove any remaining NaN rows\n",
        "    df = df.dropna()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Preprocess the data\n",
        "df_processed = preprocess_data(df, target_cols, ts_cols, static_cols)\n",
        "print(f\"Processed dataset shape: {df_processed.shape}\")\n",
        "print(f\"Date range: {df_processed['date'].min()} to {df_processed['date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Dataset Class for Time Series\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, target_cols, ts_cols, static_cols, seq_len=60, pred_len=7):\n",
        "        self.data = data\n",
        "        self.target_cols = target_cols\n",
        "        self.ts_cols = ts_cols\n",
        "        self.static_cols = static_cols\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        # Prepare features and targets\n",
        "        self.features = data[ts_cols].values\n",
        "        self.targets = data[target_cols].values\n",
        "        self.statics = data[static_cols].values\n",
        "        \n",
        "        # Normalize features\n",
        "        self.feature_scaler = StandardScaler()\n",
        "        self.target_scaler = StandardScaler()\n",
        "        \n",
        "        self.features_scaled = self.feature_scaler.fit_transform(self.features)\n",
        "        self.targets_scaled = self.target_scaler.fit_transform(self.targets)\n",
        "        \n",
        "        # Create sequences\n",
        "        self.sequences = []\n",
        "        self.labels = []\n",
        "        self.static_features = []\n",
        "        \n",
        "        for i in range(len(self.features_scaled) - seq_len - pred_len + 1):\n",
        "            seq = self.features_scaled[i:i+seq_len]\n",
        "            label = self.targets_scaled[i+seq_len:i+seq_len+pred_len]\n",
        "            static = self.statics[i+seq_len]  # Use static features at prediction time\n",
        "            \n",
        "            self.sequences.append(seq)\n",
        "            self.labels.append(label)\n",
        "            self.static_features.append(static)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'past_values': torch.FloatTensor(self.sequences[idx]),\n",
        "            'past_time_features': torch.zeros(self.seq_len, 1),  # Dummy time features\n",
        "            'static_categorical_features': torch.zeros(len(self.static_cols)),  # Dummy categorical\n",
        "            'static_real_features': torch.FloatTensor(self.static_features[idx]),\n",
        "            'future_values': torch.FloatTensor(self.labels[idx]),\n",
        "            'future_time_features': torch.zeros(self.pred_len, 1)  # Dummy time features\n",
        "        }\n",
        "    \n",
        "    def inverse_transform_targets(self, scaled_targets):\n",
        "        return self.target_scaler.inverse_transform(scaled_targets)\n",
        "\n",
        "print(\"‚úÖ TimeSeriesDataset class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Configuration and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "dataset = TimeSeriesDataset(df_processed, target_cols, ts_cols, static_cols, seq_len=60, pred_len=7)\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(dataset, range(train_size))\n",
        "val_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))\n",
        "test_dataset = torch.utils.data.Subset(dataset, range(train_size + val_size, len(dataset)))\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Autoformer\n",
        "config = AutoformerConfig(\n",
        "    context_length=60,\n",
        "    prediction_length=7,\n",
        "    num_time_features=1,\n",
        "    num_static_categorical_features=0,\n",
        "    num_static_real_features=len(static_cols),\n",
        "    cardinality=[],\n",
        "    embedding_dimension=[],\n",
        "    d_model=64,\n",
        "    encoder_attention_heads=8,\n",
        "    decoder_attention_heads=8,\n",
        "    encoder_layers=2,\n",
        "    decoder_layers=2,\n",
        "    encoder_ffn_dim=128,\n",
        "    decoder_ffn_dim=128,\n",
        "    activation_function=\"gelu\",\n",
        "    dropout=0.1,\n",
        "    encoder_layerdrop=0.1,\n",
        "    decoder_layerdrop=0.1,\n",
        "    use_cache=True,\n",
        "    num_parallel_samples=100,\n",
        "    init_std=0.02,\n",
        "    num_patches=16,\n",
        "    patch_size=\"auto\",\n",
        "    patch_stride=\"auto\",\n",
        "    num_default_real_val_embeddings=1,\n",
        "    scaling=\"std\",\n",
        "    embedding_dimension_multiplier=2.0,\n",
        "    distr_output=\"student_t\",\n",
        "    loss=\"nll\",\n",
        "    input_size=len(ts_cols),\n",
        "    num_targets=len(target_cols),\n",
        "    output_size=len(target_cols),\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Autoformer configuration created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model and training setup\n",
        "model = AutoformerForPrediction(config)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./autoformer_results\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Custom compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    \n",
        "    # Calculate metrics for each target\n",
        "    mse = mean_squared_error(labels.flatten(), predictions.flatten())\n",
        "    mae = mean_absolute_error(labels.flatten(), predictions.flatten())\n",
        "    r2 = r2_score(labels.flatten(), predictions.flatten())\n",
        "    \n",
        "    return {\n",
        "        \"mse\": mse,\n",
        "        \"mae\": mae,\n",
        "        \"r2\": r2,\n",
        "        \"rmse\": np.sqrt(mse)\n",
        "    }\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"üöÄ Starting training...\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Test Results:\")\n",
        "for key, value in test_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Generate predictions for visualization\n",
        "def generate_predictions(model, dataset, num_samples=100):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(min(num_samples, len(dataset))):\n",
        "            sample = dataset[i]\n",
        "            \n",
        "            # Prepare input\n",
        "            inputs = {\n",
        "                'past_values': sample['past_values'].unsqueeze(0),\n",
        "                'past_time_features': sample['past_time_features'].unsqueeze(0),\n",
        "                'static_categorical_features': sample['static_categorical_features'].unsqueeze(0),\n",
        "                'static_real_features': sample['static_real_features'].unsqueeze(0),\n",
        "                'future_time_features': sample['future_time_features'].unsqueeze(0)\n",
        "            }\n",
        "            \n",
        "            # Generate prediction\n",
        "            outputs = model(**inputs)\n",
        "            prediction = outputs.prediction_outputs\n",
        "            \n",
        "            predictions.append(prediction.squeeze().numpy())\n",
        "            actuals.append(sample['future_values'].numpy())\n",
        "    \n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "# Generate predictions\n",
        "predictions, actuals = generate_predictions(model, test_dataset, num_samples=50)\n",
        "print(f\"Generated {len(predictions)} predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actuals\n",
        "def plot_predictions(predictions, actuals, target_cols, num_targets=4):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i in range(min(num_targets, len(target_cols))):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Plot first prediction horizon\n",
        "        pred_values = predictions[:, 0, i]  # First prediction step\n",
        "        actual_values = actuals[:, 0, i]    # First actual step\n",
        "        \n",
        "        ax.scatter(actual_values, pred_values, alpha=0.6)\n",
        "        ax.plot([actual_values.min(), actual_values.max()], \n",
        "                [actual_values.min(), actual_values.max()], 'r--', lw=2)\n",
        "        ax.set_xlabel('Actual')\n",
        "        ax.set_ylabel('Predicted')\n",
        "        ax.set_title(f'{target_cols[i]}')\n",
        "        \n",
        "        # Calculate R¬≤\n",
        "        r2 = r2_score(actual_values, pred_values)\n",
        "        ax.text(0.05, 0.95, f'R¬≤ = {r2:.3f}', transform=ax.transAxes, \n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_predictions(predictions, actuals, target_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. EBM Model for Blend Effects Explanation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for EBM\n",
        "def prepare_ebm_data(dataset, static_cols, target_cols):\n",
        "    X_static = []\n",
        "    y_targets = []\n",
        "    \n",
        "    for i in range(len(dataset)):\n",
        "        sample = dataset[i]\n",
        "        \n",
        "        # Static features (blend characteristics)\n",
        "        static_features = sample['static_real_features'].numpy()\n",
        "        X_static.append(static_features)\n",
        "        \n",
        "        # Target values (average across prediction horizon)\n",
        "        target_values = sample['future_values'].numpy().mean(axis=0)  # Average across time steps\n",
        "        y_targets.append(target_values)\n",
        "    \n",
        "    return np.array(X_static), np.array(y_targets)\n",
        "\n",
        "# Prepare EBM data\n",
        "X_static, y_targets = prepare_ebm_data(dataset, static_cols, target_cols)\n",
        "print(f\"EBM data shape: X={X_static.shape}, y={y_targets.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train EBM models for each target\n",
        "def train_ebm_models(X_static, y_targets, target_cols, test_size=0.2):\n",
        "    ebm_models = {}\n",
        "    ebm_results = {}\n",
        "    \n",
        "    for i, target_name in enumerate(target_cols):\n",
        "        print(f\"Training EBM for {target_name}...\")\n",
        "        \n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_static, y_targets[:, i], test_size=test_size, random_state=42\n",
        "        )\n",
        "        \n",
        "        # Train EBM\n",
        "        ebm = ExplainableBoostingRegressor(\n",
        "            interactions=10,\n",
        "            max_bins=256,\n",
        "            max_interaction_bins=32,\n",
        "            random_state=42\n",
        "        )\n",
        "        \n",
        "        ebm.fit(X_train, y_train)\n",
        "        \n",
        "        # Evaluate\n",
        "        y_pred = ebm.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        ebm_models[target_name] = ebm\n",
        "        ebm_results[target_name] = {\n",
        "            'mse': mse,\n",
        "            'mae': mae,\n",
        "            'r2': r2\n",
        "        }\n",
        "        \n",
        "        print(f\"  R¬≤ = {r2:.3f}, MAE = {mae:.3f}\")\n",
        "    \n",
        "    return ebm_models, ebm_results\n",
        "\n",
        "# Train EBM models\n",
        "ebm_models, ebm_results = train_ebm_models(X_static, y_targets, target_cols)\n",
        "print(\"\\n‚úÖ EBM models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize EBM feature importance\n",
        "def plot_ebm_importance(ebm_models, static_cols, target_cols, top_n=10):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, target_name in enumerate(target_cols[:6]):  # Show first 6 targets\n",
        "        ax = axes[i]\n",
        "        ebm = ebm_models[target_name]\n",
        "        \n",
        "        # Get feature importance\n",
        "        importance = ebm.feature_importances_\n",
        "        \n",
        "        # Sort by importance\n",
        "        sorted_idx = np.argsort(importance)[::-1][:top_n]\n",
        "        \n",
        "        # Plot\n",
        "        ax.barh(range(len(sorted_idx)), importance[sorted_idx])\n",
        "        ax.set_yticks(range(len(sorted_idx)))\n",
        "        ax.set_yticklabels([static_cols[idx] for idx in sorted_idx])\n",
        "        ax.set_xlabel('Feature Importance')\n",
        "        ax.set_title(f'{target_name}\\nR¬≤ = {ebm_results[target_name][\"r2\"]:.3f}')\n",
        "        ax.invert_yaxis()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_ebm_importance(ebm_models, static_cols, target_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate EBM explanations for specific samples\n",
        "def explain_sample(ebm_models, static_cols, target_cols, sample_idx=0):\n",
        "    from interpret import show\n",
        "    \n",
        "    sample_static = X_static[sample_idx]\n",
        "    sample_targets = y_targets[sample_idx]\n",
        "    \n",
        "    print(f\"Sample {sample_idx} Explanation:\")\n",
        "    print(f\"Static features: {dict(zip(static_cols, sample_static))}\")\n",
        "    print(f\"Actual targets: {dict(zip(target_cols, sample_targets))}\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # Show explanations for each target\n",
        "    for target_name in target_cols[:3]:  # Show first 3 targets\n",
        "        ebm = ebm_models[target_name]\n",
        "        \n",
        "        # Global explanation\n",
        "        global_exp = ebm.explain_global()\n",
        "        print(f\"\\nGlobal explanation for {target_name}:\")\n",
        "        show(global_exp)\n",
        "        \n",
        "        # Local explanation\n",
        "        local_exp = ebm.explain_local(sample_static.reshape(1, -1), sample_targets[target_cols.index(target_name)])\n",
        "        print(f\"\\nLocal explanation for {target_name}:\")\n",
        "        show(local_exp)\n",
        "\n",
        "# Generate explanations\n",
        "explain_sample(ebm_models, static_cols, target_cols, sample_idx=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print summary results\n",
        "print(\"=\"*60)\n",
        "print(\"AUTOFORMER + EBM ANALYSIS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä Autoformer Performance:\")\n",
        "for key, value in test_results.items():\n",
        "    if 'eval_' in key:\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nüîç EBM Model Performance:\")\n",
        "for target_name, results in ebm_results.items():\n",
        "    print(f\"  {target_name}:\")\n",
        "    print(f\"    R¬≤ = {results['r2']:.3f}\")\n",
        "    print(f\"    MAE = {results['mae']:.3f}\")\n",
        "    print(f\"    MSE = {results['mse']:.3f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Analysis completed successfully!\")\n",
        "print(\"\\nKey Findings:\")\n",
        "print(\"1. Autoformer provides multivariate time series forecasting\")\n",
        "print(\"2. EBM models explain how blend characteristics affect each target\")\n",
        "print(\"3. Feature importance shows which crude properties matter most\")\n",
        "print(\"4. Local explanations reveal specific blend effects\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
